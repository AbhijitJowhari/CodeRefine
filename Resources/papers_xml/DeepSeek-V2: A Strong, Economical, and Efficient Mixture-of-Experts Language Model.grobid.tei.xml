<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coords="1,114.45,117.42,366.37,14.95;1,165.26,137.34,264.77,14.95">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-24">24 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main" coords="1,114.45,117.42,366.37,14.95;1,165.26,137.34,264.77,14.95">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-24">24 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">B7362DE7F18B2DAF5FE0ED4B92FF4C33</idno>
					<idno type="arXiv">arXiv:2405.04434v4[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-11T09:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. The model checkpoints are available at <ref type="url" coords="1,207.05,476.44,258.12,10.13" target="https://github.com/deepseek-ai/DeepSeek-V2">https://github.com/deepseek-ai/DeepSeek-V2</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="46" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="47" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="48" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="49" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="50" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="51" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="52" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance (MMLU)</head><note type="other">DeepSeek</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past few years, Large Language Models (LLMs) <ref type="bibr" coords="4,332.30,113.89,82.46,10.49">(Anthropic, 2023;</ref><ref type="bibr" coords="4,417.48,113.89,64.26,10.49" target="#b16">Google, 2023;</ref><ref type="bibr" coords="4,484.47,113.89,41.31,10.49;4,70.87,127.43,19.81,10.49">OpenAI, 2022</ref><ref type="bibr" coords="4,98.35,127.43,25.68,10.49" target="#b37">OpenAI, , 2023) )</ref> have undergone rapid development, offering a glimpse into the dawn of Artificial General Intelligence (AGI). In general, the intelligence of an LLM tends to improve as the number of parameters increases, allowing it to exhibit emergent capabilities across various tasks <ref type="bibr" coords="4,502.11,154.53,22.30,10.49;4,70.87,168.08,53.33,10.49" target="#b51">(Wei et al., 2022)</ref>. However, the improvement comes at the cost of larger computing resources for training and a potential decrease in inference throughput. These constraints present significant challenges that impede the widespread adoption and utilization of LLMs. In order to tackle this problem, we introduce DeepSeek-V2, a strong open-source Mixture-of-Experts (MoE) language model, characterized by economical training and efficient inference through an innovative Transformer architecture. It is equipped with a total of 236B parameters, of which 21B are activated for each token, and supports a context length of 128K tokens.</p><p>We optimize the attention modules and Feed-Forward Networks (FFNs) within the Transformer framework <ref type="bibr" coords="4,160.86,283.25,98.57,10.49" target="#b50">(Vaswani et al., 2017)</ref> with our proposed Multi-head Latent Attention (MLA) and DeepSeekMoE. (1) In the context of attention mechanisms, the Key-Value (KV) cache of the Multi-Head Attention (MHA) <ref type="bibr" coords="4,252.56,310.35,104.16,10.49" target="#b50">(Vaswani et al., 2017)</ref> poses a significant obstacle to the inference efficiency of LLMs. Various approaches have been explored to address this issue, including Grouped-Query Attention (GQA) <ref type="bibr" coords="4,290.04,337.45,96.93,10.49" target="#b1">(Ainslie et al., 2023)</ref> and Multi-Query Attention (MQA) <ref type="bibr" coords="4,106.75,351.00,68.51,10.49" target="#b45">(Shazeer, 2019)</ref>. However, these methods often compromise performance in their attempt to reduce the KV cache. In order to achieve the best of both worlds, we introduce MLA, an attention mechanism equipped with low-rank key-value joint compression. Empirically, MLA achieves superior performance compared with MHA, and meanwhile significantly reduces the KV cache during inference, thus boosting the inference efficiency. (2) For Feed-Forward Networks (FFNs), we follow the DeepSeekMoE architecture <ref type="bibr" coords="4,370.99,418.74,78.13,10.49" target="#b9">(Dai et al., 2024)</ref>, which adopts fine-grained expert segmentation and shared expert isolation for higher potential in expert specialization. The DeepSeekMoE architecture demonstrates great advantages compared with conventional MoE architectures like GShard <ref type="bibr" coords="4,287.09,459.39,102.26,10.49">(Lepikhin et al., 2021)</ref>, enabling us to train strong models at an economical cost. As we employ expert parallelism during training, we also devise supplementary mechanisms to control communication overheads and ensure load balance. By combining these two techniques, DeepSeek-V2 features strong performance (Figure <ref type="figure" coords="4,500.75,500.04,4.17,10.49" target="#fig_0">1</ref> We construct a high-quality and multi-source pre-training corpus consisting of 8.1T tokens. Compared with the corpus used in DeepSeek 67B (our previous release) ( <ref type="bibr" coords="4,411.91,547.46,88.31,10.49" target="#b11">DeepSeek-AI, 2024)</ref>, this corpus features an extended amount of data, especially Chinese data, and higher data quality. We first pretrain DeepSeek-V2 on the full pre-training corpus. Then, we collect 1.5M conversational sessions, which encompass various domains such as math, code, writing, reasoning, safety, and more, to perform Supervised Fine-Tuning (SFT) for DeepSeek-V2 Chat (SFT). Finally, we follow DeepSeekMath <ref type="bibr" coords="4,148.85,615.21,86.37,10.49" target="#b44">(Shao et al., 2024)</ref> to employ Group Relative Policy Optimization (GRPO) to further align the model with human preference and produce DeepSeek-V2 Chat (RL).</p><p>We evaluate DeepSeek-V2 on a wide range of benchmarks in English and Chinese, and compare it with representative open-source models. Evaluation results show that even with only 21B activated parameters, DeepSeek-V2 still achieves top-tier performance among open-source models and becomes the strongest open-source MoE language model. Figure <ref type="figure" coords="4,453.12,689.73,4.63,10.49" target="#fig_0">1</ref>(a) highlights that, on MMLU, DeepSeek-V2 achieves top-ranking performance with only a small number of activated parameters. In addition, as shown in Figure <ref type="figure" coords="4,350.78,716.82,4.38,10.49" target="#fig_0">1</ref>(b), compared with DeepSeek 67B, DeepSeek-V2 saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We also evaluate DeepSeek-V2 Chat (SFT) and  DeepSeek-V2 Chat (RL) on open-ended benchmarks. Notably, DeepSeek-V2 Chat (RL) achieves 38.9 length-controlled win rate on AlpacaEval 2.0 <ref type="bibr" coords="5,318.94,533.82,96.20,10.49" target="#b13">(Dubois et al., 2024)</ref>, 8.97 overall score on MT-Bench <ref type="bibr" coords="5,124.91,547.37,93.60,10.49" target="#b59">(Zheng et al., 2023)</ref>, and 7.91 overall score on AlignBench <ref type="bibr" coords="5,416.61,547.37,78.45,10.49">(Liu et al., 2023)</ref>. The English open-ended conversation evaluations demonstrate that DeepSeek-V2 Chat (RL) has toptier performance among open-source chat models. In addition, the evaluation on AlignBench indicates that in Chinese, DeepSeek-V2 Chat (RL) outperforms all of open-source models, and even beats most of closed-source models.</p><p>In order to facilitate further research and development on MLA and DeepSeekMoE, we also release DeepSeek-V2-Lite, a smaller model equipped with MLA and DeepSeekMoE, for the open-source community. It has a total of 15.7B parameters, where 2.4B are activated for each token. Detailed descriptions about DeepSeek-V2-Lite can be found in Appendix B.</p><p>In the rest of this paper, we first provide a detailed description of the model architecture of DeepSeek-V2 (Section 2). Subsequently, we introduce our pre-training endeavors, including the training data construction, hyper-parameter settings, infrastructures, long context extension, and the evaluation of model performance and efficiency (Section 3). Following this, we demonstrate our efforts in alignment, encompassing Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), the evaluation results, and other discussion (Section 4). Finally, we summarize the conclusion, deliberate on the current limitations of DeepSeek-V2, and outline our future work (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Architecture</head><p>By and large, DeepSeek-V2 is still in the Transformer architecture <ref type="bibr" coords="6,387.69,177.05,99.26,10.49" target="#b50">(Vaswani et al., 2017)</ref>, where each Transformer block consists of an attention module and a Feed-Forward Network (FFN). However, for both the attention module and the FFN, we design and employ innovative architectures. For attention, we design MLA, which utilizes low-rank key-value joint compression to eliminate the bottleneck of inference-time key-value cache, thus supporting efficient inference. For FFNs, we adopt the DeepSeekMoE architecture <ref type="bibr" coords="6,320.53,244.80,75.33,10.49" target="#b9">(Dai et al., 2024)</ref>, a high-performance MoE architecture that enables training strong models at an economical cost. An illustration of the architecture of DeepSeek-V2 is presented in Figure <ref type="figure" coords="6,315.99,271.90,4.07,10.49" target="#fig_2">2</ref>, and we will introduce the details of MLA and DeepSeekMoE in this section. For other tiny details (e.g., layer normalization and the activation function in FFNs), unless specifically stated, DeepSeek-V2 follows the settings of DeepSeek 67B (DeepSeek-AI, 2024).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-Head Latent Attention: Boosting Inference Efficiency</head><p>Conventional Transformer models usually adopts Multi-Head Attention (MHA) <ref type="bibr" coords="6,478.84,367.98,45.58,10.49;6,70.87,381.53,54.40,10.49" target="#b50">(Vaswani et al., 2017)</ref>, but during generation, its heavy Key-Value (KV) cache will become the bottleneck that limit the inference efficiency. In order to reduce the KV cache, Multi-Query Attention (MQA) <ref type="bibr" coords="6,133.32,408.63,74.80,10.49" target="#b45">(Shazeer, 2019)</ref> and Grouped-Query Attention (GQA) <ref type="bibr" coords="6,406.03,408.63,99.43,10.49" target="#b1">(Ainslie et al., 2023)</ref> are proposed. They require a smaller magnitude of KV cache, but their performance does not match MHA (we provide the ablation of MHA, GQA and MQA in Appendix D.1).</p><p>For DeepSeek-V2, we design an innovative attention mechanism called Multi-head Latent Attention (MLA). Equipped with low-rank key-value joint compression, MLA achieves better performance than MHA, but requires a significantly smaller amount of KV cache. We introduce its architecture in the following, and also provide a comparison between MLA and MHA in Appendix D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Preliminaries: Standard Multi-Head Attention</head><p>We first introduce the standard MHA mechanism as background. Let 𝑑 be the embedding dimension, 𝑛 ℎ be the number of attention heads, 𝑑 ℎ be the dimension per head, and h 𝑡 ∈ R 𝑑 be the attention input of the 𝑡-th token at an attention layer. Standard MHA first produces q 𝑡 , k 𝑡 , v 𝑡 ∈ R 𝑑 ℎ 𝑛 ℎ through three matrices 𝑊 𝑄 , 𝑊 𝐾 , 𝑊 𝑉 ∈ R 𝑑 ℎ 𝑛 ℎ ×𝑑 , respectively: Then, q 𝑡 , k 𝑡 , v 𝑡 will be sliced into 𝑛 ℎ heads for the multi-head attention computation:</p><formula xml:id="formula_0" coords="6,272.39,627.59,253.11,11.17">q 𝑡 = 𝑊 𝑄 h 𝑡 ,<label>(1)</label></formula><formula xml:id="formula_1" coords="6,272.39,645.13,253.11,11.17">k 𝑡 = 𝑊 𝐾 h 𝑡 ,<label>(2)</label></formula><formula xml:id="formula_2" coords="6,272.99,662.67,252.51,11.17">v 𝑡 = 𝑊 𝑉 h 𝑡 ,<label>(3)</label></formula><p>[q 𝑡,1 ;q 𝑡,2 ; ...; q 𝑡,𝑛 ℎ ] = q 𝑡 , (4)</p><formula xml:id="formula_3" coords="7,222.52,334.66,302.98,27.33">[k 𝑡,1 ;k 𝑡,2 ; ...; k 𝑡,𝑛 ℎ ] = k 𝑡 , (5) [v 𝑡,1 ;v 𝑡,2 ; ...; v 𝑡,𝑛 ℎ ] = v 𝑡 ,<label>(6)</label></formula><formula xml:id="formula_4" coords="7,231.45,368.35,294.05,30.38">o 𝑡,𝑖 = 𝑡 ∑︁ 𝑗=1 Softmax 𝑗 ( q 𝑇 𝑡,𝑖 k 𝑗,𝑖 √ 𝑑 ℎ )v 𝑗,𝑖 ,<label>(7)</label></formula><formula xml:id="formula_5" coords="7,235.19,406.75,290.32,11.17">u 𝑡 = 𝑊 𝑂 [o 𝑡,1 ; o 𝑡,2 ; ...; o 𝑡,𝑛 ℎ ],<label>(8)</label></formula><p>where q 𝑡,𝑖 , k 𝑡,𝑖 , v 𝑡,𝑖 ∈ R 𝑑 ℎ denote the query, key, and value of the 𝑖-th attention head, respectively; 𝑊 𝑂 ∈ R 𝑑×𝑑 ℎ 𝑛 ℎ denotes the output projection matrix. During inference, all keys and values need to be cached to accelerate inference, so MHA needs to cache 2𝑛 ℎ 𝑑 ℎ 𝑙 elements for each token. In model deployment, this heavy KV cache is a large bottleneck that limits the maximum batch size and sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Low-Rank Key-Value Joint Compression</head><p>The core of MLA is the low-rank joint compression for keys and values to reduce KV cache:</p><formula xml:id="formula_6" coords="7,264.39,562.83,261.11,11.17">c 𝐾𝑉 𝑡 = 𝑊 𝐷𝐾𝑉 h 𝑡 ,<label>(9)</label></formula><formula xml:id="formula_7" coords="7,267.91,580.37,253.05,11.17">k 𝐶 𝑡 = 𝑊 𝑈 𝐾 c 𝐾𝑉 𝑡 , (<label>10</label></formula><formula xml:id="formula_8" coords="7,268.51,581.05,256.99,28.03">) v 𝐶 𝑡 = 𝑊 𝑈𝑉 c 𝐾𝑉 𝑡 ,<label>(11)</label></formula><p>where c 𝐾𝑉 𝑡 ∈ R 𝑑 𝑐 is the compressed latent vector for keys and values; 𝑑 𝑐 (≪ 𝑑 ℎ 𝑛 ℎ ) denotes the KV compression dimension; 𝑊 𝐷𝐾𝑉 ∈ R 𝑑 𝑐 ×𝑑 is the down-projection matrix; and 𝑊 𝑈 𝐾 , 𝑊 𝑈𝑉 ∈ R 𝑑 ℎ 𝑛 ℎ ×𝑑 𝑐 are the up-projection matrices for keys and values, respectively. During inference, MLA only needs to cache c 𝐾𝑉 𝑡 , so its KV cache has only 𝑑 𝑐 𝑙 elements, where 𝑙 denotes the number of layers. In addition, during inference, since 𝑊 𝑈 𝐾 can be absorbed into 𝑊 𝑄 , and 𝑊 𝑈𝑉 can be absorbed into 𝑊 𝑂 , we even do not need to compute keys and values out for attention. Figure <ref type="figure" coords="7,467.23,690.84,5.38,10.49" target="#fig_3">3</ref> intuitively illustrates how the KV joint compression in MLA reduces the KV cache. Moreover, in order to reduce the activation memory during training, we also perform low-rank compression for the queries, even if it cannot reduce the KV cache:</p><formula xml:id="formula_9" coords="8,269.27,111.86,256.23,11.72">c 𝑄 𝑡 = 𝑊 𝐷𝑄 h 𝑡 ,<label>(12)</label></formula><formula xml:id="formula_10" coords="8,268.30,129.95,257.20,11.72">q 𝐶 𝑡 = 𝑊 𝑈𝑄 c 𝑄 𝑡 ,<label>(13)</label></formula><p>where c 𝑄 𝑡 ∈ R 𝑑 ′ 𝑐 is the compressed latent vector for queries; 𝑑 ′ 𝑐 (≪ 𝑑 ℎ 𝑛 ℎ ) denotes the query compression dimension; and 𝑊 𝐷𝑄 ∈ R 𝑑 ′ 𝑐 ×𝑑 , 𝑊 𝑈𝑄 ∈ R 𝑑 ℎ 𝑛 ℎ ×𝑑 ′ 𝑐 are the down-projection and upprojection matrices for queries, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Decoupled Rotary Position Embedding</head><p>Following DeepSeek 67B (DeepSeek-AI, 2024), we intend to use the Rotary Position Embedding (RoPE) <ref type="bibr" coords="8,133.09,250.75,73.29,10.49" target="#b47">(Su et al., 2024)</ref> for DeepSeek-V2. However, RoPE is incompatible with low-rank KV compression. To be specific, RoPE is position-sensitive for both keys and queries. If we apply RoPE for the keys k 𝐶 𝑡 , 𝑊 𝑈 𝐾 in Equation 10 will be coupled with a position-sensitive RoPE matrix. In this way, 𝑊 𝑈 𝐾 cannot be absorbed into 𝑊 𝑄 any more during inference, since a RoPE matrix related to the currently generating token will lie between 𝑊 𝑄 and 𝑊 𝑈 𝐾 and matrix multiplication does not obey a commutative law. As a result, we must recompute the keys for all the prefix tokens during inference, which will significantly hinder the inference efficiency.</p><p>As a solution, we propose the decoupled RoPE strategy that uses additional multi-head queries q 𝑅 𝑡,𝑖 ∈ R 𝑑 𝑅 ℎ and a shared key k 𝑅 𝑡 ∈ R 𝑑 𝑅 ℎ to carry RoPE, where 𝑑 𝑅 ℎ denotes the per-head dimension of the decoupled queries and key. Equipped with the decoupled RoPE strategy, MLA performs the following computation:</p><p>[q 𝑅 𝑡,1 ; q 𝑅 𝑡,2 ; ...;</p><formula xml:id="formula_11" coords="8,232.25,417.81,293.25,12.67">q 𝑅 𝑡,𝑛 ℎ ] = q 𝑅 𝑡 = RoPE(𝑊 𝑄𝑅 c 𝑄 𝑡 ),<label>(14)</label></formula><formula xml:id="formula_12" coords="8,268.72,436.62,256.78,11.17">k 𝑅 𝑡 = RoPE(𝑊 𝐾𝑅 h 𝑡 ),<label>(15)</label></formula><p>q 𝑡,𝑖 = [q 𝐶 𝑡,𝑖 ; q 𝑅 𝑡,𝑖 ], (16)</p><formula xml:id="formula_13" coords="8,266.50,472.05,259.00,11.33">k 𝑡,𝑖 = [k 𝐶 𝑡,𝑖 ; k 𝑅 𝑡 ],<label>(17)</label></formula><formula xml:id="formula_14" coords="8,267.10,489.94,258.40,34.34">o 𝑡,𝑖 = 𝑡 ∑︁ 𝑗=1 Softmax 𝑗 ( q 𝑇 𝑡,𝑖 k 𝑗,𝑖 √︃ 𝑑 ℎ + 𝑑 𝑅 ℎ )v 𝐶 𝑗,𝑖 ,<label>(18)</label></formula><formula xml:id="formula_15" coords="8,270.83,532.37,254.68,11.17">u 𝑡 = 𝑊 𝑂 [o 𝑡,1 ; o 𝑡,2 ; ...; o 𝑡,𝑛 ℎ ],<label>(19)</label></formula><p>where</p><formula xml:id="formula_16" coords="8,102.94,555.99,70.28,14.22">𝑊 𝑄𝑅 ∈ R 𝑑 𝑅 ℎ 𝑛 ℎ ×𝑑 ′</formula><p>𝑐 and 𝑊 𝐾𝑅 ∈ R 𝑑 𝑅 ℎ ×𝑑 are matrices to produce the decouples queries and key, respectively; RoPE(•) denotes the operation that applies RoPE matrices; and [•; •] denotes the concatenation operation. During inference, the decoupled key should also be cached. Therefore, DeepSeek-V2 requires a total KV cache containing (𝑑 𝑐 + 𝑑 𝑅 ℎ )𝑙 elements. In order to demonstrate the complete computation process of MLA, we also organize and provide its full formulas in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">Comparison of Key-Value Cache</head><p>We demonstrate a comparison of the KV cache per token among different attention mechanisms in Table <ref type="table" coords="8,110.60,701.01,4.01,10.49" target="#tab_2">1</ref>. MLA requires only a small amount of KV cache, equal to GQA with only 2.25 groups, but can achieve stronger performance than MHA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DeepSeekMoE: Training Strong Models at Economical Costs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Basic Architecture</head><p>For FFNs, we employ the DeepSeekMoE architecture <ref type="bibr" coords="9,329.36,343.56,75.47,10.49" target="#b9">(Dai et al., 2024)</ref>. DeepSeekMoE has two key ideas: segmenting experts into finer granularity for higher expert specialization and more accurate knowledge acquisition, and isolating some shared experts for mitigating knowledge redundancy among routed experts. With the same number of activated and total expert parameters, DeepSeekMoE can outperform conventional MoE architectures like GShard <ref type="bibr" coords="9,451.90,397.76,73.87,10.49;9,70.87,411.31,25.45,10.49">(Lepikhin et al., 2021)</ref> by a large margin.</p><p>Let u 𝑡 be the FFN input of the 𝑡-th token, we compute the FFN output h ′ 𝑡 as follows:</p><formula xml:id="formula_17" coords="9,194.91,456.56,330.59,30.19">h ′ 𝑡 = u 𝑡 + 𝑁 𝑠 ∑︁ 𝑖=1 FFN (𝑠) 𝑖 (u 𝑡 ) + 𝑁 𝑟 ∑︁ 𝑖=1 𝑔 𝑖,𝑡 FFN (𝑟) 𝑖 (u 𝑡 ),<label>(20)</label></formula><formula xml:id="formula_18" coords="9,192.45,495.67,333.05,27.05">𝑔 𝑖,𝑡 = 𝑠 𝑖,𝑡 , 𝑠 𝑖,𝑡 ∈ Topk({𝑠 𝑗,𝑡 |1 ⩽ 𝑗 ⩽ 𝑁 𝑟 }, 𝐾 𝑟 ), 0, otherwise,<label>(21)</label></formula><formula xml:id="formula_19" coords="9,193.68,530.04,331.82,11.17">𝑠 𝑖,𝑡 = Softmax 𝑖 u 𝑡 𝑇 e 𝑖 ,<label>(22)</label></formula><p>where 𝑁 𝑠 and 𝑁 𝑟 denote the numbers of shared experts and routed experts, respectively; FFN (𝑠) 𝑖 (•) and FFN (𝑟)  𝑖 (•) denote the 𝑖-th shared expert and the 𝑖-th routed expert, respectively; 𝐾 𝑟 denotes the number of activated routed experts; 𝑔 𝑖,𝑡 is the gate value for the 𝑖-th expert; 𝑠 𝑖,𝑡 is the tokento-expert affinity; e 𝑖 is the centroid of the 𝑖-th routed expert in this layer; and Topk(•, 𝐾) denotes the set comprising 𝐾 highest scores among the affinity scores calculated for the 𝑡-th token and all routed experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Device-Limited Routing</head><p>We design a device-limited routing mechanism to bound MoE-related communication costs. When expert parallelism is employed, the routed experts will be distributed across multiple devices. For each token, its MoE-related communication frequency is proportional to the number of devices covered by its target experts. Due to the fine-grained expert segmentation in DeepSeekMoE, the number of activated experts can be large, so the MoE-related communication will be more costly if we apply expert parallelism.</p><p>For DeepSeek-V2, beyond the naive top-K selection of routed experts, we additionally ensure that the target experts of each token will be distributed on at most 𝑀 devices. To be specific, for each token, we first select 𝑀 devices that have experts with the highest affinity scores in them. Then, we perform top-K selection among experts on these 𝑀 devices. In practice, we find that when 𝑀 ⩾ 3, the device-limited routing can achieve a good performance roughly aligned with the unrestricted top-K routing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Auxiliary Loss for Load Balance</head><p>We take the load balance into consideration for automatically learned routing strategies. Firstly, unbalanced load will raise the risk of routing collapse <ref type="bibr" coords="10,336.69,222.42,97.68,10.49" target="#b46">(Shazeer et al., 2017)</ref>, preventing some experts being fully trained and utilized. Secondly, when expert parallelism is employed, unbalanced load will diminish computation efficiency. During the training of DeepSeek-V2, we design three kinds of auxiliary losses, for controlling expert-level load balance (L ExpBal ), device-level load balance (L DevBal ), and communication balance (L CommBal ), respectively.</p><p>Expert-Level Balance Loss. We use an expert-level balance loss <ref type="bibr" coords="10,388.56,312.00,89.15,10.49" target="#b14">(Fedus et al., 2021;</ref><ref type="bibr" coords="10,480.42,312.00,44.00,10.49;10,70.87,325.55,53.77,10.49">Lepikhin et al., 2021)</ref> to mitigate the risk of routing collapse:</p><formula xml:id="formula_20" coords="10,191.74,349.18,333.77,30.15">L ExpBal = 𝛼 1 𝑁 𝑟 ∑︁ 𝑖=1 𝑓 𝑖 𝑃 𝑖 ,<label>(23)</label></formula><formula xml:id="formula_21" coords="10,219.80,386.90,305.70,29.92">𝑓 𝑖 = 𝑁 𝑟 𝐾 𝑟 𝑇 𝑇 ∑︁ 𝑡=1 1(Token 𝑡 selects Expert 𝑖),<label>(24)</label></formula><formula xml:id="formula_22" coords="10,217.13,424.40,308.37,29.92">𝑃 𝑖 = 1 𝑇 𝑇 ∑︁ 𝑡=1 𝑠 𝑖,𝑡 ,<label>(25)</label></formula><p>where 𝛼 1 is a hyper-parameter called expert-level balance factor; 1(•) denotes the indicator function; and 𝑇 denotes the number of tokens in a sequence.</p><p>Device-Level Balance Loss. In addition to the expert-level balance loss, we additionally design a device-level balance loss to ensure balanced computation across different devices. In the training process of DeepSeek-V2, we partition all routed experts into 𝐷 groups {E 1 , E 2 , ..., E 𝐷 }, and deploy each group on a single device. The device-level balance loss is computed as follows:</p><formula xml:id="formula_23" coords="10,249.00,579.76,271.95,29.92">L DevBal = 𝛼 2 𝐷 ∑︁ 𝑖=1 𝑓 ′ 𝑖 𝑃 ′ 𝑖 , (<label>26</label></formula><formula xml:id="formula_24" coords="10,520.95,589.28,4.54,10.49">)</formula><formula xml:id="formula_25" coords="10,275.95,614.97,245.02,28.79">𝑓 ′ 𝑖 = 1 |E 𝑖 | ∑︁ 𝑗∈ E 𝑖 𝑓 𝑗 , (<label>27</label></formula><formula xml:id="formula_26" coords="10,520.96,622.36,4.54,10.49">)</formula><formula xml:id="formula_27" coords="10,274.33,648.60,246.64,25.76">𝑃 ′ 𝑖 = ∑︁ 𝑗∈ E 𝑖 𝑃 𝑗 , (<label>28</label></formula><formula xml:id="formula_28" coords="10,520.96,652.96,4.54,10.49">)</formula><p>where 𝛼 2 is a hyper-parameter called device-level balance factor.</p><p>Communication Balance Loss. Finally, we introduce a communication balance loss to ensure that the communication of each device is balanced. Although the device-limited routing mechanism guarantees that the sending communication of each device is bounded, if a certain device receives more tokens than other devices, the practical communication efficiency will also be affected. In order to mitigate this issue, we design a communication balance loss as follows:</p><formula xml:id="formula_29" coords="11,180.90,127.06,344.60,29.92">L CommBal = 𝛼 3 𝐷 ∑︁ 𝑖=1 𝑓 ′′ 𝑖 𝑃 ′′ 𝑖 ,<label>(29)</label></formula><formula xml:id="formula_30" coords="11,214.59,164.56,310.92,29.92">𝑓 ′′ 𝑖 = 𝐷 𝑀𝑇 𝑇 ∑︁ 𝑡=1 1(Token 𝑡 is sent to Device 𝑖),<label>(30)</label></formula><formula xml:id="formula_31" coords="11,212.97,198.96,307.99,25.76">𝑃 ′′ 𝑖 = ∑︁ 𝑗∈ E 𝑖 𝑃 𝑗 , (<label>31</label></formula><formula xml:id="formula_32" coords="11,520.95,203.32,4.54,10.49">)</formula><p>where 𝛼 3 is a hyper-parameter called communication balance factor. The device-limited routing mechanism operates on the principle of ensuring that each device transmits at most 𝑀𝑇 hidden states to other devices. Simultaneously, the communication balance loss is employed to encourage each device to receive around 𝑀𝑇 hidden states from other devices. The communication balance loss guarantees a balanced exchange of information among devices, promoting efficient communications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">Token-Dropping Strategy</head><p>While balance losses aim to encourage a balanced load, it is important to acknowledge that they cannot guarantee a strict load balance. In order to further mitigate the computation wastage caused by unbalanced load, we introduce a device-level token-dropping strategy during training. This approach first computes the average computational budget for each device, which means that the capacity factor for each device is equivalent to 1.0. Then, inspired by <ref type="bibr" coords="11,477.81,413.43,46.59,10.49;11,70.87,426.98,54.13,10.49" target="#b42">Riquelme et al. (2021)</ref>, we drop tokens with the lowest affinity scores on each device until reaching the computational budget. In addition, we ensure that the tokens belonging to approximately 10% of the training sequences will never be dropped. In this way, we can flexibly decide whether to drop tokens during inference according to the efficiency requirements, and always ensure consistency between training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pre-Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setups</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Data Construction</head><p>While maintaining the same data processing stages as for DeepSeek 67B (DeepSeek-AI, 2024), we extend the amount of data and elevate the data quality. In order to enlarge our pre-training corpus, we explore the potential of the internet data and optimize our cleaning processes, thus recovering a large amount of mistakenly deleted data. Moreover, we incorporate more Chinese data, aiming to better leverage the corpus available on the Chinese internet. In addition to the amount of data, we also focus on the data quality. We enrich our pre-training corpus with high-quality data from various sources, and meanwhile improve the quality-based filtering algorithm. The improved algorithm ensures that a large amount of non-beneficial data will be removed, while the valuable data will be mostly retained. In addition, we filter out the contentious content from our pre-training corpus to mitigate the data bias introduced from specific regional cultures. A detailed discussion about the influence of this filtering strategy is presented in Appendix E.</p><p>We adopt the same tokenizer as used in DeepSeek 67B, which is built based on the Byte-level Byte-Pair Encoding (BBPE) algorithm and has a vocabulary size of 100K. Our tokenized pretraining corpus contains 8.1T tokens, where Chinese tokens are approximately 12% more than English ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Hyper-Parameters</head><p>Model Hyper-Parameters. We set the number of Transformer layers to 60 and the hidden dimension to 5120. All learnable parameters are randomly initialized with a standard deviation of 0.006. In MLA, we set the number of attention heads 𝑛 ℎ to 128 and the per-head dimension 𝑑 ℎ to 128. The KV compression dimension 𝑑 𝑐 is set to 512, and the query compression dimension</p><formula xml:id="formula_33" coords="12,70.98,234.12,8.59,9.66">𝑑 ′</formula><p>𝑐 is set to 1536. For the decoupled queries and key, we set the per-head dimension 𝑑 𝑅 ℎ to 64. Following <ref type="bibr" coords="12,123.98,249.85,75.10,10.49" target="#b9">Dai et al. (2024)</ref>, we substitute all FFNs except for the first layer with MoE layers. Each MoE layer consists of 2 shared experts and 160 routed experts, where the intermediate hidden dimension of each expert is 1536. Among the routed experts, 6 experts will be activated for each token. In addition, the low-rank compression and fine-grained expert segmentation will impact the output scale of a layer. Therefore, in practice, we employ additional RMS Norm layers after the compressed latent vectors, and multiply additional scaling factors at the width bottlenecks (i.e., the compressed latent vectors and the intermediate hidden states of routed experts) to ensure stable training. Under this configuration, DeepSeek-V2 comprises 236B total parameters, of which 21B are activated for each token.</p><p>Training Hyper-Parameters. We employ the AdamW optimizer <ref type="bibr" coords="12,383.67,393.83,141.84,10.49" target="#b34">(Loshchilov and Hutter, 2017)</ref> with hyper-parameters set to 𝛽 1 = 0.9, 𝛽 2 = 0.95, and weight_decay = 0.1. The learning rate is scheduled using a warmup-and-step-decay strategy (DeepSeek-AI, 2024). Initially, the learning rate linearly increases from 0 to the maximum value during the first 2K steps. Subsequently, the learning rate is multiplied by 0.316 after training about 60% of tokens, and again by 0.316 after training about 90% of tokens. The maximum learning rate is set to 2.4 × 10 -4 , and the gradient clipping norm is set to 1.0. We also use a batch size scheduling strategy, where the batch size is gradually increased from 2304 to 9216 in the training of the first 225B tokens, and then keeps 9216 in the remaining training. We set the maximum sequence length to 4K, and train DeepSeek-V2 on 8.1T tokens. We leverage pipeline parallelism to deploy different layers of a model on different devices, and for each layer, the routed experts will be uniformly deployed on 8 devices (𝐷 = 8). As for the device-limited routing, each token will be sent to at most 3 devices (𝑀 = 3). As for balance losses, we set 𝛼 1 to 0.003, 𝛼 2 to 0.05, and 𝛼 3 to 0.02. We employ the token-dropping strategy during training for acceleration, but do not drop any tokens for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Infrastructures</head><p>DeepSeek-V2 is trained based on the HAI-LLM framework (High-flyer, 2023), an efficient and light-weight training framework developed internally by our engineers. It employs a 16-way zero-bubble pipeline parallelism <ref type="bibr" coords="12,232.15,663.48,70.83,10.49" target="#b40">(Qi et al., 2023)</ref>, an 8-way expert parallelism <ref type="bibr" coords="12,449.22,663.48,76.56,10.49;12,70.87,677.03,23.95,10.49">(Lepikhin et al., 2021)</ref>, and ZeRO-1 data parallelism <ref type="bibr" coords="12,250.53,677.03,121.53,10.49" target="#b41">(Rajbhandari et al., 2020)</ref>. Given that DeepSeek-V2 has relatively few activated parameters, and a portion of the operators are recomputed to save activation memory, it can be trained without the necessity of tensor parallelism, thereby decreasing the communication overhead. Moreover, in order to further improve the training efficiency, we overlap the computation of shared experts with the expert parallel all-to-all communication. We also customize faster CUDA kernels for communications, routing algorithms, and fused We conduct all experiments on a cluster equipped with NVIDIA H800 GPUs. Each node in the H800 cluster contains 8 GPUs connected using NVLink and NVSwitch within nodes. Across nodes, InfiniBand interconnects are utilized to facilitate communications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Long Context Extension</head><p>After the initial pre-training of DeepSeek-V2, we employ YaRN <ref type="bibr" coords="13,375.45,489.38,83.86,10.49" target="#b39">(Peng et al., 2023)</ref> to extend the default context window length from 4K to 128K. YaRN was specifically applied to the decoupled shared key k 𝑅 𝑡 as it is responsible for carrying RoPE <ref type="bibr" coords="13,319.64,516.47,70.22,10.49" target="#b47">(Su et al., 2024)</ref>. For YaRN, we set the scale 𝑠 to 40, 𝛼 to 1, 𝛽 to 32, and the target maximum context length to 160K. Under these settings, we can expect the model to respond well for a context length of 128K. Slightly diverging from original YaRN, due to our distinct attention mechanism, we adjust the length scaling factor to modulate the attention entropy. The factor √ 𝑡 is computed as √ 𝑡 = 0.0707 ln 𝑠 + 1, aiming at minimizing the perplexity.</p><p>We additionally train the model for 1000 steps, with a sequence length of 32K and a batch size of 576 sequences. Although the training is conducted solely at the sequence length of 32K, the model still demonstrates robust performance when being evaluated at a context length of 128K. As shown in Figure <ref type="figure" coords="13,199.40,645.19,4.17,10.49" target="#fig_4">4</ref>, the results on the "Needle In A Haystack" (NIAH) tests indicate that DeepSeek-V2 performs well across all context window lengths up to 128K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Evaluation Benchmarks</head><p>DeepSeek-V2 is pretrained on a bilingual corpus, so we evaluate it on a series of benchmarks in English and Chinese. Our evaluation is based on our internal evaluation framework integrated in our HAI-LLM framework. Included benchmarks are categorized and listed as follows, where underlined benchmarks are in Chinese:</p><p>Multi-subject multiple-choice datasets include MMLU <ref type="bibr" coords="14,367.36,122.45,115.75,10.49" target="#b18">(Hendrycks et al., 2020)</ref>, C-Eval <ref type="bibr" coords="14,70.51,136.00,91.68,10.49" target="#b23">(Huang et al., 2023)</ref>, and CMMLU <ref type="bibr" coords="14,237.07,136.00,68.00,10.49" target="#b31">(Li et al., 2023)</ref>.</p><p>Language understanding and reasoning datasets include HellaSwag <ref type="bibr" coords="14,428.66,156.33,92.35,10.49" target="#b55">(Zellers et al., 2019)</ref>, PIQA <ref type="bibr" coords="14,100.36,169.88,77.24,10.49" target="#b4">(Bisk et al., 2020)</ref>, ARC <ref type="bibr" coords="14,210.67,169.88,83.33,10.49" target="#b6">(Clark et al., 2018)</ref>, and BigBench Hard (BBH) <ref type="bibr" coords="14,428.57,169.88,93.14,10.49" target="#b49">(Suzgun et al., 2022)</ref>.</p><p>Closed-book question answering datasets include TriviaQA <ref type="bibr" coords="14,388.21,190.20,85.07,10.49" target="#b25">(Joshi et al., 2017)</ref> and Natu-ralQuestions <ref type="bibr" coords="14,135.20,203.75,122.35,10.49" target="#b26">(Kwiatkowski et al., 2019)</ref>. <ref type="bibr" coords="14,328.45,224.07,71.68,10.49" target="#b28">Lai et al. (2017)</ref>, DROP <ref type="bibr" coords="14,441.48,224.07,79.54,10.49" target="#b12">(Dua et al., 2019)</ref>, C3 <ref type="bibr" coords="14,86.78,237.62,76.82,10.49" target="#b48">(Sun et al., 2019)</ref>, and CMRC <ref type="bibr" coords="14,228.02,237.62,75.65,10.49" target="#b8">(Cui et al., 2019)</ref>. <ref type="bibr" coords="14,358.59,257.94,103.18,10.49" target="#b43">Sakaguchi et al. (2019)</ref> and CLUEWSC <ref type="bibr" coords="14,70.51,271.49,72.02,10.49" target="#b53">(Xu et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reading comprehension datasets include RACE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference disambiguation datasets include WinoGrande</head><p>Language modeling datasets include Pile <ref type="bibr" coords="14,291.15,291.82,77.90,10.49" target="#b15">(Gao et al., 2020)</ref>.</p><p>Chinese understanding and culture datasets include CHID <ref type="bibr" coords="14,377.34,312.14,90.88,10.49" target="#b58">(Zheng et al., 2019)</ref> and CCPM <ref type="bibr" coords="14,70.51,325.69,68.00,10.49" target="#b32">(Li et al., 2021)</ref>.</p><p>Math datasets include GSM8K <ref type="bibr" coords="14,243.47,346.01,92.55,10.49" target="#b7">(Cobbe et al., 2021)</ref>, MATH <ref type="bibr" coords="14,382.07,346.01,115.24,10.49" target="#b19">(Hendrycks et al., 2021)</ref>, and CMath <ref type="bibr" coords="14,107.01,359.56,76.47,10.49" target="#b52">(Wei et al., 2023)</ref>.</p><p>Code datasets include HumanEval <ref type="bibr" coords="14,266.85,379.89,89.21,10.49" target="#b7">(Chen et al., 2021)</ref>, MBPP <ref type="bibr" coords="14,399.98,379.89,96.46,10.49" target="#b2">(Austin et al., 2021)</ref>, and CRUXEval <ref type="bibr" coords="14,125.84,393.44,73.06,10.49" target="#b17">(Gu et al., 2024)</ref>. <ref type="bibr" coords="14,271.98,413.76,88.79,10.49">(Zhong et al., 2023)</ref>. Note that AGIEval includes both English and Chinese subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standardized exams include AGIEval</head><p>Following our previous work (DeepSeek-AI, 2024), we adopt perplexity-based evaluation for datasets including HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High, MMLU, ARC-Easy, ARC-Challenge, CHID, C-Eval, CMMLU, C3, and CCPM, and adopt generationbased evaluation for TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, HumanEval, MBPP, CRUXEval, BBH, AGIEval, CLUEWSC, CMRC, and CMath. In addition, we perform languagemodeling-based evaluation for Pile-test and use Bits-Per-Byte (BPB) as the metric to guarantee fair comparison among models with different tokenizers.</p><p>For an intuitive overview of these benchmarks, we additionally provide our evaluation formats for each benchmark in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Evaluation Results</head><p>In Table <ref type="table" coords="14,112.32,615.67,4.13,10.49" target="#tab_3">2</ref>, we compare DeepSeek-V2 with several representative open-source models, including DeepSeek 67B (DeepSeek-AI, 2024) (our previous release), Qwen1.5 72B <ref type="bibr" coords="14,445.68,629.22,75.30,10.49" target="#b3">(Bai et al., 2023)</ref>, LLaMA3 70B (AI@Meta, 2024), and Mixtral 8x22B <ref type="bibr" coords="14,311.40,642.77,66.79,10.49" target="#b35">(Mistral, 2024)</ref>. We evaluate all these models with our internal evaluation framework, and ensure that they share the same evaluation setting. Overall, with only 21B activated parameters, DeepSeek-V2 significantly outperforms DeepSeek 67B on almost all benchmarks, and achieves top-tier performance among open-source models.</p><p>Further, we elaborately compare DeepSeek-V2 with its open-source counterparts one by one. (1) Compared with Qwen1.5 72B, another model that supports both Chinese and English, DeepSeek-V2 demonstrates overwhelming advantages on the majority of English, code, and math benchmarks. As for Chinese benchmarks, Qwen1.5 72B shows better performance on multi-subject multiple-choice tasks while DeepSeek-V2 is comparable or better on others. Note that for the CHID benchmark, the tokenizer of Qwen1.5 72B will encounter errors in our evaluation framework, so we leave the CHID score blank for Qwen1.5 72B.</p><p>(2) Compared with Mixtral 8x22B, DeepSeek-V2 achieves comparable or better English performance, except for TriviaQA, NaturalQuestions, and HellaSwag, which are closely related to English commonsense knowledge. Notably, DeepSeek-V2 outperforms Mixtral 8x22B on MMLU. On code and math benchmarks, DeepSeek-V2 demonstrates comparable performance with Mixtral 8x22B. Since Mixtral 8x22B is not specifically trained on Chinese data, its Chinese capability lags far behind DeepSeek-V2.</p><p>(3) Compared with LLaMA3 70B, DeepSeek-V2 is trained on fewer than a quarter of English tokens. Therefore, we acknowledge that DeepSeek-V2 still has a slight gap in basic English capabilities with LLaMA3 70B. However, even with much fewer training tokens and activated parameters, DeepSeek-V2 still demonstrates comparable code and math capability with LLaMA3 70B. Also, as a bilingual language model, DeepSeek-V2 outperforms LLaMA3 70B overwhelmingly on Chinese benchmarks.</p><p>Finally, it is worth mentioning that certain prior studies <ref type="bibr" coords="16,365.08,108.90,77.99,10.49" target="#b22">(Hu et al., 2024)</ref> incorporate SFT data during the pre-training stage, whereas DeepSeek-V2 has never been exposed to SFT data during pre-training. Inference Efficiency. In order to efficiently deploy DeepSeek-V2 for service, we first convert its parameters into the precision of FP8. In addition, we also perform KV cache quantization <ref type="bibr" coords="16,92.84,346.40,97.21,10.49" target="#b21">(Hooper et al., 2024;</ref><ref type="bibr" coords="16,192.79,346.40,82.29,10.49" target="#b57">Zhao et al., 2023)</ref> for DeepSeek-V2 to further compress each element in its KV cache into 6 bits on average. Benefiting from MLA and these optimizations, actually deployed DeepSeek-V2 requires significantly less KV cache than DeepSeek 67B, and thus can serve a much larger batch size. We evaluate the generation throughput of DeepSeek-V2 based on the prompt and generation length distribution from the actually deployed DeepSeek 67B service. On a single node with 8 H800 GPUs, DeepSeek-V2 achieves a generation throughput exceeding 50K tokens per second, which is 5.76 times the maximum generation throughput of DeepSeek 67B. In addition, the prompt input throughput of DeepSeek-V2 exceeds 100K tokens per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Training and Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Alignment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Supervised Fine-Tuning</head><p>Building upon our prior research (DeepSeek-AI, 2024), we curate our instruction tuning datasets to include 1.5M instances, comprising 1.2M instances for helpfulness and 0.3M instances for safety. In comparison to the initial version, we improve the data quality to mitigate hallucinatory responses and enhance writing proficiency. We fine-tune DeepSeek-V2 with 2 epochs, and the learning rate is set to 5 × 10 -6 . For the evaluation of DeepSeek-V2 Chat (SFT), we mainly include generation-based benchmarks, except for several representative multiple-choice tasks (MMLU and ARC). We also conduct an instruction-following evaluation (IFEval) <ref type="bibr" coords="16,467.03,620.90,58.74,10.49;16,70.87,634.45,25.50,10.49" target="#b62">(Zhou et al., 2023)</ref> for DeepSeek-V2 Chat (SFT), using prompt-level loose accuracy as the metric. Moreover, we employ LiveCodeBench <ref type="bibr" coords="16,208.41,648.00,80.26,10.49" target="#b24">(Jain et al., 2024)</ref> questions from September 1st, 2023 to April 1st, 2024 to evaluate chat models. In addition to the standard benchmarks, we further evaluate our model on open-ended conversation benchmarks including MT-Bench <ref type="bibr" coords="16,430.71,675.10,90.31,10.49" target="#b59">(Zheng et al., 2023)</ref>, AlpacaEval 2.0 <ref type="bibr" coords="16,144.65,688.65,92.60,10.49" target="#b13">(Dubois et al., 2024)</ref>, and AlignBench <ref type="bibr" coords="16,324.92,688.65,74.28,10.49">(Liu et al., 2023)</ref>. For comparison, we also evaluate Qwen1.5 72B Chat, LLaMA-3-70B Instruct, and Mistral-8x22B Instruct in our evaluation framework and settings. As for DeepSeek 67B Chat, we directly refer to the evaluation results reported in our previous release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Reinforcement Learning</head><p>In order to further unlock the potential of DeepSeek-V2 and align it with human preference, we conduct Reinforcement Learning (RL) to adjust its preference.</p><p>Reinforcement Learning Algorithm. In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) <ref type="bibr" coords="17,252.84,173.52,81.39,10.49" target="#b44">(Shao et al., 2024)</ref>, which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜 1 , 𝑜 2 , • • • , 𝑜 𝐺 } from the old policy 𝜋 𝜃 𝑜𝑙𝑑 and then optimizes the policy model 𝜋 𝜃 by maximizing the following objective:</p><formula xml:id="formula_34" coords="17,89.23,251.15,436.27,48.55">J 𝐺𝑅𝑃𝑂 (𝜃) = E[𝑞 ∼ 𝑃(𝑄), {𝑜 𝑖 } 𝐺 𝑖=1 ∼ 𝜋 𝜃 𝑜𝑙𝑑 (𝑂|𝑞)] 1 𝐺 𝐺 ∑︁ 𝑖=1 min 𝜋 𝜃 (𝑜 𝑖 |𝑞) 𝜋 𝜃 𝑜𝑙𝑑 (𝑜 𝑖 |𝑞) 𝐴 𝑖 , clip 𝜋 𝜃 (𝑜 𝑖 |𝑞) 𝜋 𝜃 𝑜𝑙𝑑 (𝑜 𝑖 |𝑞) , 1 -𝜀, 1 + 𝜀 𝐴 𝑖 -𝛽D 𝐾𝐿 𝜋 𝜃 ||𝜋 𝑟𝑒 𝑓 ,<label>(32)</label></formula><formula xml:id="formula_35" coords="17,189.37,312.37,331.59,25.06">D 𝐾𝐿 𝜋 𝜃 ||𝜋 𝑟𝑒 𝑓 = 𝜋 𝑟𝑒 𝑓 (𝑜 𝑖 |𝑞) 𝜋 𝜃 (𝑜 𝑖 |𝑞) -log 𝜋 𝑟𝑒 𝑓 (𝑜 𝑖 |𝑞) 𝜋 𝜃 (𝑜 𝑖 |𝑞) -1, (<label>33</label></formula><formula xml:id="formula_36" coords="17,520.95,320.84,4.54,10.49">)</formula><p>where 𝜀 and 𝛽 are hyper-parameters; and 𝐴 𝑖 is the advantage, computed using a group of rewards {𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } corresponding to the outputs within each group:</p><formula xml:id="formula_37" coords="17,225.52,381.28,295.43,26.54">𝐴 𝑖 = 𝑟 𝑖 -m𝑒𝑎𝑛({𝑟 1 , 𝑟 2 , • • • , 𝑟 𝐺 }) s𝑡𝑑({𝑟 1 , 𝑟 2 , • • • , 𝑟 𝐺 }) . (<label>34</label></formula><formula xml:id="formula_38" coords="17,520.95,388.96,4.54,10.49">)</formula><p>Training Strategy. In our preliminary experiments, we find that the RL training on reasoning data, such as code and math prompts, exhibits unique characteristics that are distinct from the training on general data. For example, the mathematical and coding abilities of our model can keep improving over a longer period of training steps. Therefore, we employ a two-stage RL training strategy, which first performs reasoning alignment, and then performs human preference alignment. In the first reasoning alignment stage, we train a reward model 𝑅𝑀 𝑟𝑒𝑎𝑠𝑜𝑛𝑖𝑛𝑔 for code and math reasoning tasks, and optimize the policy model with the feedback of 𝑅𝑀 𝑟𝑒𝑎𝑠𝑜𝑛𝑖𝑛𝑔 :</p><formula xml:id="formula_39" coords="17,254.41,534.56,271.09,10.79">𝑟 𝑖 = 𝑅𝑀 𝑟𝑒𝑎𝑠𝑜𝑛𝑖𝑛𝑔 (𝑜 𝑖 ).<label>(35)</label></formula><p>In the second human preference alignment stage, we adopt a multi-reward framework, which acquires rewards from a helpful reward model 𝑅𝑀 ℎ𝑒𝑙 𝑝 𝑓 𝑢𝑙 , a safety reward model 𝑅𝑀 𝑠𝑎 𝑓 𝑒𝑡 𝑦 , and a rule-based reward model 𝑅𝑀 𝑟𝑢𝑙𝑒 . The final reward of a response 𝑜 𝑖 is</p><formula xml:id="formula_40" coords="17,173.57,609.86,351.93,11.68">𝑟 𝑖 = 𝑐 1 • 𝑅𝑀 ℎ𝑒𝑙 𝑝 𝑓 𝑢𝑙 (𝑜 𝑖 ) + 𝑐 2 • 𝑅𝑀 𝑠𝑎 𝑓 𝑒𝑡 𝑦 (𝑜 𝑖 ) + 𝑐 3 • 𝑅𝑀 𝑟𝑢𝑙𝑒 (𝑜 𝑖 ),<label>(36)</label></formula><p>where 𝑐 1 , 𝑐 2 , and 𝑐 3 are corresponding coefficients.</p><p>In order to obtain reliable reward models that play crucial roles in the RL training, we carefully collect preference data, and meticulously conduct quality filtering and proportion adjustments. We obtain code preference data based on compiler-feedback, and mathematical preference data based on the ground-truth labels. For reward model training, we initialize the reward models with DeepSeek-V2 Chat (SFT) and train them with either a point-wise or a pair-wise loss. In our experiments, we observe that the RL training can fully tap into and activate the potential of our model, enabling it to select the correct and satisfactory answer from possible responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimizations for Training Efficiency.</head><p>Conducting RL training on extremely large models places high demands on the training framework. It requires careful engineering optimization to manage the GPU memory and RAM pressure, and meanwhile maintain a fast training speed. For this goal, we implement the following engineering optimizations. (1) Firstly, we propose a hybrid engine that adopts different parallel strategies for training and inference respectively to achieve higher GPU utilization. (2) Secondly, we leverage vLLM <ref type="bibr" coords="18,383.18,156.33,89.53,10.49" target="#b27">(Kwon et al., 2023)</ref> with large batch sizes as our inference backend to accelerate the inference speed. (3) Thirdly, we carefully design a scheduling strategy for offloading models to CPUs and loading models back to GPUs, which achieves a near-optimal balance between the training speed and memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Results</head><p>Evaluations on Standard Benchmarks. Initially, we evaluate DeepSeek-V2 Chat (SFT) and DeepSeek-V2 Chat (RL) on standard benchmarks. Notably, DeepSeek-V2 Chat (SFT) demonstrates substantial improvements in GSM8K, MATH, and HumanEval evaluations compared with its base version. This progress can be attributed to the inclusion of our SFT data, which comprises a considerable volume of math and code related content. In addition, DeepSeek-V2 Chat (RL) further boosts the performance on math and code benchmarks. We show more code and math evaluations in Appendix F.</p><p>As for the comparisons with other models, we first compare DeepSeek-V2 Chat (SFT) with Qwen1.5 72B Chat, and find that DeepSeek-V2 Chat (SFT) surpasses Qwen1.5 72B Chat on almost all of English, math, and code benchmarks. On Chinese benchmarks, DeepSeek-V2 Chat (SFT) demonstrates slightly lower scores than Qwen1.5 72B Chat on multi-subject multiple-choice tasks, consistent with the performance observed from their base versions. When compared with the state-of-the-art open-source MoE model, Mixtral 8x22B Instruct, DeepSeek-V2 Chat (SFT) exhibits better performance on most benchmarks, except for NaturalQuestions and IFEval. Furthermore, in comparison to the state-of-the-art open-source model LLaMA3 70B Chat, DeepSeek-V2 Chat (SFT) shows similar performance in code and math related benchmarks. LLaMA3 70B Chat exhibits better performance on MMLU and IFEval, while DeepSeek-V2 Chat (SFT) showcases stronger performance on Chinese tasks. Ultimately, DeepSeek-V2 Chat (RL) demonstrates further enhanced performance in both mathematical and coding tasks compared with DeepSeek-V2 Chat (SFT). These comparisons highlight the strengths of DeepSeek-V2 Chat in relation to other language models in various domains and languages.</p><p>Evaluations on Open-Ended Generation. We proceed with additional evaluations of our models on open-ended conversation benchmarks. For English open-ended conversation generation, we utilize MT-Bench and AlpacaEval 2.0 as the benchmarks. Evaluation results presented in Table <ref type="table" coords="18,112.80,591.13,70.39,10.49" target="#tab_6">4 demonstrate</ref>   72B Chat on both Chinese reasoning and language. Moreover, both DeepSeek-V2 Chat (SFT) and DeepSeek-V2 Chat (RL) outperform GPT-4-0613 and ERNIEBot 4.0, solidifying the position of our models in the top-tier LLMs that support Chinese. Specifically, DeepSeek-V2 Chat (RL) shows remarkable performance in Chinese language understanding, which outperforms all models including GPT-4-Turbo-1106-Preview. On the other hand, the reasoning capability of DeepSeek-V2 Chat (RL) still lags behind giant models, such as Erniebot-4.0 and GPT-4s. Table <ref type="table" coords="20,98.67,407.77,5.42,10.49" target="#tab_4">5</ref> | AlignBench leaderboard rated by GPT-4-0613. Models are ranked in descending order based on the overall score. Models marked with * represent that we evaluate them through their API service or open-weighted model, instead of referring to the results reported in their original papers. Suffixes of Erniebot-4.0 and Moonshot denote the timestamps when we called their API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>Amount of SFT Data. The discussion surrounding the necessity of a large SFT corpus has been a topic of intense debate. Previous works <ref type="bibr" coords="20,269.50,519.23,89.46,10.49" target="#b54">(Young et al., 2024;</ref><ref type="bibr" coords="20,361.69,519.23,81.79,10.49" target="#b61">Zhou et al., 2024)</ref> argue that fewer than 10K instances of SFT data are enough to produce satisfactory results. However, in our experiments, we observe a significant performance decline on the IFEval benchmark if we use fewer than 10K instances. A possible explanation is that, a language model necessitates a certain amount of data to develop specific skills. Although the requisite data amount may diminish with the model size increasing, it cannot be entirely eliminated. Our observation underscores the critical need for sufficient data to equip an LLM with desired capabilities. Moreover, the quality of SFT data is also crucial, especially for tasks involving writing or open-ended questions.</p><p>Alignment Tax of Reinforcement Learning. During human preference alignment, we observe a significant performance enhancement on the open-ended generation benchmarks, in terms of the scores rated by both AI and human evaluators. However, we also notice a phenomenon of "alignment tax" <ref type="bibr" coords="20,164.27,690.31,98.43,10.49" target="#b38">(Ouyang et al., 2022)</ref>, i.e., the alignment process can negatively impact the performance on some standard benchmarks such as BBH. In order to alleviate the alignment tax, during the RL stage, we make significant efforts in data processing and improving training strategies, finally achieving a tolerable trade-off between the performance on standard and open-ended benchmarks. Exploring how to align a model with human preferences without compromising its general performance presents a valuable direction for future research.</p><p>Online Reinforcement Learning. In our preference alignment experiments, we find that the online approach significantly outperforms the offline approach. Therefore, we invest tremendous efforts in implementing an online RL framework for aligning DeepSeek-V2. The conclusion about online or offline preference alignment can vary in different contexts, and we reserve a more thorough comparison and analysis between them for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion, Limitation, and Future Work</head><p>In this paper, we introduce DeepSeek-V2, a large MoE language model that supports 128K context length. In addition to strong performance, it is also characterized by economical training and efficient inference, benefiting from its innovative architecture including MLA and DeepSeekMoE. In practice, compared with DeepSeek DeepSeek-V2 and its chat versions share the acknowledged limitations commonly found in other LLMs, including the lack of ongoing knowledge updates after pre-training, the possibility of generating non-factual information such as unverified advice, and a chance to produce hallucinations. In addition, since our data primarily consist of Chinese and English content, our model may exhibit limited proficiency in other languages. In scenarios beyond Chinese and English, it should be used with caution.</p><p>DeepSeek will continuously invest in open-source large models with longtermism, aiming to progressively approach the goal of artificial general intelligence.</p><p>• In our ongoing exploration, we are dedicated to devising methods that enable further scaling up MoE models while maintaining economical training and inference costs. The goal of our next step is to achieve performance on par with GPT-4 in our upcoming release. • Our alignment team continuously strives to enhance our models, aiming to develop a model that is not only helpful but also honest and safe for worldwide users. Our ultimate objective is to align the values of our model with human values, while minimizing the need for human supervision. By prioritizing ethical considerations and responsible development, we are dedicated to creating a positive and beneficial impact on society. • Currently, DeepSeek-V2 is designed to support the text modality exclusively. In our forward-looking agenda, we intend to enable our model to support multiple modalities, enhancing its versatility and utility in a wider range of scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DeepSeek-V2-Lite</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Performance Evaluation</head><p>Base Model. We evaluate the performance of DeepSeek-V2-Lite and compare it with our previous small-size base models in Table <ref type="table" coords="30,251.17,611.62,4.04,10.49" target="#tab_10">6</ref>. DeepSeek-V2-Lite exhibits overwhelming performance advantages, especially in reasoning, coding, and math.</p><p>Chat Model. We evaluate the performance of DeepSeek-V2-Lite Chat and compare it with our previous small-size chat models in Table <ref type="table" coords="30,271.62,674.30,4.17,10.49" target="#tab_11">7</ref>. DeepSeek-V2-Lite also outperforms our previous small-size chat models by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Full Formulas of MLA</head><p>In order to demonstrate the complete computation process of MLA, we provide its full formulas in the following:</p><formula xml:id="formula_41" coords="31,269.76,150.72,255.74,11.72">c 𝑄 𝑡 = 𝑊 𝐷𝑄 h 𝑡 ,<label>(37)</label></formula><p>[q 𝐶 𝑡,1 ; q 𝐶 𝑡,2 ; ...; q 𝐶 𝑡,𝑛 ℎ ] = q 𝐶 𝑡 = 𝑊 𝑈𝑄 c 𝑄 𝑡 ,</p><p>[q 𝑅 𝑡,1 ; q 𝑅 𝑡,2 ; ...; q 𝑅 𝑡,𝑛 ℎ ] = q 𝑅 𝑡 = RoPE(𝑊 𝑄𝑅 c 𝑄 𝑡 ),</p><p>q 𝑡,𝑖 = [q 𝐶 𝑡,𝑖 ; q 𝑅 𝑡,𝑖 ], (40)</p><formula xml:id="formula_44" coords="31,261.89,227.70,263.61,11.17">c 𝐾𝑉 𝑡 = 𝑊 𝐷𝐾𝑉 h 𝑡 ,<label>(41)</label></formula><formula xml:id="formula_45" coords="31,174.87,247.32,350.63,12.12">[k 𝐶 𝑡,1 ; k 𝐶 𝑡,2 ; ...; k 𝐶 𝑡,𝑛 ℎ ] = k 𝐶 𝑡 = 𝑊 𝑈 𝐾 c 𝐾𝑉 𝑡 ,<label>(42)</label></formula><formula xml:id="formula_46" coords="31,265.33,268.97,260.17,11.17">k 𝑅 𝑡 = RoPE(𝑊 𝐾𝑅 h 𝑡 ),<label>(43)</label></formula><formula xml:id="formula_47" coords="31,266.50,288.58,259.00,11.33">k 𝑡,𝑖 = [k 𝐶 𝑡,𝑖 ; k 𝑅 𝑡 ],<label>(44)</label></formula><formula xml:id="formula_48" coords="31,177.27,306.47,348.22,12.12">[v 𝐶 𝑡,1 ; v 𝐶 𝑡,2 ; ...; v 𝐶 𝑡,𝑛 ℎ ] = v 𝐶 𝑡 = 𝑊 𝑈𝑉 c 𝐾𝑉 𝑡 ,<label>(45)</label></formula><formula xml:id="formula_49" coords="31,267.10,324.74,258.40,34.34">o 𝑡,𝑖 = 𝑡 ∑︁ 𝑗=1 Softmax 𝑗 ( q 𝑇 𝑡,𝑖 k 𝑗,𝑖 √︃ 𝑑 ℎ + 𝑑 𝑅 ℎ )v 𝐶 𝑗,𝑖 ,<label>(46)</label></formula><formula xml:id="formula_50" coords="31,270.83,367.17,254.68,11.17">u 𝑡 = 𝑊 𝑂 [o 𝑡,1 ; o 𝑡,2 ; ...; o 𝑡,𝑛 ℎ ],<label>(47)</label></formula><p>where the boxed vectors in blue need to be cached for generation. During inference, the naive formula needs to recover k 𝐶 𝑡 and v 𝐶 𝑡 from c 𝐾𝑉 𝑡 for attention. Fortunately, due to the associative law of matrix multiplication, we can absorb 𝑊 𝑈 𝐾 into 𝑊 𝑈𝑄 , and 𝑊 𝑈𝑉 into 𝑊 𝑂 . Since this optimization is related to only model parameters, it can be completed offline at once. Through this optimization, we avoid the computational overhead for recomputing k 𝐶 𝑡 and v 𝐶 𝑡 during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation of Attention Mechanisms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Ablation of MHA, GQA, and MQA</head><p>We show the evaluation results for 7B dense models with MHA, GQA, and MQA on four hard benchmarks in Table <ref type="table" coords="31,170.60,558.47,4.01,10.49" target="#tab_12">8</ref>. All of these three models are trained on 1.33T tokens, and share the same architecture except for the attention mechanisms. In addition, for a fair comparison, we align the number of parameters of them to around 7B by adjusting the number of layers. From the table, we can find that MHA demonstrates significant advantages over GQA and MQA on these benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Comparison Between MLA and MHA</head><p>In Table <ref type="table" coords="31,113.81,668.09,4.17,10.49" target="#tab_13">9</ref>, we show the evaluation results for MoE models equipped with MLA and MHA, respectively, on four hard benchmarks. For a solid conclusion, we train and evaluate models across two scales. Two small MoE models comprise about 16B total parameters, and we train them on 1.33T tokens. Two large MoE models comprise about 250B total parameters, and we train them on 420B tokens. Also, two small MoE models and two large MoE models respectively share the same architecture except for the attention mechanisms. From the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion About Pre-Training Data Debiasing</head><p>During pre-training data preparation, we identify and filter out contentious content, such as values influenced by regional cultures, to avoid our model exhibiting unnecessary subjective biases on these controversial topics. Consequently, we observe that DeepSeek-V2 performs slightly worse on the test sets that are closely associated with specific regional cultures. For example, when evaluated on MMLU, although DeepSeek-V2 achieves comparable or superior performance on the majority of testsets compared with its competitors like Mixtral 8x22B, it still lags behind on the Humanity-Moral subset, which is mainly associated with American values.</p><p>Further, we conduct a manual analysis on this subset. Three well-educated human annotators conduct independent annotations on 420 moral scenarios from the MMLU Humanity-Moral subset. Then, we compute the agreement among their annotations and the ground-truth label. As shown in Table <ref type="table" coords="32,145.86,668.97,9.00,10.49" target="#tab_2">10</ref>, three human annotators and the ground-truth label exhibit a low agreement with each other. Therefore, we attribute the abnormal performance of DeepSeek-V2 on these value-sensitive test sets to our efforts in debiasing the pre-training corpus. Table <ref type="table" coords="33,97.80,176.95,10.69,10.49" target="#tab_2">10</ref> | Three well-educated human annotators conduct independent annotations on 420 moral scenarios from the MMLU Humanity-Moral subset, on which DeepSeek-V2 and its competitive models demonstrate performance inconsistency. Three annotators and the ground-truth label exhibit a low agreement with each other. This indicates that the answers to the Humanity-Moral subset can be contentious according to specific regional cultures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agreement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Evaluations on Math and Code</head><p>The evaluation employs the SC-Math6 We further share more results in Figure <ref type="figure" coords="33,283.84,654.52,5.56,10.49" target="#fig_7">5</ref> on HumanEval and LiveCodeBench, where the questions of LiveCodeBench are selected from the period between September 1st, 2023, and April 1st, 2024. As shown in the figure, DeepSeek-V2 Chat (RL) demonstrates considerable proficiency in LiveCodeBench, achieving a Pass@1 score that even surpasses some giant models. This performance highlights the strong capability of DeepSeek-V2 Chat (RL) in tackling live coding tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Evaluation Formats</head><p>We present our evaluation formats for each benchmark in  Remember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from highest priority to lowest priority is "not", "and", "or", respectively. We first simplify this expression "Z" as follows: "Z = not ( ( not not True ) ) = not ( ( A ) )" where "A = not not True". Let's evaluate A: A = not not True = not (not True) = not False = True. Plugging in A, we get: Z = not ( ( A ) ) = not ( ( True ) ) = not True = False. So the answer is False.</p><p>Q: True and False and not True and True is A: Let's think step by step. Remember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from highest priority to lowest priority is "not", "and", "or", respectively. We first simplify this expression "Z" as follows: Remember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from highest priority to lowest priority is "not", "and", "or", respectively. We first simplify this expression "Z" as follows: "Z = not not ( not ( False ) ) = not not ( A )" where "A = not ( False )". Let's evaluate A: A = not ( False ) = not False = True. Plugging in A, we get: Z = not not ( A ) = not not (True) = not not False = True. So the answer is True.</p><p>Q: False and False and False or not False is A: Let's think step by step.   The gender makeup of the city was 64.3% male and 35.7% female.</p><formula xml:id="formula_51" coords="39,127.56,237.74,340.16,81.12">PROMPT Q: 某 小 学 在"献 爱 心-为 汶 川 地 震 区 捐 款"活 动 中 ， 六 年 级 五 个 班 共 捐 款8000元 ， 其 中 一 班 捐 款1500元 ， 二 班 比 一 班 多 捐 款200元 ， 三 班 捐 款1600元，四班与五班捐款数之比是3：5．四班捐款多少元？ A: 一 班 捐 款1500元 ， 而 二 班 比 一 班 多 捐200元 ， 所 以 二 班 捐 款1500+200=1700元 ， 又 知 道 六 年 级 五 个 班 一 共 捐 款8000元 ， 所 以 四 班 和 五 班 捐 款 之 和= 一 共 捐 款-一 班 和 二 班 和 三 班 捐 款 之 和 ， 即8000-</formula><p>Answer the following questions based on the above passage, please calculate carefully if calculation is necessary. Q: How many percent were not from 25 to 44?</p><p>A: The answer type is number. So according to above Passage, the answer is 83.9.</p><p>Q: How many in percent weren't 25 to 44?</p><p>A: The answer type is number. So according to above Passage, the answer is   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OPTIONS</head><p>-is playing the piano with his hands and his face.</p><p>-bigins to play a song by timbaland on the piano.</p><p>-plays slowly, and pauses to snap his fingers.</p><p>-is playing a song in front of him.   PROMPT A woman notices that she is depressed every autumn, and wonders why. A friend suggests to her that perhaps certain changes that take place as seasons move from warm to cold may be having an effect on her. When pressed for an example of these changes, the friend cites</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OPTIONS</head><p>-flowers blooming -grass turning brown -trees growing -blossoms blooming Table <ref type="table" coords="48,229.59,532.15,10.91,10.49" target="#tab_5">31</ref> | An example of OpenBookQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROMPT</head><p>To make it easier to push the reset button of the garbage disposable machine which is located underneath the machine,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OPTIONS</head><p>-place a wall mirror on the floor of the cabinet -hold a hand mirror under the garbage disposable machine </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,70.87,727.44,455.45,10.79;1,70.51,741.29,413.71,10.49"><head>Figure 1 |</head><label>1</label><figDesc>Figure 1 | (a) MMLU accuracy vs. activated parameters, among different open-source models. (b) Training costs and inference efficiency of DeepSeek 67B (Dense) and DeepSeek-V2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,504.92,500.04,20.85,10.49;4,70.87,513.59,436.25,10.49"><head></head><label></label><figDesc>(a)), economical training costs, and efficient inference throughput (Figure 1(b)), simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,70.87,457.93,453.97,10.79;5,70.87,471.77,453.54,10.49;5,70.87,485.32,296.16,10.49"><head>Figure 2 |</head><label>2</label><figDesc>Figure 2 | Illustration of the architecture of DeepSeek-V2. MLA ensures efficient inference by significantly reducing the KV cache for generation, and DeepSeekMoE enables training strong models at an economical cost through the sparse architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,70.87,218.01,455.35,10.79;7,70.87,231.86,453.55,10.49;7,70.87,245.41,453.94,10.49;7,70.87,258.96,112.18,10.49"><head>Figure 3 |</head><label>3</label><figDesc>Figure 3 | Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,70.87,328.41,453.55,10.79;13,70.54,342.25,291.35,10.49"><head>Figure 4 |</head><label>4</label><figDesc>Figure 4 | Evaluation results on the "Needle In A Haystack" (NIAH) tests. DeepSeek-V2 performs well across all context window lengths up to 128K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="16,211.50,166.68,46.85,10.09;16,70.87,188.87,453.55,10.49;16,70.87,202.42,453.55,10.49;16,70.87,215.97,455.36,10.49;16,70.87,229.52,453.77,10.49;16,70.87,243.07,453.55,10.49;16,70.87,256.62,453.93,10.49;16,70.87,270.17,453.55,10.49;16,70.87,283.72,300.29,10.49"><head></head><label></label><figDesc>EfficiencyTraining Costs. Since DeepSeek-V2 activates fewer parameters for each token and requires fewer FLOPs than DeepSeek 67B, training DeepSeek-V2 will be more economical than training DeepSeek 67B theoretically. Although training an MoE model will introduce additional communication overheads, through our operator and communication optimizations, the training for DeepSeek-V2 can attain a relatively high Model FLOPs Utilization (MFU). During our practical training on the H800 cluster, for training on each trillion tokens, DeepSeek 67B requires 300.6K GPU hours, while DeepSeek-V2 needs only 172.8K GPU hours, i.e., sparse DeepSeek-V2 can save 42.5% training costs compared with dense DeepSeek 67B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="18,186.26,591.13,338.37,10.49;18,70.87,604.68,455.36,10.49;18,70.87,618.23,454.63,10.49;18,70.87,631.78,453.55,10.49;18,70.87,645.33,453.54,10.49;18,70.87,658.88,453.55,10.49;18,70.87,672.43,453.97,10.49;18,70.87,685.98,428.36,10.49;18,87.80,706.30,438.52,10.49;18,70.44,719.85,75.42,10.49"><head></head><label></label><figDesc>a significant performance advantage of DeepSeek-V2 Chat (RL) over DeepSeek-V2 Chat (SFT). This outcome showcases the effectiveness of our RL training in achieving improved alignment. In comparison to other open-source models, DeepSeek-V2 Chat (RL) demonstrates superior performance over Mistral 8x22B Instruct and Qwen1.5 72B Chat on both benchmarks. When compared with LLaMA3 70B Instruct, DeepSeek-V2 Chat (RL) showcases competitive performance on MT-Bench and notably outperforms it on AlpacaEval 2.0. These results highlight the strong performance of DeepSeek-V2 Chat (RL) in generating high-quality and contextually relevant responses, particularly in instruction-based conversation tasks. In addition, we evaluate the Chinese open-ended generation capability based on AlignBench. As presented in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="34,70.87,368.90,455.35,10.79;34,70.87,382.74,429.62,10.49"><head>Figure 5 |</head><label>5</label><figDesc>Figure 5 | Evaluation results on HumanEval and LiveCodeBench. The questions of Live-CodeBench are selected from the period between September 1st, 2023 and April 1st, 2024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="36,448.68,399.42,19.03,9.58;36,127.25,411.37,340.47,9.58;36,127.56,423.33,340.15,9.58;36,127.56,435.28,341.91,9.58;36,127.56,447.24,340.16,9.58;36,127.56,459.19,69.03,9.58;36,127.56,483.10,119.00,9.58;36,127.56,495.06,117.08,9.58"><head></head><label></label><figDesc>"Z = True and False and not True and True = A and B" where "A = True and False" and "B = not True and True". Let's evaluate A: A = True and False = False. Let's evaluate B: B = not True and True = not (True and True) = not (True) = False. Plugging in A and B, we get: Z = A and B = False and False = False. So the answer is False. Q: not not ( not ( False ) ) is A: Let's think step by step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,85.26,91.11,424.76,78.74"><head>Attention Mechanism KV Cache per Token (# Element) Capability</head><label></label><figDesc></figDesc><table coords="9,85.26,110.05,421.76,59.81"><row><cell>Multi-Head Attention (MHA)</cell><cell>2𝑛 ℎ 𝑑 ℎ 𝑙</cell><cell>Strong</cell></row><row><cell>Grouped-Query Attention (GQA)</cell><cell>2𝑛 𝑔 𝑑 ℎ 𝑙</cell><cell>Moderate</cell></row><row><cell>Multi-Query Attention (MQA)</cell><cell>2𝑑 ℎ 𝑙</cell><cell>Weak</cell></row><row><cell>MLA (Ours)</cell><cell>(𝑑 𝑐 + 𝑑 𝑅 ℎ )𝑙 ≈ 9 2 𝑑 ℎ 𝑙</cell><cell>Stronger</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,70.53,182.39,455.24,92.08"><head>Table 1 |</head><label>1</label><figDesc>Comparison of the KV cache per token among different attention mechanisms. 𝑛 ℎ denotes the number of attention heads, 𝑑 ℎ denotes the dimension per attention head, 𝑙 denotes the number of layers, 𝑛 𝑔 denotes the number of groups in GQA, and 𝑑 𝑐 and 𝑑 𝑅 ℎ denote the KV compression dimension and the per-head dimension of the decoupled queries and key in MLA, respectively. The amount of KV cache is measured by the number of elements, regardless of the storage precision. For DeepSeek-V2, 𝑑 𝑐 is set to 4𝑑 ℎ and 𝑑 𝑅 ℎ is set to 𝑑 ℎ 2 . So, its KV cache is equal to GQA with only 2.25 groups, but its performance is stronger than MHA.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="15,70.53,90.61,453.89,466.83"><head>Table 2 |</head><label>2</label><figDesc>Comparison among DeepSeek-V2 and other representative open-source models. All models are evaluated in our internal framework and share the same evaluation setting. Bold denotes the best and underline denotes the second-best. Scores with a gap smaller than 0.3 are regarded as at the same level. With only 21B activated parameters, DeepSeek-V2 achieves top-tier performance among open-source models.</figDesc><table coords="15,83.57,90.61,428.14,386.60"><row><cell></cell><cell>Benchmark (Metric)</cell><cell># Shots</cell><cell cols="5">DeepSeek Qwen1.5 Mixtral LLaMA 3 DeepSeek-V2 67B 72B 8x22B 70B</cell></row><row><cell></cell><cell>Architecture</cell><cell>-</cell><cell>Dense</cell><cell>Dense</cell><cell>MoE</cell><cell>Dense</cell><cell>MoE</cell></row><row><cell></cell><cell># Activated Params</cell><cell>-</cell><cell>67B</cell><cell>72B</cell><cell>39B</cell><cell>70B</cell><cell>21B</cell></row><row><cell></cell><cell># Total Params</cell><cell>-</cell><cell>67B</cell><cell>72B</cell><cell>141B</cell><cell>70B</cell><cell>236B</cell></row><row><cell></cell><cell>Pile-test (BPB)</cell><cell>-</cell><cell>0.642</cell><cell>0.637</cell><cell>0.623</cell><cell>0.602</cell><cell>0.606</cell></row><row><cell></cell><cell>BBH (EM)</cell><cell>3-shot</cell><cell>68.7</cell><cell>59.9</cell><cell>78.9</cell><cell>81.0</cell><cell>78.9</cell></row><row><cell></cell><cell>MMLU (Acc.)</cell><cell>5-shot</cell><cell>71.3</cell><cell>77.2</cell><cell>77.6</cell><cell>78.9</cell><cell>78.5</cell></row><row><cell></cell><cell>DROP (F1)</cell><cell>3-shot</cell><cell>69.7</cell><cell>71.5</cell><cell>80.4</cell><cell>82.5</cell><cell>80.1</cell></row><row><cell></cell><cell>ARC-Easy (Acc.)</cell><cell>25-shot</cell><cell>95.3</cell><cell>97.1</cell><cell>97.3</cell><cell>97.9</cell><cell>97.6</cell></row><row><cell></cell><cell>ARC-Challenge (Acc.)</cell><cell>25-shot</cell><cell>86.4</cell><cell>92.8</cell><cell>91.2</cell><cell>93.3</cell><cell>92.4</cell></row><row><cell></cell><cell>HellaSwag (Acc.)</cell><cell>10-shot</cell><cell>86.3</cell><cell>85.8</cell><cell>86.6</cell><cell>87.9</cell><cell>84.2</cell></row><row><cell>English</cell><cell>PIQA (Acc.) WinoGrande (Acc.)</cell><cell>0-shot 5-shot</cell><cell>83.6 84.9</cell><cell>83.3 82.4</cell><cell>83.6 83.7</cell><cell>85.0 85.7</cell><cell>83.7 84.9</cell></row><row><cell></cell><cell>RACE-Middle (Acc.)</cell><cell>5-shot</cell><cell>69.9</cell><cell>63.4</cell><cell>73.3</cell><cell>73.3</cell><cell>73.1</cell></row><row><cell></cell><cell>RACE-High (Acc.)</cell><cell>5-shot</cell><cell>50.7</cell><cell>47.0</cell><cell>56.7</cell><cell>57.9</cell><cell>52.7</cell></row><row><cell></cell><cell>TriviaQA (EM)</cell><cell>5-shot</cell><cell>78.9</cell><cell>73.1</cell><cell>82.1</cell><cell>81.6</cell><cell>79.9</cell></row><row><cell></cell><cell cols="2">NaturalQuestions (EM) 5-shot</cell><cell>36.6</cell><cell>35.6</cell><cell>39.6</cell><cell>40.2</cell><cell>38.7</cell></row><row><cell></cell><cell>AGIEval (Acc.)</cell><cell>0-shot</cell><cell>41.3</cell><cell>64.4</cell><cell>43.4</cell><cell>49.8</cell><cell>51.2</cell></row><row><cell></cell><cell>HumanEval (Pass@1)</cell><cell>0-shot</cell><cell>45.1</cell><cell>43.9</cell><cell>53.1</cell><cell>48.2</cell><cell>48.8</cell></row><row><cell>Code</cell><cell>MBPP (Pass@1) CRUXEval-I (Acc.)</cell><cell>3-shot 2-shot</cell><cell>57.4 42.5</cell><cell>53.6 44.3</cell><cell>64.2 52.4</cell><cell>68.6 49.4</cell><cell>66.6 52.8</cell></row><row><cell></cell><cell>CRUXEval-O (Acc.)</cell><cell>2-shot</cell><cell>41.0</cell><cell>42.3</cell><cell>52.8</cell><cell>54.3</cell><cell>49.8</cell></row><row><cell></cell><cell>GSM8K (EM)</cell><cell>8-shot</cell><cell>63.4</cell><cell>77.9</cell><cell>80.3</cell><cell>83.0</cell><cell>79.2</cell></row><row><cell>Math</cell><cell>MATH (EM)</cell><cell>4-shot</cell><cell>18.7</cell><cell>41.4</cell><cell>42.5</cell><cell>42.2</cell><cell>43.6</cell></row><row><cell></cell><cell>CMath (EM)</cell><cell>3-shot</cell><cell>63.0</cell><cell>77.8</cell><cell>72.3</cell><cell>73.9</cell><cell>78.7</cell></row><row><cell></cell><cell>CLUEWSC (EM)</cell><cell>5-shot</cell><cell>81.0</cell><cell>80.5</cell><cell>77.5</cell><cell>78.3</cell><cell>82.2</cell></row><row><cell></cell><cell>C-Eval (Acc.)</cell><cell>5-shot</cell><cell>66.1</cell><cell>83.7</cell><cell>59.6</cell><cell>67.5</cell><cell>81.7</cell></row><row><cell></cell><cell>CMMLU (Acc.)</cell><cell>5-shot</cell><cell>70.8</cell><cell>84.3</cell><cell>60.0</cell><cell>69.3</cell><cell>84.0</cell></row><row><cell>Chinese</cell><cell>CMRC (EM)</cell><cell>1-shot</cell><cell>73.4</cell><cell>66.6</cell><cell>73.1</cell><cell>73.3</cell><cell>77.5</cell></row><row><cell></cell><cell>C3 (Acc.)</cell><cell>0-shot</cell><cell>75.3</cell><cell>78.2</cell><cell>71.4</cell><cell>74.0</cell><cell>77.4</cell></row><row><cell></cell><cell>CHID (Acc.)</cell><cell>0-shot</cell><cell>92.1</cell><cell>-</cell><cell>57.0</cell><cell>83.2</cell><cell>92.7</cell></row><row><cell></cell><cell>CCPM (Acc.)</cell><cell>0-shot</cell><cell>88.5</cell><cell>88.1</cell><cell>61.0</cell><cell>68.1</cell><cell>93.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="18,70.87,719.85,453.55,37.59"><head>Table 5 ,</head><label>5</label><figDesc>DeepSeek-V2 Chat (RL) exhibits a slight advantage over DeepSeek-V2 Chat (SFT). Notably, DeepSeek-V2 Chat (SFT) surpasses all open-source Chinese models by a significant margin. It significantly outperforms the second-best open-source model, Qwen1.5</figDesc><table coords="19,74.45,90.61,444.39,298.93"><row><cell></cell><cell>Benchmark</cell><cell># Shots</cell><cell cols="6">DeepSeek Qwen 1.5 LLaMA3 Mixtral DeepSeek-V2 DeepSeek-V2 67B Chat 72B Chat 70B Inst. 8x22B Inst. Chat (SFT) Chat (RL)</cell></row><row><cell></cell><cell cols="2">Context Length -</cell><cell>4K</cell><cell>32K</cell><cell>8K</cell><cell>64K</cell><cell>128K</cell><cell>128K</cell></row><row><cell></cell><cell>Architecture</cell><cell>-</cell><cell>Dense</cell><cell>Dense</cell><cell>Dense</cell><cell>MoE</cell><cell>MoE</cell><cell>MoE</cell></row><row><cell></cell><cell cols="2"># Activated Params -</cell><cell>67B</cell><cell>72B</cell><cell>70B</cell><cell>39B</cell><cell>21B</cell><cell>21B</cell></row><row><cell></cell><cell># Total Params</cell><cell>-</cell><cell>67B</cell><cell>72B</cell><cell>70B</cell><cell>141B</cell><cell>236B</cell><cell>236B</cell></row><row><cell></cell><cell>TriviaQA</cell><cell>5-shot</cell><cell>81.5</cell><cell>79.6</cell><cell>69.1</cell><cell>80.0</cell><cell>85.4</cell><cell>86.7</cell></row><row><cell></cell><cell cols="2">NaturalQuestions 5-shot</cell><cell>47.0</cell><cell>46.9</cell><cell>44.6</cell><cell>54.9</cell><cell>51.9</cell><cell>53.4</cell></row><row><cell></cell><cell>MMLU</cell><cell>5-shot</cell><cell>71.1</cell><cell>76.2</cell><cell>80.3</cell><cell>77.8</cell><cell>78.4</cell><cell>77.8</cell></row><row><cell>English</cell><cell cols="2">ARC-Easy ARC-Challenge 25-shot 25-shot</cell><cell>96.6 88.9</cell><cell>96.8 91.7</cell><cell>96.9 92.6</cell><cell>97.1 90.0</cell><cell>97.6 92.5</cell><cell>98.1 92.3</cell></row><row><cell></cell><cell>BBH</cell><cell>3-shot</cell><cell>71.7</cell><cell>65.9</cell><cell>80.1</cell><cell>78.4</cell><cell>81.3</cell><cell>79.7</cell></row><row><cell></cell><cell>AGIEval</cell><cell>0-shot</cell><cell>46.4</cell><cell>62.8</cell><cell>56.6</cell><cell>41.4</cell><cell>63.2</cell><cell>61.4</cell></row><row><cell></cell><cell>IFEval</cell><cell>0-shot</cell><cell>55.5</cell><cell>57.3</cell><cell>79.7</cell><cell>72.1</cell><cell>64.1</cell><cell>63.8</cell></row><row><cell></cell><cell>HumanEval</cell><cell>0-shot</cell><cell>73.8</cell><cell>68.9</cell><cell>76.2</cell><cell>75.0</cell><cell>76.8</cell><cell>81.1</cell></row><row><cell>Code</cell><cell cols="2">MBPP CRUXEval-I-COT 2-shot 3-shot</cell><cell>61.4 49.1</cell><cell>52.2 51.4</cell><cell>69.8 61.1</cell><cell>64.4 59.4</cell><cell>70.4 59.5</cell><cell>72.0 61.5</cell></row><row><cell></cell><cell cols="2">CRUXEval-O-COT 2-shot</cell><cell>50.9</cell><cell>56.5</cell><cell>63.6</cell><cell>63.6</cell><cell>60.7</cell><cell>63.0</cell></row><row><cell></cell><cell cols="2">LiveCodeBench 0-shot</cell><cell>18.3</cell><cell>18.8</cell><cell>30.5</cell><cell>25.0</cell><cell>28.7</cell><cell>32.5</cell></row><row><cell></cell><cell>GSM8K</cell><cell>8-shot</cell><cell>84.1</cell><cell>81.9</cell><cell>93.2</cell><cell>87.9</cell><cell>90.8</cell><cell>92.2</cell></row><row><cell>Math</cell><cell>MATH CMath</cell><cell>4-shot 0-shot</cell><cell>32.6 80.3</cell><cell>40.6 82.8</cell><cell>48.5 79.2</cell><cell>49.8 75.1</cell><cell>52.7 82.0</cell><cell>53.9 81.9</cell></row><row><cell></cell><cell>CLUEWSC</cell><cell>5-shot</cell><cell>78.5</cell><cell>90.1</cell><cell>85.4</cell><cell>75.8</cell><cell>88.6</cell><cell>89.9</cell></row><row><cell>Chinese</cell><cell>C-Eval</cell><cell>5-shot</cell><cell>65.2</cell><cell>82.2</cell><cell>67.9</cell><cell>60.0</cell><cell>80.9</cell><cell>78.0</cell></row><row><cell></cell><cell>CMMLU</cell><cell>5-shot</cell><cell>67.8</cell><cell>82.9</cell><cell>70.7</cell><cell>61.0</cell><cell>82.4</cell><cell>81.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="19,70.41,404.78,454.22,181.18"><head>Table 3 |</head><label>3</label><figDesc>Comparison among DeepSeek-V2 Chat (SFT), DeepSeek-V2 Chat (RL), and other representative open-source chat models. Regarding TriviaQA and NaturalQuestions, it is worth noting that chat models, such as LLaMA3 70B Instruct, might not strictly adhere to the format constraints typically specified in the few-shot setting. Consequently, this can lead to underestimation of certain models in our evaluation framework.</figDesc><table coords="19,158.93,488.78,277.41,97.17"><row><cell>Model</cell><cell cols="2">MT-Bench AlpacaEval 2.0</cell></row><row><cell>DeepSeek 67B Chat</cell><cell>8.35</cell><cell>16.6</cell></row><row><cell>Mistral 8x22B Instruct v0.1</cell><cell>8.66</cell><cell>30.9</cell></row><row><cell>Qwen1.5 72B Chat</cell><cell>8.61</cell><cell>36.6</cell></row><row><cell>LLaMA3 70B Instruct</cell><cell>8.95</cell><cell>34.4</cell></row><row><cell>DeepSeek-V2 Chat (SFT)</cell><cell>8.62</cell><cell>30.0</cell></row><row><cell>DeepSeek-V2 Chat (RL)</cell><cell>8.97</cell><cell>38.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="19,70.53,601.42,455.69,24.34"><head>Table 4 |</head><label>4</label><figDesc>English open-ended conversation evaluations. For AlpacaEval 2.0, we use the lengthcontrolled win rate as the metric.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="20,72.93,88.23,449.44,304.69"><head>Math. Logi. Avg. Fund. Chi. Open. Writ. Role. Pro.</head><label></label><figDesc></figDesc><table coords="20,72.93,88.23,449.44,304.69"><row><cell>Model</cell><cell>Overall</cell><cell cols="3">Reasoning 中文推理</cell><cell></cell><cell></cell><cell cols="3">Language 中文语言</cell></row><row><cell cols="3">Avg. 模型 推理 总分 总分</cell><cell>数学 计算</cell><cell>逻辑 推理</cell><cell>语言 总分</cell><cell>基本 任务</cell><cell>中文 理解</cell><cell>综合 问答</cell><cell>文本 写作</cell><cell>角色 扮演</cell><cell>专业 能力</cell></row><row><cell>GPT-4-1106-Preview</cell><cell>8.01</cell><cell>7.73</cell><cell>7.80</cell><cell cols="2">7.66 8.29</cell><cell cols="2">7.99 7.33</cell><cell>8.61</cell><cell cols="2">8.67 8.47 8.65</cell></row><row><cell>DeepSeek-V2 Chat (RL)</cell><cell>7.91</cell><cell>7.45</cell><cell>7.77</cell><cell cols="2">7.14 8.36</cell><cell cols="2">8.10 8.28</cell><cell>8.37</cell><cell cols="2">8.53 8.33 8.53</cell></row><row><cell>ERNIEBot-4.0-202404*(文心一言)</cell><cell>7.89</cell><cell>7.61</cell><cell>7.81</cell><cell cols="2">7.41 8.17</cell><cell cols="2">7.56 8.53</cell><cell>8.13</cell><cell cols="2">8.45 8.24 8.09</cell></row><row><cell>DeepSeek-V2 Chat (SFT)</cell><cell>7.74</cell><cell>7.30</cell><cell>7.34</cell><cell cols="2">7.26 8.17</cell><cell cols="2">8.04 8.26</cell><cell>8.13</cell><cell cols="2">8.00 8.10 8.49</cell></row><row><cell>GPT-4-0613</cell><cell>7.53</cell><cell>7.47</cell><cell>7.56</cell><cell cols="2">7.37 7.59</cell><cell cols="2">7.81 6.93</cell><cell>7.42</cell><cell cols="2">7.93 7.51 7.94</cell></row><row><cell>ERNIEBot-4.0-202312*(文心一言)</cell><cell>7.36</cell><cell>6.84</cell><cell>7.00</cell><cell cols="2">6.67 7.88</cell><cell cols="2">7.47 7.88</cell><cell>8.05</cell><cell cols="2">8.19 7.84 7.85</cell></row><row><cell>Moonshot-v1-32k-202404*(月之暗面)</cell><cell>7.22</cell><cell>6.42</cell><cell>6.41</cell><cell cols="2">6.43 8.02</cell><cell cols="2">7.82 7.58</cell><cell>8.00</cell><cell cols="2">8.22 8.19 8.29</cell></row><row><cell>Qwen1.5-72B-Chat*</cell><cell>7.19</cell><cell>6.45</cell><cell>6.58</cell><cell cols="2">6.31 7.93</cell><cell cols="2">7.38 7.77</cell><cell>8.15</cell><cell cols="2">8.02 8.05 8.24</cell></row><row><cell>DeepSeek-67B-Chat</cell><cell>6.43</cell><cell>5.75</cell><cell>5.71</cell><cell cols="2">5.79 7.11</cell><cell cols="2">7.12 6.52</cell><cell>7.58</cell><cell cols="2">7.20 6.91 7.37</cell></row><row><cell>ChatGLM-Turbo(智谱清言)</cell><cell>6.24</cell><cell>5.00</cell><cell>4.74</cell><cell cols="2">5.26 7.49</cell><cell cols="2">6.82 7.17</cell><cell>8.16</cell><cell cols="2">7.77 7.76 7.24</cell></row><row><cell>ERNIEBot-3.5(文心一言)</cell><cell>6.14</cell><cell>5.15</cell><cell>5.03</cell><cell cols="2">5.27 7.13</cell><cell cols="2">6.62 7.60</cell><cell>7.26</cell><cell cols="2">7.56 6.83 6.90</cell></row><row><cell>Yi-34B-Chat*</cell><cell>6.12</cell><cell>4.86</cell><cell>4.97</cell><cell cols="2">4.74 7.38</cell><cell cols="2">6.72 7.28</cell><cell>7.76</cell><cell cols="2">7.44 7.58 7.53</cell></row><row><cell>GPT-3.5-Turbo-0613</cell><cell>6.08</cell><cell>5.35</cell><cell>5.68</cell><cell cols="2">5.02 6.82</cell><cell cols="2">6.71 5.81</cell><cell>7.29</cell><cell cols="2">7.03 7.28 6.77</cell></row><row><cell>ChatGLM-Pro(智谱清言)</cell><cell>5.83</cell><cell>4.65</cell><cell>4.54</cell><cell cols="2">4.75 7.01</cell><cell cols="2">6.51 6.76</cell><cell>7.47</cell><cell cols="2">7.07 7.34 6.89</cell></row><row><cell>SparkDesk-V2(讯飞星火)</cell><cell>5.74</cell><cell>4.73</cell><cell>4.71</cell><cell cols="2">4.74 6.76</cell><cell cols="2">5.84 6.97</cell><cell>7.29</cell><cell cols="2">7.18 6.92 6.34</cell></row><row><cell>Qwen-14B-Chat</cell><cell>5.72</cell><cell>4.81</cell><cell>4.91</cell><cell cols="2">4.71 6.63</cell><cell cols="2">6.90 6.36</cell><cell>6.74</cell><cell cols="2">6.64 6.59 6.56</cell></row><row><cell>Baichuan2-13B-Chat</cell><cell>5.25</cell><cell>3.92</cell><cell>3.76</cell><cell cols="2">4.07 6.59</cell><cell cols="2">6.22 6.05</cell><cell>7.11</cell><cell cols="2">6.97 6.75 6.43</cell></row><row><cell>ChatGLM3-6B</cell><cell>4.97</cell><cell>3.85</cell><cell>3.55</cell><cell cols="2">4.14 6.10</cell><cell cols="2">5.75 5.29</cell><cell>6.71</cell><cell cols="2">6.83 6.28 5.73</cell></row><row><cell>Baichuan2-7B-Chat</cell><cell>4.97</cell><cell>3.66</cell><cell>3.56</cell><cell cols="2">3.75 6.28</cell><cell cols="2">5.81 5.50</cell><cell>7.13</cell><cell cols="2">6.84 6.53 5.84</cell></row><row><cell>InternLM-20B</cell><cell>4.96</cell><cell>3.66</cell><cell>3.39</cell><cell cols="2">3.92 6.26</cell><cell cols="2">5.96 5.50</cell><cell>7.18</cell><cell cols="2">6.19 6.49 6.22</cell></row><row><cell>Qwen-7B-Chat</cell><cell>4.91</cell><cell>3.73</cell><cell>3.62</cell><cell cols="2">3.83 6.09</cell><cell cols="2">6.40 5.74</cell><cell>6.26</cell><cell cols="2">6.31 6.19 5.66</cell></row><row><cell>ChatGLM2-6B</cell><cell>4.48</cell><cell>3.39</cell><cell>3.16</cell><cell cols="2">3.61 5.58</cell><cell cols="2">4.91 4.52</cell><cell>6.66</cell><cell cols="2">6.25 6.08 5.08</cell></row><row><cell>InternLM-Chat-7B</cell><cell>3.65</cell><cell>2.56</cell><cell>2.45</cell><cell cols="2">2.66 4.75</cell><cell cols="2">4.34 4.09</cell><cell>5.82</cell><cell cols="2">4.89 5.32 4.06</cell></row><row><cell>Chinese-LLaMA-2-7B-Chat</cell><cell>3.57</cell><cell>2.68</cell><cell>2.29</cell><cell cols="2">3.07 4.46</cell><cell cols="2">4.31 4.26</cell><cell>4.50</cell><cell cols="2">4.63 4.91 4.13</cell></row><row><cell>LLaMA-2-13B-Chinese-Chat</cell><cell>3.35</cell><cell>2.47</cell><cell>2.21</cell><cell cols="2">2.73 4.23</cell><cell cols="2">4.13 3.31</cell><cell>4.79</cell><cell cols="2">3.93 4.53 4.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="21,70.54,280.39,454.30,64.69"><head></head><label></label><figDesc>67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. Evaluation results further demonstrate that with only 21B activated parameters, DeepSeek-V2 achieves top-tier performance among open-source models and becomes the strongest open-source MoE model.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="29,70.41,87.30,455.91,492.85"><head>: A 16B Model Equipped with MLA and DeepSeekMoE B.1. Model Description Architectures.</head><label></label><figDesc>DeepSeek-V2-Lite has 27 layers and a hidden dimension of 2048. It also employs MLA and has 16 attention heads, where each head has a dimension of 128. Its KV compression dimension is 512, but slightly different from DeepSeek-V2, it does not compress the queries. For the decoupled queries and key, it has a per-head dimension of 64. DeepSeek-V2-Lite also employs DeepSeekMoE, and all FFNs except for the first layer are replaced with MoE layers. Each MoE layer consists of 2 shared experts and 64 routed experts, where the intermediate hidden dimension of each expert is 1408. Among the routed experts, 6 experts will be activated for each token. Under this configuration, DeepSeek-V2-Lite comprises 15.7B total parameters, of which 2.4B are activated for each token.</figDesc><table coords="29,84.67,271.39,425.94,308.76"><row><cell></cell><cell>Benchmark</cell><cell cols="3">DeepSeek 7B DeepSeekMoE 16B DeepSeek-V2-Lite</cell></row><row><cell></cell><cell>Architecture</cell><cell>MHA+Dense</cell><cell>MHA+MoE</cell><cell>MLA+MoE</cell></row><row><cell></cell><cell>Context Length</cell><cell>4K</cell><cell>4K</cell><cell>32K</cell></row><row><cell></cell><cell># Activated Params</cell><cell>6.9B</cell><cell>2.8B</cell><cell>2.4B</cell></row><row><cell></cell><cell># Total Params</cell><cell>6.9B</cell><cell>16.4B</cell><cell>15.7B</cell></row><row><cell></cell><cell># Training Tokens</cell><cell>2T</cell><cell>2T</cell><cell>5.7T</cell></row><row><cell></cell><cell>MMLU</cell><cell>48.2</cell><cell>45.0</cell><cell>58.3</cell></row><row><cell></cell><cell>BBH</cell><cell>39.5</cell><cell>38.9</cell><cell>44.1</cell></row><row><cell></cell><cell>TriviaQA</cell><cell>59.7</cell><cell>64.8</cell><cell>64.2</cell></row><row><cell>English</cell><cell>NaturalQuestions ARC-Easy</cell><cell>22.2 67.9</cell><cell>25.5 68.1</cell><cell>26.0 70.9</cell></row><row><cell></cell><cell>ARC-Challenge</cell><cell>48.1</cell><cell>49.8</cell><cell>51.2</cell></row><row><cell></cell><cell>AGIEval</cell><cell>26.4</cell><cell>17.4</cell><cell>33.2</cell></row><row><cell></cell><cell>HumanEval</cell><cell>26.2</cell><cell>26.8</cell><cell>29.9</cell></row><row><cell>Code</cell><cell>MBPP</cell><cell>39.0</cell><cell>39.2</cell><cell>43.2</cell></row><row><cell></cell><cell>GSM8K</cell><cell>17.4</cell><cell>18.8</cell><cell>41.1</cell></row><row><cell>Math</cell><cell>MATH CMath</cell><cell>3.3 34.5</cell><cell>4.3 40.4</cell><cell>17.1 58.4</cell></row><row><cell></cell><cell>CLUEWSC</cell><cell>73.1</cell><cell>72.1</cell><cell>74.3</cell></row><row><cell>Chinese</cell><cell>C-Eval</cell><cell>45.0</cell><cell>40.6</cell><cell>60.3</cell></row><row><cell></cell><cell>CMMLU</cell><cell>47.2</cell><cell>42.5</cell><cell>64.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="29,70.41,595.62,454.22,163.78"><head>Table 6 |</head><label>6</label><figDesc>Performance of DeepSeek-V2-Lite, DeepSeekMoE 16B, and DeepSeek 7B. DeepSeek-V2-Lite is also trained from scratch on the same pre-training corpus of DeepSeek-V2, which is not polluted by any SFT data. It uses the AdamW optimizer with hyper-parameters set to 𝛽 1 = 0.9, 𝛽 2 = 0.95, and weight_decay = 0.1. The learning rate is scheduled using a warmup-and-step-decay strategy. Initially, the learning rate linearly increases from 0 to the maximum value during the first 2K steps. Subsequently, the learning rate is multiplied by 0.316 after training about 80% of tokens, and again by 0.316 after training about 90% of tokens. The maximum learning rate is set to 4.2 × 10 -4 , and the gradient clipping norm is set to 1.0. We do not employ the batch size scheduling strategy for it, and it is trained with a constant batch size of 4608 sequences. During pre-training, we set the maximum sequence length to 4K, and train DeepSeek-V2-Lite on 5.7T tokens. We leverage pipeline parallelism to deploy different layers of it on different devices, but for each layer, all experts will be deployed on the same device. Therefore, we only employ a small expert-level balance loss with 𝛼 1 = 0.001, and do not employ device-level balance loss and communication balance loss for it. After pre-training, we also perform long context extension and SFT for DeepSeek-V2-Lite and get a chat model called DeepSeek-V2-Lite Chat.</figDesc><table coords="29,70.87,640.59,86.28,10.23"><row><cell>Training Details.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="30,70.53,522.62,453.88,24.34"><head>Table 7 |</head><label>7</label><figDesc>Performance of DeepSeek-V2-Lite Chat, DeepSeekMoE 16B Chat, and DeepSeek 7B Chat.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="31,70.87,735.84,453.97,24.04"><head>Table 8 |</head><label>8</label><figDesc>table, we can observe that MLA shows better performance than MHA. More importantly, MLA requires a significantly Comparison among 7B dense models with MHA, GQA, and MQA, respectively. MHA demonstrates significant advantages over GQA and MQA on hard benchmarks. smaller amount of KV cache (14% for small MoE models and 4% for large MoE models) than MHA.</figDesc><table coords="32,71.11,91.11,453.07,335.68"><row><cell cols="3">Benchmark (Metric) # Shots</cell><cell cols="4">Dense 7B w/ MQA w/ GQA (8 Groups) w/ MHA Dense 7B Dense 7B</cell></row><row><cell># Params</cell><cell>-</cell><cell></cell><cell></cell><cell>7.1B</cell><cell>6.9B</cell><cell>6.9B</cell></row><row><cell>BBH (EM)</cell><cell cols="2">3-shot</cell><cell></cell><cell>33.2</cell><cell>35.6</cell><cell>37.0</cell></row><row><cell>MMLU (Acc.)</cell><cell cols="2">5-shot</cell><cell></cell><cell>37.9</cell><cell>41.2</cell><cell>45.2</cell></row><row><cell>C-Eval (Acc.)</cell><cell cols="2">5-shot</cell><cell></cell><cell>30.0</cell><cell>37.7</cell><cell>42.9</cell></row><row><cell>CMMLU (Acc.)</cell><cell cols="2">5-shot</cell><cell></cell><cell>34.6</cell><cell>38.4</cell><cell>43.5</cell></row><row><cell>Benchmark (Metric)</cell><cell cols="3"># Shots</cell><cell cols="3">Small MoE Small MoE Large MoE Large MoE w/ MHA w/ MLA w/ MHA w/ MLA</cell></row><row><cell># Activated Params</cell><cell></cell><cell>-</cell><cell></cell><cell>2.5B</cell><cell>2.4B</cell><cell>25.0B</cell><cell>21.5B</cell></row><row><cell># Total Params</cell><cell></cell><cell>-</cell><cell></cell><cell>15.8B</cell><cell>15.7B</cell><cell>250.8B</cell><cell>247.4B</cell></row><row><cell cols="2">KV Cache per Token (# Element)</cell><cell>-</cell><cell></cell><cell>110.6K</cell><cell>15.6K</cell><cell>860.2K</cell><cell>34.6K</cell></row><row><cell>BBH (EM)</cell><cell cols="2">3-shot</cell><cell></cell><cell>37.9</cell><cell>39.0</cell><cell>46.6</cell><cell>50.7</cell></row><row><cell>MMLU (Acc.)</cell><cell cols="2">5-shot</cell><cell></cell><cell>48.7</cell><cell>50.0</cell><cell>57.5</cell><cell>59.0</cell></row><row><cell>C-Eval (Acc.)</cell><cell cols="2">5-shot</cell><cell></cell><cell>51.6</cell><cell>50.9</cell><cell>57.9</cell><cell>59.2</cell></row><row><cell>CMMLU (Acc.)</cell><cell cols="2">5-shot</cell><cell></cell><cell>52.3</cell><cell>53.4</cell><cell>60.7</cell><cell>62.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="32,70.53,442.27,454.10,24.34"><head>Table 9 |</head><label>9</label><figDesc>Comparison between MLA and MHA on hard benchmarks. DeepSeek-V2 shows better performance than MHA, but requires a significantly smaller amount of KV cache.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="33,83.99,91.11,427.29,70.08"><head>Label Annotator 1 Annotator 2 Annotator 3</head><label></label><figDesc></figDesc><table coords="33,83.99,110.05,411.88,51.14"><row><cell>Ground-Truth Label</cell><cell>100.0%</cell><cell>66.7%</cell><cell>59.8%</cell><cell>42.1%</cell></row><row><cell>Annotator 1</cell><cell>66.7%</cell><cell>100.0%</cell><cell>57.9%</cell><cell>69.0%</cell></row><row><cell>Annotator 2</cell><cell>59.8%</cell><cell>57.9%</cell><cell>100.0%</cell><cell>65.5%</cell></row><row><cell>Annotator 3</cell><cell>42.1%</cell><cell>69.0%</cell><cell>65.5%</cell><cell>100.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="33,70.53,291.40,455.79,344.28"><head>Table 11 |</head><label>11</label><figDesc>corpus, which consists of thousands of Chinese math problems. DeepSeek-V2 Chat (RL) outperforms all Chinese LLMs, including both open-source and close-source models. SC-Math6 Model Reasoning Level. "R Level" stands for Reasoning Level, "Comp. Score" stands for Comprehensive Score, "Reas. Steps Score" stands for Reasoning Steps Score, and "OvrAcc Score" stands for Overall Accuracy Score.</figDesc><table coords="33,88.45,344.18,418.37,238.14"><row><cell>Model Name</cell><cell cols="4">R Level Comp. Score Reas. Steps Score OvrAcc Score</cell></row><row><cell>GPT-4-1106-Preview</cell><cell>5</cell><cell>90.71</cell><cell>91.65</cell><cell>89.77</cell></row><row><cell>GPT-4</cell><cell>5</cell><cell>88.40</cell><cell>89.10</cell><cell>87.71</cell></row><row><cell>DeepSeek-V2 Chat (RL)</cell><cell>5</cell><cell>83.35</cell><cell>85.73</cell><cell>84.54</cell></row><row><cell>Ernie-bot 4.0</cell><cell>5</cell><cell>85.60</cell><cell>86.82</cell><cell>84.38</cell></row><row><cell>Qwen-110B-Chat</cell><cell>5</cell><cell>83.25</cell><cell>84.93</cell><cell>84.09</cell></row><row><cell>GLM-4</cell><cell>5</cell><cell>84.24</cell><cell>85.72</cell><cell>82.77</cell></row><row><cell>Xinghuo 3.5</cell><cell>5</cell><cell>83.73</cell><cell>85.37</cell><cell>82.09</cell></row><row><cell>Qwen-72B-Chat</cell><cell>4</cell><cell>78.42</cell><cell>80.07</cell><cell>79.25</cell></row><row><cell>ChatGLM-Turbo</cell><cell>4</cell><cell>57.70</cell><cell>60.32</cell><cell>55.09</cell></row><row><cell>GPT-3.5-Turbo</cell><cell>4</cell><cell>57.05</cell><cell>59.61</cell><cell>54.50</cell></row><row><cell>Qwen-14B-Chat</cell><cell>4</cell><cell>53.12</cell><cell>55.99</cell><cell>50.26</cell></row><row><cell>ChatGLM3-6B</cell><cell>3</cell><cell>40.90</cell><cell>44.20</cell><cell>37.60</cell></row><row><cell>Xinghuo 3.0</cell><cell>3</cell><cell>40.08</cell><cell>45.27</cell><cell>34.89</cell></row><row><cell>Baichuan2-13B-Chat</cell><cell>3</cell><cell>39.40</cell><cell>42.63</cell><cell>36.18</cell></row><row><cell>Ernie-3.5-turbo</cell><cell>2</cell><cell>25.19</cell><cell>27.70</cell><cell>22.67</cell></row><row><cell>Chinese-Alpaca2-13B</cell><cell>2</cell><cell>20.55</cell><cell>22.52</cell><cell>18.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="34,127.56,442.99,341.41,95.73"><head></head><label></label><figDesc>Table 12-37, respectively.</figDesc><table coords="34,127.56,471.04,340.16,67.69"><row><cell>PROMPT</cell></row><row><cell>以下是一道中国高考生物选择题，请选择正确的答案。</cell></row><row><cell>问题：下列有关高尔基体、线粒体和叶绿体的叙述, 正确的是选项：(A)三者都</cell></row><row><cell>存在于蓝藻中(B)三者都含有DNA (C)三者都是ATP 合成的场所(D)三者的膜结</cell></row><row><cell>构中都含有蛋白质</cell></row><row><cell>答案：从A到D, 我们应选择</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="34,214.52,555.91,165.89,10.79"><head>Table 12 |</head><label>12</label><figDesc>An example of AGIEval.</figDesc><table coords="36,127.56,208.21,228.46,57.32"><row><cell>PROMPT</cell></row><row><cell>Evaluate the result of a random Boolean expression.</cell></row><row><cell>Q: not ( ( not not True ) ) is</cell></row><row><cell>A: Let's think step by step.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="36,224.29,627.49,146.36,10.79"><head>Table 14 |</head><label>14</label><figDesc>An example of BBH.</figDesc><table coords="38,127.56,166.71,119.55,159.13"><row><cell>PROMPT</cell></row><row><cell>女：这些药怎么吃?</cell></row><row><cell>男：一天三次，一次两片。</cell></row><row><cell>请根据上文回答问题：</cell></row><row><cell>他们在哪儿?</cell></row><row><cell>答案：</cell></row><row><cell>OPTIONS</cell></row><row><cell>-商店</cell></row><row><cell>-饭店</cell></row><row><cell>-医院</cell></row><row><cell>-教室</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="38,127.56,341.11,340.16,312.63"><head>Table 16 |</head><label>16</label><figDesc>An example of C3.</figDesc><table coords="38,127.56,518.61,340.16,135.13"><row><cell>PROMPT</cell></row><row><cell>以下是将某句古诗文翻译而成的现代表述：春天已至，万物复苏，春风如一位</cell></row><row><cell>美丽而又心灵手巧的姑娘，迈着纤纤细步款款而来，她挥舞剪刀，尽情地展示</cell></row><row><cell>那高超的女工技巧，她先裁出了柳叶，随着柳条袅袅依依地舞蹈，又裁出杏</cell></row><row><cell>叶，桃叶。</cell></row><row><cell>该翻译所对应的古诗文是：</cell></row><row><cell>OPTIONS</cell></row><row><cell>-春风骋巧如翦刀</cell></row><row><cell>-剪裁无巧似春风</cell></row><row><cell>-风吹怨恨快如刀</cell></row><row><cell>-春风欲擅秋风巧</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" coords="38,219.31,668.99,156.33,10.79"><head>Table 17 |</head><label>17</label><figDesc>An example of CCPM.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="39,214.75,597.97,165.45,10.79"><head>Table 18 |</head><label>18</label><figDesc>An example of CMATH.</figDesc><table coords="41,127.56,101.98,340.16,175.87"><row><cell>PROMPT</cell></row><row><cell>文章：英雄广场(Heldenplatz)是奥地利首都维也纳的一个广场。在此曾发</cell></row><row><cell>生许多重要事件-最著名的是1938年希特勒在此宣告德奥合并。英雄广场是</cell></row><row><cell>霍夫堡皇宫的外部广场，兴建于皇帝弗朗茨•约瑟夫一世统治时期，是没有完</cell></row><row><cell>全建成的所谓"帝国广场"(Kaiserforum)的一部分。其东北部是霍夫堡皇宫</cell></row><row><cell>的Leopoldinian Tract，东南方是新霍夫堡，西南方的内环路，将其与"城门</cell></row><row><cell>外"(Äußeres Burgtor)隔开。西北部没有任何建筑物，可以很好地眺望内环</cell></row><row><cell>路、国会大厦、市政厅，以及城堡剧院。广场上有2尊军事领袖的骑马像：欧</cell></row><row><cell>根亲王和卡尔大公。</cell></row><row><cell>根据上文回答下面的问题。</cell></row><row><cell>问题：英雄广场是哪个皇宫的外部广场？</cell></row><row><cell>答案：霍夫堡皇宫</cell></row><row><cell>问题：广场上有哪两位军事领袖的骑马像？</cell></row><row><cell>答案：</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="41,127.56,295.03,341.90,93.40"><head>Table 20 |</head><label>20</label><figDesc>An example of CMRC2018. The median age in the city was 22.1 years. 10.1% of residents were under the age of 18; 56.2% were between the ages of 18 and 24; 16.1% were from 25 to 44; 10.5% were from 45 to 64; and 7% were 65 years of age or older.</figDesc><table /><note coords="41,127.56,343.06,44.27,9.34;41,127.56,354.93,37.97,9.58"><p><p>PROMPT</p>Passage:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" coords="41,127.56,523.24,340.16,195.24"><head>Table 21 |</head><label>21</label><figDesc>An example of DROP.</figDesc><table coords="41,127.56,571.26,340.16,147.21"><row><cell>PROMPT</cell></row><row><cell>中新网12月7日电综合外媒6日报道,在美国得克萨斯州,负责治疗新冠肺炎患者</cell></row><row><cell>的医生约瑟夫•瓦隆(Joseph Varon)已连续上班超260天,每天只睡不超过2小时。</cell></row><row><cell>瓦隆日前接受采访时呼吁,美国民众应遵从防疫规定,一线的医护人员"已</cell></row><row><cell>OPTIONS</cell></row><row><cell>-神清气爽"。</cell></row><row><cell>-诡计多端"。</cell></row><row><cell>-精疲力竭"。</cell></row><row><cell>-分工合作"。</cell></row><row><cell>-寅吃卯粮"。</cell></row><row><cell>-土豪劣绅"。</cell></row><row><cell>-芸芸众生"。</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24" coords="41,221.03,733.73,152.88,10.79"><head>Table 22 |</head><label>22</label><figDesc>An example of CHID.</figDesc><table coords="42,127.56,231.35,340.16,357.43"><row><cell>PROMPT</cell></row><row><cell>胡雪岩离船登岸，坐轿进城，等王有龄到家，他接着也到了他那里，脸上是掩</cell></row><row><cell>抑不住的笑容，王有龄夫妇都觉得奇怪，问他什么事这么高兴。</cell></row><row><cell>上面的句子中的"他"指的是</cell></row><row><cell>胡雪岩</cell></row><row><cell>渐渐地，汤中凝结出一团团块状物，将它们捞起放进盆里冷却，肥皂便出现在</cell></row><row><cell>世上了。</cell></row><row><cell>上面的句子中的"它们"指的是</cell></row><row><cell>块状物</cell></row><row><cell>"她序上明明引着JulesTellier的比喻，说有个生脱发病的人去理发，那剃头的</cell></row><row><cell>对他说不用剪发，等不了几天，头毛压儿全掉光了；大部分现代文学也同样的</cell></row><row><cell>不值批评。这比喻还算俏皮。"</cell></row><row><cell>上面的句子中的"他"指的是</cell></row><row><cell>生脱发病的人</cell></row><row><cell>在洛伦佐大街的尽头处，矗立着著名的圣三一大教堂。它有着巨大的穹顶，还</cell></row><row><cell>有明亮的彩色玻璃窗，上面描绘着《旧约》和《新约》的场景。</cell></row><row><cell>上面的句子中的"它"指的是</cell></row><row><cell>圣三一大教堂</cell></row><row><cell>他伯父还有许多女弟子，大半是富商财主的外室；这些财翁白天忙着赚钱，怕</cell></row><row><cell>小公馆里的情妇长日无聊，要不安分，常常叫她们学点玩艺儿消遣。</cell></row><row><cell>上面的句子中的"她们"指的是</cell></row><row><cell>情妇</cell></row><row><cell>赵雨又拿出了一个杯子，我们热情地请老王入座，我边给他倒酒边问：1962年</cell></row><row><cell>的哪次记得吗？"</cell></row><row><cell>上面的句子中的"他"指的是</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25" coords="42,208.53,604.36,177.87,10.79"><head>Table 23 |</head><label>23</label><figDesc>An example of CLUEWSC.</figDesc><table coords="44,127.56,195.30,201.28,21.46"><row><cell>PROMPT</cell></row><row><cell>Playing piano: A man is seated at a piano. He</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26" coords="44,127.56,297.27,302.33,315.92"><head>Table 25 |</head><label>25</label><figDesc>An example of HellaSwag.</figDesc><table coords="44,127.56,531.95,302.33,81.24"><row><cell>PROMPT</cell></row><row><cell>def starts_one_ends(n):</cell></row><row><cell>"""</cell></row><row><cell>Given a positive integer n, return the count of the numbers of n-digit</cell></row><row><cell>positive integers that start or end with 1.</cell></row><row><cell>"""</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27" coords="44,206.22,640.40,182.50,10.79"><head>Table 26 |</head><label>26</label><figDesc>An example of HumanEval.</figDesc><table coords="48,127.56,123.26,206.68,164.92"><row><cell>PROMPT</cell></row><row><cell>Answer these questions:</cell></row><row><cell>Q: Who is hosting the fifa world cup in 2022?</cell></row><row><cell>A: Qatar</cell></row><row><cell>Q: Who won the first women 's fifa world cup?</cell></row><row><cell>A: United States</cell></row><row><cell>Q: When did miami vice go off the air?</cell></row><row><cell>A: 1989</cell></row><row><cell>Q: Who wrote the song shout to the lord?</cell></row><row><cell>A: Darlene Zschech</cell></row><row><cell>Q: Who was thrown in the lion 's den?</cell></row><row><cell>A: Daniel</cell></row><row><cell>Q: What is the meaning of the name habib?</cell></row><row><cell>A:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28" coords="48,192.37,303.44,210.21,10.79"><head>Table 30 |</head><label>30</label><figDesc>An example of NaturalQuestions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29" coords="48,221.83,712.45,151.28,10.79"><head>Table 32 |</head><label>32</label><figDesc>An example of PIQA.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Q: Danny brings 3 watermelons to his family picnic. He cuts each watermelon into 10 slices. His sister brings 1 watermelon to the family picnic, and she cuts the watermelon into 15 slices. How many watermelon slices are there in total at the picnic? A: Let's think step by step. From Danny, there are 3 * 10 = 30 watermelon slices. From his sister, there are 1 * 15 = 15 watermelon slices. There are a total of 30 + 15 = 45 watermelon slices. The answer is 45. Q: Angela is a bike messenger in New York. She needs to deliver 8 times as many packages as meals. If she needs to deliver 27 meals and packages combined, how many meals does she deliver? A: Let's think step by step. Let p be the number of packages Angela delivers and m be the number of meals. We know that p + m = 27 and p = 8m. Substituting the second equation into the first equation, we get 8m + m = 27. Combining like terms, we get 9m = 27. Dividing both sides by 9, we get m = 3. The answer is 3. Q: Cori is 3 years old today. In 5 years, she will be one-third the age of her aunt. How old is her aunt today? A: Let's think step by step. In 5 years, Cori will be 3 + 5 = 8 years old. In 5 years, Cori's aunt will be 8 x 3 = 24 years old. Today, her aunt is 24 -5 = 19 years old. The answer is 19.</p><p>Q: Indras has 6 letters in her name. Her sister's name has 4 more letters than half of the letters in Indras' name. How many letters are in Indras and her sister's names? A: Let's think step by step.  The expressions inside each square root must be non-negative. Therefore, $x-2 \ge 0$, so $x\ge2$, and $5 -x \ge 0$, so $x \le 5$. Also, the denominator cannot be equal to zero, so $5-x&gt;0$, which gives $x&lt;5$. Therefore, the domain of the expression is $\boxed{[2,5)}$. Final Answer: The final answer is $[2,5)$. I hope it is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem:</head><p>If $\det \mathbf{A} = 2$ and $\det \mathbf{B} = 12,$ then find $\det (\mathbf{A} \mathbf{B}).$</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution:</head><p>We have that $\det (\mathbf{A} \mathbf{B}) = (\det \mathbf{A})(\det \mathbf{B}) = (2)(12) = \boxed{24}.$ Final Answer: The final answer is $24$. I hope it is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem:</head><p>Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight? Solution: If Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\cdot 12\cdot20=480$ pounds of weight. If he lifts two 15-pound weights instead for $n$ times, he will lift a total of $2\cdot15\cdot n=30n$ pounds of weight. Equating this to 480 pounds, we can solve for $n$: \begin{align*} 30n&amp;=480\\ \Rightarrow\qquad n&amp;=480/30=\boxed{16} \end{align*} Final Answer: The final answer is $16$. I hope it is correct.</p><p>Problem: If the system of equations \begin{align*} 6x-4y&amp;=a,\\ 6y-9x &amp;=b. \end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\frac{a}{b},$ assuming $b$ is nonzero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution:</head><p>If we multiply the first equation by $-\frac{3}{2}$, we obtain $$6y-9x=-\frac{3}{2}a.$$Since we also know that $6y-9x=b$, we have $$-\frac{3}{2}a=b\Rightarrow\frac{a}{b}=\boxed{-\frac{2}{3}}.$$ Final Answer: The final answer is $-\frac{2}{3}$. I hope it is correct.</p><p>Problem: Evaluate $\log_21$. Solution:  <ref type="table" coords="46,231.09,178.24,13.35,9.58">(3,</ref><ref type="table" coords="46,246.93,178.24,7.47,9.58">4,</ref><ref type="table" coords="46,256.89,178.24,7.47,9.58">5,</ref><ref type="table" coords="46,266.86,178.24,10.79,9.58">6),</ref><ref type="table" coords="46,277.65,178.24,10.79,9.58">(5,</ref><ref type="table" coords="46,290.93,178.24,7.47,9.58">7,</ref><ref type="table" coords="46,300.89,178.24,7.47,9.58">4,</ref><ref type="table" coords="46,310.85,178.24,44.44,9.58">10)) == (4,</ref><ref type="table" coords="46,357.78,178.24,8.30,9.58;46,127.56,190.20,116.88,9.58">5)  assert similar_elements((1,</ref><ref type="table" coords="46,246.93,190.20,7.47,9.58">2,</ref><ref type="table" coords="46,256.89,190.20,7.47,9.58">3,</ref><ref type="table" coords="46,266.86,190.20,10.79,9.58">4),</ref><ref type="table" coords="46,277.65,190.20,10.79,9.58">(5,</ref><ref type="table" coords="46,290.93,190.20,7.47,9.58">4,</ref><ref type="table" coords="46,300.89,190.20,7.47,9.58">3,</ref><ref type="table" coords="46,310.85,190.20,3.87,9.58">7</ref>)) == (3, 4) assert similar_elements <ref type="bibr" coords="46,227.04,202.15,131.12,9.58">((11, 12, 14, 13),(17, 15, 14, 13)</ref> You are an expert Python programmer, and here is your task: Write a function to find the largest integers from a given list of numbers using heap queue algorithm. Your code should pass these tests: assert heap_queue_largest <ref type="bibr" coords="46,242.00,501.03,143.15,9.58">( [25, 35, 22, 85, 14, 65, 75, 22, 58]</ref>,3)== <ref type="bibr" coords="46,406.78,501.03,17.31,9.58">[85,</ref><ref type="bibr" coords="46,426.58,501.03,12.45,9.58">75,</ref><ref type="bibr" coords="46,441.52,501.03,13.28,9.58">65]</ref> assert heap_queue_largest <ref type="bibr" coords="46,242.00,512.99,143.15,9.58">( [25, 35, 22, 85, 14, 65, 75, 22, 58]</ref>,2)== <ref type="bibr" coords="46,406.78,512.99,17.31,9.58">[85,</ref><ref type="bibr" coords="46,426.58,512.99,13.28,9.58">75]</ref> assert heap_queue_largest <ref type="bibr" coords="46,242.00,524.94,143.18,9.58">( [25, 35, 22, 85, 14, 65, 75, 22, 58]</ref>,5)== <ref type="bibr" coords="46,406.81,524.94,17.31,9.58">[85,</ref><ref type="bibr" coords="46,426.62,524.94,12.45,9.58">75,</ref><ref type="bibr" coords="46,441.56,524.94,12.45,9.58">65,</ref><ref type="bibr" coords="46,456.50,524.94,12.45,9.58">58,</ref><ref type="bibr" coords="46,127.56,536.90,13.28,9.58">35]</ref> [BEGIN] import heapq as hq def heap_queue_largest(nums,n): largest_nums = hq.nlargest(n, nums) return largest_nums <ref type="bibr" coords="46,127.56,608.63,36.54,9.58">[DONE]</ref> You are an expert Python programmer, and here is your task: Write a function to return the sum of all divisors of a number. Your code should pass these tests: assert sum_div(8)==7 assert sum_div(12)==16 assert sum_div <ref type="bibr" coords="46,191.67,680.36,15.53,9.58" target="#b29">(7)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OPTIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROMPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Article:</head><p>When you read an article you will understand and remember it better if you can work out how the writer has put the ideas together. Sometimes a writer puts ideas together by asking questions and then answering them.For example, if the article is about groundhogs, the set of questions in the writer's head might be: What does a groundhog look like? Where do groundhogs live? What do they eat?... In the article,the author might answer those questions. Sometimes an author writes out her questions in the article.These questions give you signals.They tell you what the author is going to write next.Often an author has a question in her head but she doesn't write it out for you.You have to work out her question for yourself.Here's a sample reading for you to practice this method. Earthworms Do you know how many kinds of earthworms there are?There are about 1800 kinds in the world! They can be brown,purple,green.They can be as small as 3 cm long and as large as 3 m long. The best time to see earthworms is at night,especially a cool,damp night.That's when they come up from their burrows to hunt for food.Earthworms don't like to be in the sun.That's because they breathe through their skin,and they can't breathe if their skin gets too dry.Earthworms must come out of the earth if it rains a lot,because they can't breathe in their flooded burrows.What a dangerous life! Earthworms don't have eyes,so how can they tell when it's dark? They have special places on their skin that are sensitive to light.These spots tell whether it's light or dark.If you shine a flashlight on an earthworm at night,it will quickly disappear into the ground. Earthworms don't have ears either,but they can hear by feeling movements in the earth.If you want to hear like an earthworm,lie on the ground with your fingers in your ears.Then have a friend stamp his or her feet near you.This is how earthworms feel birds and people walking,and moles digging,near them. Earthworms are useful.Farmers and gardeners like having lots of earthworms in their land because the worms help to make better soil when they dig.That digging keeps the soil loose and airy .In one year earthworms can pile up as much as 23,000 kg of castings in an area about the size of a football field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: What's the purpose of reading Earthworms?</head><p>A: To put the writer's idea into real use. Q: Which question CANNOT be answered in the passage? A: Why can human listen like earthworms? Q: How can you understand Earthworms better according to this passage? A: Read to work out all the questions in the writer's head while reading.</p><p>Q: What's the best title for the passage? A:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OPTIONS</head><p>-One way to help with understanding -One way to practice with a new idea -One way to learn to be a wise writer -One way to be clearer about worms   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>You will be given a function f and an output in the form f(??) == output. Find any input such that executing f on the input leads to the given output. There may be multiple answers, but you should only output one. In [ANSWER] and [/ANSWER] tags, complete the assertion with one such input that will produce the output when executing the function.</p><p>[PYTHON] def f(my_list): count = 0 for i in my_list: if len(i) % 2 == 0: count += 1 return count assert f(??   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="21,82.82,682.68,441.59,10.49;21,81.71,696.46,135.03,9.85" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><surname>Meta</surname></persName>
		</author>
		<ptr target="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" />
		<title level="m" coords="21,122.64,682.68,94.15,10.49">Llama 3 model card</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,70.87,718.75,453.55,10.49;21,81.71,732.30,442.70,10.49;21,81.71,745.85,110.92,10.49" xml:id="b1">
	<monogr>
		<title level="m" type="main" coords="21,456.54,718.75,67.88,10.49;21,81.71,732.30,363.66,10.49">Gqa: Training generalized multi-query transformer models from multi-head checkpoints</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lebrón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sanghai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13245</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,70.87,124.64,454.90,10.49;22,81.71,138.19,444.06,10.49;22,81.71,151.74,24.55,10.49" xml:id="b2">
	<monogr>
		<title level="m" type="main" coords="22,139.50,138.19,221.64,10.49">Program synthesis with large language models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,70.87,174.26,454.91,10.49;22,81.71,187.81,444.06,10.49;22,81.53,201.36,444.25,10.49;22,81.71,214.91,444.06,10.49;22,81.71,228.46,356.03,10.49" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m" coords="22,141.10,228.46,105.48,10.49">Qwen technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,70.87,250.97,453.54,10.49;22,81.71,264.52,442.70,10.49;22,81.71,278.07,442.70,10.49;22,81.71,291.62,442.70,10.49;22,81.71,305.17,444.06,10.49;22,81.17,318.72,406.58,10.49" xml:id="b4">
	<analytic>
		<title level="a" type="main" coords="22,303.94,250.97,220.47,10.49;22,81.71,264.52,90.33,10.49">PIQA: reasoning about physical commonsense in natural language</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6239</idno>
		<ptr target="https://doi.org/10.1609/aaai.v34i05.6239" />
	</analytic>
	<monogr>
		<title level="m" coords="22,193.71,264.52,330.70,10.49;22,81.71,278.07,442.70,10.49;22,81.71,291.62,408.19,10.49">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,70.87,341.23,454.90,10.49;22,81.71,354.78,444.06,10.49;22,81.71,368.33,444.06,10.49;22,81.71,381.88,444.06,10.49;22,81.29,395.43,444.49,10.49;22,81.29,408.98,444.48,10.49;22,81.71,422.53,442.70,10.49;22,81.17,436.08,445.15,10.49;22,81.71,449.63,229.50,10.49" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Guss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2107.03374" />
		<imprint/>
	</monogr>
	<note>Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021</note>
</biblStruct>

<biblStruct coords="22,70.87,472.14,453.54,10.49;22,81.71,485.69,444.06,10.49;22,81.71,499.24,251.67,10.49" xml:id="b6">
	<monogr>
		<title level="m" type="main" coords="22,474.57,472.14,49.84,10.49;22,81.71,485.69,325.98,10.49">Think you have solved question answering? try arc, the AI2 reasoning challenge</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<idno>CoRR, abs/1803.05457</idno>
		<ptr target="http://arxiv.org/abs/1803.05457" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,70.87,521.76,454.91,10.49;22,81.53,535.30,442.88,10.49;22,81.71,548.85,110.92,10.49" xml:id="b7">
	<monogr>
		<title level="m" type="main" coords="22,214.86,535.30,230.30,10.49">Training verifiers to solve math word problems</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,70.87,571.37,453.55,10.49;22,81.71,584.92,444.07,10.49;22,81.71,598.47,442.70,10.49;22,81.71,612.02,442.70,10.49;22,81.71,625.57,444.50,10.49;22,81.71,639.12,443.01,10.49" xml:id="b8">
	<analytic>
		<title level="a" type="main" coords="22,435.45,571.37,88.96,10.49;22,81.71,584.92,252.05,10.49">A span-extraction dataset for Chinese machine reading comprehension</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1600</idno>
		<ptr target="https://aclanthology.org/D19-1" />
	</analytic>
	<monogr>
		<title level="m" coords="22,123.49,598.47,400.92,10.49;22,81.71,612.02,442.70,10.49;22,81.71,625.57,84.22,10.49;22,404.11,625.57,122.11,10.49;22,81.71,639.12,80.93,10.49">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11">Nov. 2019</date>
			<biblScope unit="page" from="5883" to="5889" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="22,97.93,652.89,5.41,9.85;22,70.87,675.18,455.45,10.49;22,81.71,688.73,442.69,10.49;22,81.71,702.28,443.03,10.49;22,81.71,716.06,261.03,9.85" xml:id="b9">
	<monogr>
		<title level="m" type="main" coords="22,331.91,688.73,192.49,10.49;22,81.71,702.28,260.86,10.49">Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2401.06066</idno>
		<idno>CoRR, abs/2401.06066</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2401.06066" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,70.87,738.34,452.04,10.49" xml:id="b10">
	<monogr>
		<title level="m" type="main" coords="22,108.71,738.34,382.28,10.49">FlashAttention-2: Faster attention with better parallelism and work partitioning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,70.87,88.58,454.91,10.49;23,81.71,102.13,394.66,10.49" xml:id="b11">
	<monogr>
		<title level="m" type="main" coords="23,140.81,88.58,344.70,10.49">Deepseek LLM: scaling open-source language models with longtermism</title>
		<author>
			<persName coords=""><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2401.02954</idno>
		<idno>CoRR, abs/2401.02954</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2401.02954" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,70.87,124.64,455.35,10.49;23,81.71,138.19,442.70,10.49;23,81.38,151.74,443.03,10.49;23,81.71,165.29,443.04,10.49;23,81.71,178.84,444.34,10.49;23,81.71,192.39,443.03,10.49;23,81.71,206.17,230.03,9.85" xml:id="b12">
	<analytic>
		<title level="a" type="main" coords="23,398.67,124.64,127.54,10.49;23,81.71,138.19,303.91,10.49">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1246</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1246" />
	</analytic>
	<monogr>
		<title level="m" coords="23,169.60,151.74,354.81,10.49;23,81.71,165.29,443.04,10.49;23,81.71,178.84,19.64,10.49">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s" coords="23,352.50,178.84,104.43,10.49">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,70.87,228.46,453.54,10.49;23,81.26,242.00,360.17,10.49" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Galambosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.04475</idno>
		<title level="m" coords="23,337.05,228.46,187.36,10.49;23,81.26,242.00,168.81,10.49">Length-controlled alpacaeval: A simple way to debias automatic evaluators</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="23,70.87,264.52,453.54,10.49;23,81.26,278.07,444.41,10.49;23,81.71,291.85,90.93,9.85" xml:id="b14">
	<monogr>
		<title level="m" type="main" coords="23,245.35,264.52,279.06,10.49;23,81.26,278.07,156.48,10.49">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>CoRR, abs/2101.03961</idno>
		<ptr target="https://arxiv.org/abs/2101.03961" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,70.87,314.13,454.91,10.49;23,81.71,327.68,443.00,10.49;23,81.71,341.23,153.49,10.49" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m" coords="23,181.84,327.68,307.21,10.49">The Pile: An 800GB dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="23,70.87,363.75,456.70,10.49;23,81.08,377.53,287.22,9.85" xml:id="b16">
	<monogr>
		<title level="m" type="main" coords="23,117.57,363.75,298.72,10.49">Introducing gemini: our largest and most capable ai model</title>
		<author>
			<persName coords=""><surname>Google</surname></persName>
		</author>
		<ptr target="https://blog.google/technology/ai/google-gemini-ai/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,70.87,399.81,453.97,10.49;23,81.71,413.36,323.63,10.49" xml:id="b17">
	<monogr>
		<title level="m" type="main" coords="23,463.29,399.81,61.55,10.49;23,81.71,413.36,291.43,10.49">Cruxeval: A benchmark for code reasoning, understanding and execution</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Leather</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,70.87,435.88,453.55,10.49;23,81.71,449.43,397.88,10.49" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03300</idno>
		<title level="m" coords="23,471.95,435.88,52.47,10.49;23,81.71,449.43,206.07,10.49">Measuring massive multitask language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="23,70.87,471.94,455.36,10.49;23,81.71,485.49,444.06,10.49;23,81.71,499.04,24.55,10.49" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<title level="m" coords="23,502.08,471.94,24.14,10.49;23,81.71,485.49,280.73,10.49">Measuring mathematical problem solving with the math dataset</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="23,70.87,520.79,453.54,11.26;23,81.71,535.34,110.33,9.85" xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>High-Flyer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hai-Llm</surname></persName>
		</author>
		<ptr target="https://www.high-flyer.cn/en/blog/hai-llm" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>高效且轻量的大模型训练工具</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,70.87,557.62,455.45,10.49;23,81.71,571.17,444.06,10.49;23,81.71,584.72,394.66,10.49" xml:id="b21">
	<monogr>
		<title level="m" type="main" coords="23,81.71,571.17,405.70,10.49">Kvquant: Towards 10 million context length LLM inference with KV cache quantization</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mohammadzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2401.18079</idno>
		<idno>CoRR, abs/2401.18079</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2401.18079" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,70.87,607.24,455.45,10.49;23,81.71,620.78,444.60,10.49;23,81.71,634.33,182.60,10.49" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.06395</idno>
		<title level="m" coords="23,131.71,620.78,390.22,10.49">Unveiling the potential of small language models with scalable training strategies</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="23,70.87,656.85,453.97,10.49;23,81.71,670.40,442.70,10.49;23,81.71,683.95,110.92,10.49" xml:id="b23">
	<monogr>
		<title level="m" type="main" coords="23,477.63,656.85,47.21,10.49;23,81.71,670.40,364.36,10.49">C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08322</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="23,70.87,706.46,455.45,10.49;23,81.71,720.01,444.60,10.49;23,81.71,733.56,182.60,10.49" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.07974</idno>
		<title level="m" coords="23,81.71,720.01,439.62,10.49">Livecodebench: Holistic and contamination free evaluation of large language models for code</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="24,70.87,88.58,455.36,10.49;24,81.71,102.13,442.70,10.49;24,81.71,115.68,442.70,10.49;24,81.71,129.23,442.69,10.49;24,81.71,142.78,433.54,10.49" xml:id="b25">
	<analytic>
		<title level="a" type="main" coords="24,292.94,88.58,233.29,10.49;24,81.71,102.13,188.49,10.49">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
		<ptr target="https://aclanthology.org/P17-1147" />
	</analytic>
	<monogr>
		<title level="m" coords="24,453.72,102.13,70.69,10.49;24,81.71,115.68,359.10,10.49;24,499.37,115.68,25.04,10.49;24,81.71,129.23,34.28,10.49">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</editor>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct coords="24,70.87,165.29,454.90,10.49;24,81.71,178.84,444.06,10.49;24,81.53,192.39,442.88,10.49;24,81.71,205.94,444.61,10.49;24,81.71,219.49,254.69,10.49" xml:id="b26">
	<analytic>
		<title level="a" type="main" coords="24,248.48,192.39,275.92,10.49;24,81.71,205.94,38.70,10.49">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00276" />
	</analytic>
	<monogr>
		<title level="j" coords="24,129.53,205.94,162.67,10.49">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,70.87,242.00,455.45,10.49;24,81.71,255.55,442.70,10.49;24,81.71,269.10,433.80,10.49" xml:id="b27">
	<analytic>
		<title level="a" type="main" coords="24,81.71,255.55,421.90,10.49">Efficient memory management for large language model serving with pagedattention</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="24,81.71,269.10,401.91,10.49">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</title>
		<meeting>the ACM SIGOPS 29th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,70.87,291.62,453.55,10.49;24,81.71,305.17,442.70,10.49;24,81.71,318.72,444.06,10.49;24,81.71,332.27,442.69,10.49;24,81.71,345.82,443.02,10.49" xml:id="b28">
	<analytic>
		<title level="a" type="main" coords="24,314.63,291.62,209.78,10.49;24,81.71,305.17,129.55,10.49">RACE: large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1082</idno>
		<ptr target="https://doi.org/10.18653/v1/d1" />
	</analytic>
	<monogr>
		<title level="m" coords="24,452.42,305.17,71.99,10.49;24,81.71,318.72,368.35,10.49">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hwa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</editor>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09">2017. September 9-11, 2017. 2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,87.26,359.59,35.17,9.85" xml:id="b29">
	<monogr>
		<title/>
		<idno type="DOI">10.18653/v1/d17-1082</idno>
		<imprint>
			<biblScope unit="page">1082</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,70.87,381.88,455.45,10.49;24,81.71,395.43,442.70,10.49;24,81.71,408.98,444.60,10.49;24,81.71,422.53,298.79,10.49" xml:id="b30">
	<analytic>
		<title level="a" type="main" coords="24,81.71,395.43,405.55,10.49">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qrwe7XHTmYb" />
	</analytic>
	<monogr>
		<title level="m" coords="24,509.05,395.43,15.36,10.49;24,81.71,408.98,439.60,10.49">9th International Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="24,70.87,445.04,455.36,10.49;24,81.71,458.59,444.06,10.49;24,81.71,472.14,24.55,10.49" xml:id="b31">
	<monogr>
		<title level="m" type="main" coords="24,438.85,445.04,87.37,10.49;24,81.71,458.59,279.53,10.49">CMMLU: Measuring massive multitask language understanding in Chinese</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Koto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09212</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="24,70.87,494.66,454.90,10.49;24,81.71,508.21,24.55,10.49" xml:id="b32">
	<monogr>
		<title level="m" type="main" coords="24,272.77,494.66,248.25,10.49">Ccpm: A chinese classical poetry matching dataset</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,70.87,530.72,454.91,10.49;24,81.71,544.27,442.70,10.49;24,81.71,557.82,443.13,10.49;24,81.71,571.37,373.51,10.49" xml:id="b33">
	<monogr>
		<title level="m" type="main" coords="24,387.74,544.27,136.68,10.49;24,81.71,557.82,211.35,10.49">Alignbench: Benchmarking chinese alignment of large language models</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">L</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2311.18743</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2311.18743" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,70.87,593.89,453.54,10.49;24,81.71,607.43,110.92,10.49" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m" coords="24,231.39,593.89,201.85,10.49">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="24,70.87,629.95,453.55,10.49;24,81.71,643.50,369.41,10.49" xml:id="b35">
	<monogr>
		<title level="m" type="main" coords="24,113.47,629.95,410.94,10.49;24,81.71,643.50,73.09,10.49">Cheaper, better, faster, stronger: Continuing to push the frontier of ai and making it accessible to all</title>
		<author>
			<persName coords=""><surname>Mistral</surname></persName>
		</author>
		<ptr target="https://mistral.ai/news/mixtral-8x22b" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,70.87,666.01,407.39,10.49" xml:id="b36">
	<analytic>
		<title/>
		<ptr target="https://openai.com/blog/chatgpt" />
	</analytic>
	<monogr>
		<title level="j" coords="24,70.87,666.01,147.07,10.49">OpenAI. Introducing ChatGPT</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,70.87,688.53,339.97,10.49" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m" coords="24,116.39,688.53,103.29,10.49">GPT4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="24,70.87,711.05,454.91,10.49;24,81.71,724.60,444.60,10.49;24,81.71,738.14,358.59,10.49" xml:id="b38">
	<analytic>
		<title level="a" type="main" coords="24,191.36,724.60,329.96,10.49">Training language models to follow instructions with human feedback</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="24,81.71,738.14,247.09,10.49">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,70.87,88.58,453.54,10.49;25,81.71,102.13,271.62,10.49" xml:id="b39">
	<monogr>
		<title level="m" type="main" coords="25,291.77,88.58,232.63,10.49;25,81.71,102.13,79.67,10.49">Yarn: Efficient context window extension of large language models</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Quesnelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shippole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.00071</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="25,70.87,124.64,453.54,10.49;25,81.71,138.19,110.92,10.49" xml:id="b40">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.10241</idno>
		<title level="m" coords="25,275.67,124.64,163.54,10.49">Zero bubble pipeline parallelism</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="25,70.87,160.71,455.36,10.49;25,81.71,174.26,444.06,10.49;25,81.71,187.81,283.55,10.49" xml:id="b41">
	<analytic>
		<title level="a" type="main" coords="25,294.16,160.71,232.07,10.49;25,81.71,174.26,104.04,10.49">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="25,206.98,174.26,318.79,10.49;25,81.71,187.81,164.26,10.49">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,70.87,210.32,453.54,10.49;25,81.71,223.87,442.70,10.49;25,81.71,237.42,444.06,10.49;25,81.71,250.97,443.01,10.49;25,81.08,264.75,362.33,9.85" xml:id="b42">
	<analytic>
		<title level="a" type="main" coords="25,140.75,223.87,207.74,10.49">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/48237" />
	</analytic>
	<monogr>
		<title level="m" coords="25,368.28,223.87,156.13,10.49;25,81.71,237.42,444.06,10.49;25,81.71,250.97,63.74,10.49">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8583" to="8595" />
		</imprint>
	</monogr>
	<note>d9f2dea8c74c2a72126cf63d933-Abstract.html</note>
</biblStruct>

<biblStruct coords="25,70.87,287.04,453.54,10.49;25,81.71,300.59,152.51,10.49" xml:id="b43">
	<monogr>
		<title level="m" type="main" coords="25,335.07,287.04,189.34,10.49;25,81.71,300.59,120.89,10.49">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,70.87,323.10,454.91,10.49;25,81.71,336.65,442.70,10.49;25,81.71,350.20,110.92,10.49" xml:id="b44">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03300</idno>
		<title level="m" coords="25,449.97,323.10,75.80,10.49;25,81.71,336.65,358.47,10.49">Deepseekmath: Pushing the limits of mathematical reasoning in open language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="25,70.87,372.71,454.91,10.49;25,81.71,386.26,251.67,10.49" xml:id="b45">
	<monogr>
		<title level="m" type="main" coords="25,128.66,372.71,277.51,10.49">Fast transformer decoding: One write-head is all you need</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>CoRR, abs/1911.02150</idno>
		<ptr target="http://arxiv.org/abs/1911.02150" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,70.87,408.78,453.97,10.49;25,81.71,422.33,442.70,10.49;25,81.71,435.88,445.85,10.49;25,81.08,449.66,223.23,9.85" xml:id="b46">
	<analytic>
		<title level="a" type="main" coords="25,459.99,408.78,64.84,10.49;25,81.71,422.33,334.74,10.49">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1ckMDqlg" />
	</analytic>
	<monogr>
		<title level="m" coords="25,442.59,422.33,81.82,10.49;25,81.71,435.88,339.89,10.49">5th International Conference on Learning Representations, ICLR 2017. OpenReview.net</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,70.87,471.94,453.97,10.49;25,81.39,485.49,274.69,10.49" xml:id="b47">
	<analytic>
		<title level="a" type="main" coords="25,308.57,471.94,216.27,10.49;25,81.39,485.49,94.06,10.49">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="25,185.15,485.49,80.62,10.49">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,70.87,508.01,453.55,10.49;25,81.71,521.56,187.89,10.49" xml:id="b48">
	<monogr>
		<title level="m" type="main" coords="25,255.66,508.01,268.76,10.49;25,81.71,521.56,155.11,10.49">Investigating prior knowledge for challenging chinese machine reading comprehension</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,70.87,544.07,454.91,10.49;25,81.71,557.62,442.70,10.49;25,81.71,571.17,214.01,10.49" xml:id="b49">
	<monogr>
		<title level="m" type="main" coords="25,199.91,557.62,324.50,10.49;25,81.71,571.17,21.99,10.49">Challenging big-bench tasks and whether chain-of-thought can solve them</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.09261</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="25,70.87,593.69,455.35,10.49;25,81.71,607.24,444.06,10.49;25,81.71,620.78,24.55,10.49" xml:id="b50">
	<analytic>
		<title level="a" type="main" coords="25,123.56,607.24,120.06,10.49">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="25,254.24,607.24,249.23,10.49">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,70.87,643.30,454.91,10.49;25,81.71,656.85,444.06,10.49;25,81.71,670.40,24.55,10.49" xml:id="b51">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07682</idno>
		<title level="m" coords="25,160.65,656.85,202.48,10.49">Emergent abilities of large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="25,70.87,692.91,453.54,10.49;25,81.71,706.46,170.11,10.49" xml:id="b52">
	<monogr>
		<title level="m" type="main" coords="25,295.59,692.91,228.82,10.49;25,81.71,706.46,138.74,10.49">Cmath: Can your language model pass chinese elementary school math test?</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,70.87,728.98,454.90,10.49;25,81.71,742.53,444.06,10.49;26,81.71,88.58,444.50,10.49;26,81.71,102.13,444.06,10.49;26,81.71,115.68,442.70,10.49;26,81.71,129.23,444.51,10.49;26,81.71,142.78,443.03,10.49;26,81.71,156.56,305.63,9.85" xml:id="b53">
	<analytic>
		<title level="a" type="main" coords="26,462.30,88.58,63.92,10.49;26,81.71,102.13,245.60,10.49">CLUE: A chinese language understanding evaluation benchmark</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.419</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.coling-main.419" />
	</analytic>
	<monogr>
		<title level="m" coords="26,81.71,115.68,442.70,10.49;26,81.71,129.23,19.64,10.49">Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Scott</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Bel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<meeting>the 28th International Conference on Computational Linguistics, COLING 2020<address><addrLine>Barcelona, Spain (Online)</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee on Computational Linguistics</publisher>
			<date type="published" when="2020">December 8-13, 2020. 2020</date>
			<biblScope unit="page" from="4762" to="4772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,70.87,178.84,455.45,10.49;26,81.35,192.39,368.87,10.49" xml:id="b54">
	<monogr>
		<title level="m" type="main" coords="26,97.31,192.39,147.76,10.49">Open foundation models by 01</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.04652</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="26,70.87,214.91,453.54,10.49;26,81.41,228.46,443.00,10.49;26,81.71,242.00,443.12,10.49;26,81.71,255.55,442.70,10.49;26,81.71,269.10,443.01,10.49" xml:id="b55">
	<analytic>
		<title level="a" type="main" coords="26,339.21,214.91,185.20,10.49;26,81.41,228.46,70.15,10.49">HellaSwag: Can a machine really finish your sentence?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1472</idno>
		<ptr target="https://doi.org/10.18653/v1/p1" />
	</analytic>
	<monogr>
		<title level="m" coords="26,412.88,228.46,111.53,10.49;26,81.71,242.00,343.16,10.49">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,81.71,282.88,41.03,9.85" xml:id="b56">
	<monogr>
		<title/>
		<idno type="DOI">10.18653/v1/p19-1472</idno>
		<imprint>
			<biblScope unit="page" from="9" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,70.87,305.17,453.55,10.49;26,81.71,318.72,444.06,10.49;26,81.71,332.27,394.66,10.49" xml:id="b57">
	<monogr>
		<title level="m" type="main" coords="26,142.48,318.72,338.98,10.49">Atom: Low-bit quantization for efficient and accurate LLM serving</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kasikci</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2310.19102</idno>
		<idno>CoRR, abs/2310.19102</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2310.19102" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,70.87,354.78,453.55,10.49;26,81.29,368.33,443.12,10.49;26,81.71,381.88,444.06,10.49;26,81.71,395.43,444.60,10.49;26,81.71,408.98,394.66,10.49" xml:id="b58">
	<analytic>
		<title level="a" type="main" coords="26,239.20,354.78,266.93,10.49">Chid: A large-scale chinese idiom dataset for cloze test</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1075</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1075" />
	</analytic>
	<monogr>
		<title level="m" coords="26,340.86,368.33,183.55,10.49;26,81.71,381.88,275.11,10.49;26,159.86,395.43,57.85,10.49">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="778" to="787" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="26,70.87,431.49,454.91,10.49;26,81.71,445.04,442.70,10.49;26,81.71,458.59,56.60,10.49" xml:id="b59">
	<monogr>
		<title level="m" type="main" coords="26,275.53,445.04,248.88,10.49;26,81.71,458.59,24.44,10.49">Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,70.87,481.11,453.97,10.49;26,81.71,494.66,444.61,10.49;26,81.71,508.21,443.20,10.49" xml:id="b60">
	<monogr>
		<title level="m" type="main" coords="26,469.39,481.11,55.44,10.49;26,81.71,494.66,294.83,10.49">AGIEval: A human-centric benchmark for evaluating foundation models</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Saied</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.06364</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2304.06364" />
		<imprint>
			<publisher>CoRR</publisher>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,70.87,530.72,453.55,10.49;26,81.71,544.27,400.13,10.49" xml:id="b61">
	<analytic>
		<title level="a" type="main" coords="26,491.97,530.72,32.45,10.49;26,81.71,544.27,90.82,10.49">Less is more for alignment</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="26,181.63,544.27,251.31,10.49">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,70.87,566.79,453.54,10.49;26,81.71,580.34,368.82,10.49" xml:id="b62">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.07911</idno>
		<title level="m" coords="26,423.38,566.79,101.03,10.49;26,81.71,580.34,176.87,10.49">Instruction-following evaluation for large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
