<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coords="1,146.01,82.66,319.98,16.15;1,143.42,102.59,325.16,16.15">Granite Code Models: A Family of Open Foundation Models for Code Intelligence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-07">7 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,114.34,146.84,72.78,9.34"><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.71,146.84,61.70,9.34"><forename type="first">Matt</forename><surname>Stallone</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.00,146.84,73.34,9.34"><forename type="first">Gaoyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.94,146.84,57.84,9.34"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,433.74,146.84,63.92,9.34"><forename type="first">Aditya</forename><surname>Prasad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,117.78,157.80,90.77,9.34"><forename type="first">Adriana</forename><forename type="middle">Meza</forename><surname>Soria</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.51,157.80,70.56,9.34"><forename type="first">Michele</forename><surname>Merler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.03,157.80,102.66,9.34"><forename type="first">Parameswaran</forename><surname>Selvam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.65,157.80,82.18,9.34"><forename type="first">Saptha</forename><surname>Surendran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,118.06,168.76,72.78,9.34"><forename type="first">Shivdeep</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.80,168.76,61.14,9.34"><forename type="first">Manish</forename><surname>Sethi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.90,168.76,79.98,9.34"><forename type="first">Xuan-Hong</forename><surname>Dang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.85,168.76,57.29,9.34"><forename type="first">Pengyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,429.09,168.76,64.85,9.34"><forename type="first">Kun-Lung</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,123.24,179.72,56.18,9.34"><forename type="first">Syed</forename><surname>Zawad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.38,179.72,80.53,9.34"><forename type="first">Andrew</forename><surname>Coleman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.87,179.72,71.10,9.34"><forename type="first">Matthew</forename><surname>White</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.93,179.72,54.51,9.34"><forename type="first">Mark</forename><surname>Lewis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.40,179.72,63.36,9.34"><forename type="first">Raju</forename><surname>Pavuluri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,130.35,190.67,62.10,9.34"><forename type="first">Yan</forename><surname>Koyfman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.40,190.67,73.32,9.34"><forename type="first">Boris</forename><surname>Lublinsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.69,190.67,99.62,9.34"><forename type="first">Maximilien</forename><surname>De Bayser</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.27,190.67,86.06,9.34"><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,108.00,201.63,52.52,9.34"><forename type="first">Kinjal</forename><surname>Basu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,170.49,201.63,77.56,9.34"><forename type="first">Mayank</forename><surname>Agarwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.01,201.63,35.67,9.34"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.65,201.63,63.95,9.34"><forename type="first">Chris</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.56,201.63,66.67,9.34"><forename type="first">Aanchal</forename><surname>Goyal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,454.20,201.63,49.80,9.34"><forename type="first">Hima</forename><surname>Patel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,121.77,212.59,56.55,9.34"><forename type="first">Yousaf</forename><surname>Shah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,188.28,212.59,60.05,9.34"><forename type="first">Petros</forename><surname>Zerfos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.30,212.59,66.13,9.34"><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.39,212.59,71.11,9.34"><forename type="first">Asim</forename><surname>Munawar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,415.47,212.59,74.43,9.34"><forename type="first">Maxwell</forename><surname>Crouse</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,115.02,223.55,87.16,9.34"><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.15,223.55,67.79,9.34"><forename type="first">Shweta</forename><surname>Salaria</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.90,223.55,45.11,9.34"><forename type="first">Bob</forename><surname>Calio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.97,223.55,54.88,9.34"><forename type="first">Sophia</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.82,223.55,87.16,9.34"><forename type="first">Seetharami</forename><surname>Seelam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,120.59,234.51,73.34,9.34"><forename type="first">Brian</forename><surname>Belgodere</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.89,234.51,67.79,9.34"><forename type="first">Carlos</forename><surname>Fonseca</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,281.65,234.51,68.90,9.34"><forename type="first">Amith</forename><surname>Singhee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.52,234.51,59.48,9.34"><forename type="first">Nirmit</forename><surname>Desai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,429.96,234.51,61.71,9.34"><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.97,247.03,52.84,9.34;1,283.91,244.96,1.89,7.28"><forename type="first">Ruchir</forename><surname>Puri</surname></persName>
							<email>ruchir@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.25,247.03,78.31,9.34;1,376.65,244.96,1.89,7.28"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
							<email>rpanda@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research ⋆ Equal Contribution</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coords="1,146.01,82.66,319.98,16.15;1,143.42,102.59,325.16,16.15">Granite Code Models: A Family of Open Foundation Models for Code Intelligence</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-07">7 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">784578C5529D7E789934DB4CD8600249</idno>
					<idno type="arXiv">arXiv:2405.04324v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-11T09:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile "all around" code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the last several decades, software has been woven into the fabric of every aspect of our society. As demand for software development surges, it is more critical than ever to increase software development productivity, and LLMs provide promising path for augmenting human programmers. Prominent enterprise use cases for LLMs in software development productivity include code generation, code explanation, code fixing, unit test and documentation generation, application modernization, vulnerability detection, code translation, and more.</p><p>Recent years have seen rapid progress in LLM's ability to generate and manipulate code, and a range of models with impressive coding abilities are available today. Models range in  <ref type="bibr" coords="2,285.98,412.83,108.29,9.58" target="#b50">(Muennighoff et al., 2023)</ref>, spanning 3 coding tasks and 6 programming languages. See Tables <ref type="table" coords="2,296.44,423.79,8.54,9.58">3,</ref><ref type="table" coords="2,304.98,423.79,8.54,9.58" target="#tab_11">10</ref>,11 for more details. Best viewed in color.</p><p>size from single-digit billions of parameters (e.g. Llama-7B <ref type="bibr" coords="2,369.64,464.06,90.76,9.58" target="#b66">(Touvron et al., 2023)</ref>, Gemma-7B (Gemma- <ref type="bibr" coords="2,164.94,475.02,77.10,9.58" target="#b17">Team et al., 2024)</ref>, etc.) to hundreds of billions: DBRX <ref type="bibr" coords="2,415.84,475.02,53.27,9.58">(Databricks)</ref>, Arctic <ref type="bibr" coords="2,107.67,485.98,51.66,9.58">(Snowflake)</ref>, Grok, Mixtral 8x22B (MistralAI), Command R+ (Cohere), and vary in the generality of intended use, with some models aiming to cover a range of uses outside of code, while others focus primarily on coding-related tasks (e.g. StarCoder <ref type="bibr" coords="2,436.79,507.89,68.45,9.58">(Li et al., 2023a;</ref><ref type="bibr" coords="2,108.00,518.85,87.41,9.58" target="#b47">Lozhkov et al., 2024)</ref>, CodeGen <ref type="bibr" coords="2,245.49,518.85,90.58,9.58" target="#b52">(Nijkamp et al., 2023)</ref>, <ref type="bibr" coords="2,342.78,518.85,137.91,9.58">CodeLlama (Rozière et al., 2023)</ref>, and CodeGemma (CodeGemma <ref type="bibr" coords="2,232.92,529.81,72.84,9.58" target="#b17">Team et al., 2024)</ref>).</p><p>However, there remain important gaps in the current field of LLMs for code, especially in the context of enterprise software development. First, while very large, generalist LLMs can achieve excellent coding performance, their size makes them expensive to deploy. Smaller code-focused models <ref type="bibr" coords="2,201.10,579.63,65.94,9.58">(Li et al., 2023a;</ref><ref type="bibr" coords="2,269.23,579.63,87.14,9.58" target="#b47">Lozhkov et al., 2024;</ref><ref type="bibr" coords="2,358.57,579.63,87.04,9.58" target="#b52">Nijkamp et al., 2023;</ref><ref type="bibr" coords="2,447.79,579.63,57.45,9.58;2,108.00,590.58,22.26,9.58">Rozière et al., 2023;</ref><ref type="bibr" coords="2,132.76,590.58,135.30,9.58" target="#b17">CodeGemma Team et al., 2024)</ref> can achieve excellent code generation performance in a smaller and more flexible package, but performance in coding tasks beyond generation (e.g. fixing and explanation) can lag behind code generation performance.</p><p>In many enterprise contexts, code LLM adoption can be further complicated by factors beyond the performance of the models. For instance, even open models are sometimes plagued by a lack of transparency about the data sources and data processing methods that went into model, which can erode trust in models in mission critical and regulated contexts. Furthermore, license terms in today's open LLMs can encumber and complicate an enterprise's ability to use a model.</p><p>Here, we present Granite Code models, a series of highly capable code LLMs, designed to support enterprise software development across a wide range of coding tasks. Granite Code models has two main variants that we release in four different sizes (3B, 8B, 20B, and 34B):</p><p>• Granite Code Base: base foundation models for code-related tasks;</p><p>• Granite Code Instruct: instruction following models finetuned using a combination of Git commits paired with human instructions and open-source synthetically generated code instruction datasets.</p><p>The base models in the series have been trained from scratch with a two-phase training strategy. In phase 1, our model is trained on 3 to 4 trillion tokens sourced from 116 programming languages, ensuring a comprehensive understanding of programming languages and syntax. In phase 2, our model is further trained on 500 billion tokens with a carefully designed mixture of high-quality data from code and natural language domains to improve the model's ability to reason. We use the unsupervised language modeling objective to train the base models in both the phases of training. The instruct models are derived by further finetuning the above trained base models on a combination of a filtered variant of CommitPack <ref type="bibr" coords="3,169.13,235.54,114.00,9.58" target="#b50">(Muennighoff et al., 2023)</ref>, natural language instruction following datasets (OASST <ref type="bibr" coords="3,146.34,246.48,76.44,9.61">(K öpf et al., 2023)</ref>, HelpSteer <ref type="bibr" coords="3,276.92,246.50,80.62,9.58" target="#b69">(Wang et al., 2023)</ref>) and open-source math datasets (MathInstruct <ref type="bibr" coords="3,171.09,257.46,71.43,9.58" target="#b73">(Yue et al., 2023)</ref> and MetaMathQA <ref type="bibr" coords="3,328.32,257.46,64.17,9.58" target="#b39">(Yu et al., 2023)</ref>), including synthetically generated code datasets for improving instruction following and reasoning capabilities.</p><p>We conduct extensive evaluations of our code LLMs on a comprehensive set of benchmarks, including HumanEvalPack <ref type="bibr" coords="3,233.00,296.32,114.47,9.58" target="#b50">(Muennighoff et al., 2023)</ref>, MBPP(+) <ref type="bibr" coords="3,398.86,296.32,86.90,9.58" target="#b9">(Austin et al., 2021;</ref><ref type="bibr" coords="3,488.70,296.32,15.29,9.58;3,108.00,307.28,50.78,9.58">Liu et al., 2023a)</ref>, RepoBench <ref type="bibr" coords="3,216.38,307.28,71.44,9.58">(Liu et al., 2023b)</ref>, ReCode <ref type="bibr" coords="3,330.85,307.28,76.34,9.58" target="#b68">(Wang et al., 2022)</ref>, and more. This set of benchmarks encompasses many different kinds of coding tasks beyond just code synthesis in Python, e.g., code fixing, code explanation, code editing, code translation, etc., across most major programming languages (Python, JavaScript, Java, Go, C++, Rust, etc.).</p><p>Our findings reveal that among open-source models, the Granite Code models overall show very strong performance across all model sizes and benchmarks (often outperforming other open-source code models that are twice large compared to Granite). As an illustration, figure 1 (top) shows a comparison of Granite-8B-Code-Base with other open-source base code LLMs, including recent high-performing general purpose base LLMs like Mistral <ref type="bibr" coords="3,455.32,400.92,49.92,9.58;3,108.00,411.88,28.23,9.58">(Jiang et al., 2023b)</ref> and LLama-3 (AI@Meta, 2024) on HumanEvalPack <ref type="bibr" coords="3,361.65,411.88,109.19,9.58" target="#b50">(Muennighoff et al., 2023)</ref>. While CodeGemma and StarCoder2 perform reasonably well in generating code, they perform significantly worse on the code fixing and explanation variants of HumanEvalPack. On average, Granite-8B-Code-Base outperforms the most competitive CodeGemma-8B model by almost 12 points on HumanEvalPack (33.2% vs 21.3%), despite being trained on significantly less number of tokens (4.5T vs 7.5T tokens). Besides base models, the instruction tuned variants of our Granite Code models also show strong performance on HumanEvalPack, outperforming other open-source (code) instruction models, demonstrating benefits to a wider set of coding tasks with natural language instructions (see figure <ref type="figure" coords="3,422.26,499.55,4.98,9.58" target="#fig_0">1</ref> (bottom)).</p><p>Furthermore, since reasoning is critical for solving complicated questions and tasks, we also test our Granite-8B-Code-Base model on six mathematical benchmarks, including MATH <ref type="bibr" coords="3,141.06,538.41,80.79,9.58" target="#b16">(Cobbe et al., 2021)</ref>, GSM8K <ref type="bibr" coords="3,265.26,538.41,82.56,9.58" target="#b16">(Cobbe et al., 2021)</ref> and problem solving with access to computational tools, where our Granite 8B model achieves better performance compared to most state-of-the-art 7B or 8B LLMs. For example, Granite-8B-Code-Base outperforms Llama-3-8B-Base by ∼12 points on GSM8K and ∼6 points on MATH (see table <ref type="table" coords="3,455.16,571.29,7.89,9.58" target="#tab_18">15</ref>).</p><p>The key advantages of Granite Code models include:</p><p>• All-rounder Code LLM: Granite Code models achieve competitive or state-of-theart performance on different kinds of code-related tasks, including code generation, explanation, fixing, editing, translation, etc., demonstrating their ability to solve diverse coding tasks;</p><p>• Trustworthy Enterprise-Grade LLM: All our models are trained on licensepermissible data collected following IBM's AI Ethics principles<ref type="foot" coords="3,430.11,672.56,3.79,7.28" target="#foot_0">1</ref> and guided by IBM's Corporate Legal team for trustworthy enterprise usage. All the Granite Code models are released under the Apache 2.0 license.</p><p>We describe our entire data collection, filtering, and preprocessing pipeline in section 2. Section 3 describes the details of model architecture, followed by training details in Section 4. Section 5 provides the details about instruction tuning, and Section 6 describes the experiments and results comparing Granite Code models with other open-source LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Collection</head><p>In this section, we describe the process of crawling and filtering (Sec. 2.1), deduplication (Sec. 2.2), HAP/PII filtering (Sec. 2.3) used to prepare the code data for model training. We also provide an overview of high-quality natural language data used to enhance the model's language understanding and mathematical reasoning skills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Crawling and Filtering</head><p>The pretraining code data was sourced from a combination of publicly available datasets like Github Code Clean<ref type="foot" coords="4,208.80,253.22,3.79,7.28" target="#foot_1">2</ref> , StarCoderdata<ref type="foot" coords="4,280.33,253.22,3.79,7.28" target="#foot_2">3</ref> , and additional public code repositories and issues from GitHub. We filter raw data to retain a list of 116 programming languages out of 300+ languages, as listed in Appendix A. The assignment of data to programming languages is performed based solely on file extension, similar to StarCoder <ref type="bibr" coords="4,402.08,288.09,69.61,9.58">(Li et al., 2023a)</ref>. After language filtering, we apply four key filtering rules to filter out lower-quality code <ref type="bibr" coords="4,467.62,299.05,37.62,9.58;4,108.00,310.01,26.43,9.58">(Li et al., 2023a)</ref>: (1) remove files with fewer than 25% alphabetic characters, (2) except for the XSLT language, filter out files where the string "&lt;?xml version=" appears within the first 100 characters, (3) for HTML files, only keep files where the visible text makes up at least 20% of the HTML code and has a minimum length of 100 characters, (4) for JSON and YAML files, only keep files that have a character count ranging from 50 to 5000 characters. We also filter GitHub issues using a set of quality metrics that include removing auto-generated text, filtering out non-English issues, excluding comments from bots, and using the number of users engaged in the conversation as an indicator of quality. We also annotate each code file with license information associated with the respective repository, found via Github APIs and only keep files with permissive licenses for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Exact and Fuzzy Deduplication</head><p>We adopt an aggressive deduplication strategy including both exact and fuzzy deduplication to remove documents having (near) identical code content in our training set. For exact deduplication, we first compute SHA256 hash on the document content and remove records having identical hashes. Post exact deduplication, we apply fuzzy deduplication with the goal of removing code files that may have slight variations and thereby unbiasing the data further. We apply a two-step method for this: (1) compute MinHashes of all the documents and then utilize Locally Sensitive Hashing (LSH) to group documents based on their MinHash fingerprints, (2) measure Jaccard similarity between each pair of documents in the same bucket and annotate documents except one as duplicates based on a similarity threshold of 0.7. We apply this near-deduplication process to all programming languages including GitHub issues to enhance the richness and diversity of the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">HAP, PII, Malware Filtering</head><p>To reduce the likelihood of generating hateful, abusive, or profane (HAP) language from the models, we make diligent efforts to filter HAP content from the training set. We first create a dictionary of HAP keywords and then annotate each code document with the number of occurrences of such keywords in the content including comments. We filter out documents which exceeds the HAP threshold, computed based on a distributional analysis as well as manual inspection of code files. Moreover, to protect privacy, we follow StarCoder <ref type="bibr" coords="4,491.45,658.48,12.55,9.58;4,108.00,669.44,55.48,9.58">(Li et al., 2023a)</ref> and make diligent efforts to redact Personally Identifiable Information (PII) from the training set. Specifically, we leverage the StarPII 4 model to detect IP addresses, keys, email addresses, names, user names, and passwords found in the content. The PII redaction step replaces the PII text with the corresponding tokens ⟨NAME⟩, ⟨EMAIL⟩, ⟨KEY⟩, ⟨PASSWORD⟩ and change the IP address with a synthetically generated IP address, as in <ref type="bibr" coords="5,132.12,119.56,67.09,9.58">Li et al. (2023a)</ref>. We also scan our datasets using ClamAV<ref type="foot" coords="5,392.85,117.57,3.79,7.28" target="#foot_4">5</ref> to identify and remove instances of malware in the source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Natural Language Datasets</head><p>In addition to collecting code data for model training, we curate several publicly available high-quality natural language datasets for improving the model's proficiency in language understanding and mathematical reasoning. Representative datasets under this category include web documents (Stackexchange, CommonCrawl), mathematical web text (OpenWeb-Math; <ref type="bibr" coords="5,136.73,218.47,78.73,9.58" target="#b54">Paster et al. (2023)</ref>, StackMathQA; Zhang (2024)), academic text (Arxiv, Wikipedia), and instruction tuning datasets (FLAN; <ref type="bibr" coords="5,281.40,229.43,87.01,9.58" target="#b46">Longpre et al. (2023)</ref>, HelpSteer <ref type="bibr" coords="5,420.77,229.43,76.79,9.58" target="#b69">(Wang et al., 2023)</ref>).</p><p>We do not deduplicate these already preprocessed natural language datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Architecture</head><p>We train a series of code models of varying sizes based on the transformer decoder architecture <ref type="bibr" coords="5,128.13,297.12,89.92,9.58" target="#b67">(Vaswani et al., 2017)</ref>. The model hyperparameters for these models are given in Table <ref type="table" coords="5,107.50,308.08,3.73,9.58" target="#tab_0">1</ref>. For all model architectures, we use pre-normalization <ref type="bibr" coords="5,355.42,308.08,79.25,9.58" target="#b70">(Xiong et al., 2020)</ref>: normalization applied to the input of attention and MLP blocks.  <ref type="bibr" coords="5,107.67,519.85,66.15,9.58" target="#b65">(Su et al., 2023)</ref> and Multi-Head Attention <ref type="bibr" coords="5,294.78,519.85,90.59,9.58" target="#b67">(Vaswani et al., 2017)</ref>. This model use the swish activation function <ref type="bibr" coords="5,192.68,530.80,121.02,9.58" target="#b57">(Ramachandran et al., 2017)</ref> with GLU <ref type="bibr" coords="5,362.48,530.80,65.39,9.58" target="#b61">(Shazeer, 2020)</ref> for the MLP, also commonly referred to as swiglu. For normalization, we use RMSNorm <ref type="bibr" coords="5,419.76,541.76,85.48,9.58;5,108.00,552.72,18.97,9.58" target="#b74">(Zhang &amp; Sennrich, 2019</ref>) since it's computationally more efficient than LayerNorm <ref type="bibr" coords="5,397.27,552.72,66.68,9.58" target="#b11">(Ba et al., 2016)</ref>. The 3B model is trained with a context length of 2048 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8B:</head><p>The 8B model has a similar architecture as the 3B model with the exception of using Grouped-Query Attention (GQA) <ref type="bibr" coords="5,259.22,591.58,84.70,9.58" target="#b7">(Ainslie et al., 2023)</ref>. Using GQA offers a better tradeoff between model performance and inference efficiency at this scale. We train the 8B model with a context length of 4096 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20B:</head><p>The 20B code model is trained with learned absolute position embeddings. We use Multi-Query Attention <ref type="bibr" coords="5,211.68,641.39,66.41,9.58" target="#b61">(Shazeer, 2019)</ref> during training for efficient downstream inference.</p><p>For the MLP block, we use the GELU activation function <ref type="bibr" coords="5,358.43,652.35,123.55,9.58">(Hendrycks &amp; Gimpel, 2023)</ref>. For normalizing the activations, we use LayerNorm <ref type="bibr" coords="5,318.41,663.31,63.48,9.58" target="#b11">(Ba et al., 2016)</ref>. This model is trained with a context length of 8192 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>34B:</head><p>To train the 34B model, we follow the approach by <ref type="bibr" coords="5,358.43,691.20,59.12,9.58">Kim et al. for</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Pretraining</head><p>In this section, we provide details on two phase training (Sec. 4.1), training objectives (Sec. 4.2), optimization (Sec. 4.3) and infrastructure (Sec. 4.4) used in pretraining the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Two Phase Training</head><p>Granite Code models are trained on 3.5T to 4.5T tokens of code data and natural language datasets related to code. Data is tokenized via byte pair encoding (BPE, <ref type="bibr" coords="6,435.27,440.23,69.97,9.58;6,108.00,451.19,20.67,9.58" target="#b60">(Sennrich et al., 2015)</ref>), employing the same tokenizer as StarCoder <ref type="bibr" coords="6,333.28,451.19,66.78,9.58">(Li et al., 2023a)</ref>. Following <ref type="bibr" coords="6,454.63,451.19,50.62,9.58;6,108.00,462.15,22.42,9.58" target="#b62">(Shen et al., 2024;</ref><ref type="bibr" coords="6,132.91,462.15,64.09,9.58" target="#b29">Hu et al., 2024)</ref>, we utilize high-quality data with two phases of training as follows.</p><p>• Phase 1 (code only training): During phase 1, both 3B and 8B models are trained for 4 trillion tokens of code data comprising 116 languages. The 20B parameter model is trained on 3 trillion tokens of code. The 34B model is trained on 1.4T tokens after the depth upscaling which is done on the 1.6T checkpoint of 20B model. • Phase 2 (code + language training): In phase 2, we include additional high-quality publicly available data from various domains, including technical, mathematics, and web documents, to further improve the model's performance in reasoning and problem solving skills, which are essential for code generation. We train all our models for 500B tokens (80% code and 20% language data) in phase 2 training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Objective</head><p>For training of all our models, we use the causal language modeling objective and Fill-Inthe-Middle (FIM) <ref type="bibr" coords="6,189.20,631.41,96.67,9.58" target="#b12">(Bavarian et al., 2022)</ref> objective. The FIM objective is tasked to predict inserted tokens with the given context and subsequent text. We train our models to work with both PSM (Prefix-Suffix-Middle) and SPM (Suffix-Prefix-Middle) modes, with relevant formatting control tokens, same as StarCoder <ref type="bibr" coords="6,309.33,664.28,66.98,9.58">(Li et al., 2023a)</ref>.</p><p>The overall loss is computed as a weighted combination of the 2 objectives:</p><formula xml:id="formula_0" coords="6,246.43,696.36,258.57,10.97">L = αL CLM + (1 -α)L FI M<label>(1)</label></formula><p>We emperically set α = 0.5 during training and find that this works well in practice leading to SOTA performance on both code completion and code infilling tasks. It should be noted that the FIM objective is only used during pretraining, however we drop it during instruction finetuning i.e we set α = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization</head><p>We use AdamW optimizer <ref type="bibr" coords="7,226.30,141.14,90.20,9.58" target="#b36">(Kingma &amp; Ba, 2017)</ref> with β 1 = 0.9, β 2 = 0.95 and weight decay of 0.1 for training all our Granite code models. For the phase-1 pretraining, the learning rate follows a cosine schedule starting from 3 × 10 -4 which decays to 3 × 10 -5 with an initial linear warmup step of 2k iterations. For phase-2 pretraining, we start from 3 × 10 -4 (1.5 × 10 -4 for 20B and 34B models) and adopt an exponential decay schedule to anneal it to 10% of the initial learning rate. We use a batch size of 4M-5M tokens depending on the model size during both phases of pretraining.</p><p>To accelerate training, we use FlashAttention 2 <ref type="bibr" coords="7,317.00,229.07,72.81,9.58" target="#b20">(Dao et al., 2022;</ref><ref type="bibr" coords="7,392.31,229.07,44.94,9.58" target="#b19">Dao, 2023)</ref>, the persistent layernorm kernel, Fused RMSNorm kernel (depending on the model) and the Fused Adam kernel available in NVIDIA's Apex library. We use a custom fork of NVIDIA's Megatron-LM <ref type="bibr" coords="7,126.15,261.95,90.03,9.58" target="#b63">(Shoeybi et al., 2019;</ref><ref type="bibr" coords="7,218.69,261.95,101.74,9.58" target="#b51">Narayanan et al., 2021)</ref> for distributed training of all our models. We train with a mix of 3D parallelism: tensor parallel, pipeline parallel and data parallel.</p><p>We also use sequence parallelism <ref type="bibr" coords="7,265.32,283.87,111.88,9.58">(Korthikanti et al., 2023)</ref> for reducing the activation memory consumption of large context length during training. We use Megatron's distributed optimizer with mixed precision training <ref type="bibr" coords="7,285.47,305.78,110.21,9.58" target="#b48">(Micikevicius et al., 2018)</ref> in BF16 <ref type="bibr" coords="7,433.20,305.78,72.05,9.58;7,108.00,316.74,23.24,9.58" target="#b34">(Kalamkar et al., 2019)</ref> with gradient all-reduce and gradient accumulation in FP32 for training stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Infrastructure</head><p>We train the Granite Code models using IBM's two supercomputing clusters, namely Vela and Blue Vela, outfitted with NVIDIA A100 and H100 GPUs, respectively. In the Vela A100 GPU cluster, each node has 2× Intel Xeon Scalable Processors with 8× 80GB A100 GPUs connected to each other by NVLink and NVSwitch. The Vela cluster adopts RoCE (RDMA over Converged Ethernet) and GDR (GPU-direct RDMA) for high-performance networking. Similarly, each node in Blue Vela cluster consists of dual 48-core Intel processors with 8× 80GB H100 GPUs. Blue Vela employs 3.2Tbps InfiniBand interconnect to facilitate seamless communication between nodes, known for their high throughput and low latency.</p><p>In addition, Blue Vela employs a separate, dedicated InfiniBand Storage fabric providing 800Gbps per compute node, backed by multiple ESS6000 storage appliances. Both clusters provide a scalable and efficient infrastructure for training our models over thousands of GPUs. We estimate the carbon emissions from pretraining the Granite Code models to be ∼455 tCO 2 eq, which is computed based on the total energy usage in the models and US national average carbon intensity factor of 0.423 kg CO 2 eq/KWh without taking the location of data centers in consideration. The Blue Vela cluster runs on 100% renewable energy to minimize the environmental impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Instruction Tuning</head><p>Finetuning code LLMs on a variety of tasks explained via instructions has been shown to improve model usability and general performance. While there has been much progress in code instruction tuning, most of them adopt synthetically generated data from OpenAI models, which limits the model use in many enterprise applications. Thus, following OctoCoder <ref type="bibr" coords="7,158.83,622.69,111.28,9.58" target="#b50">(Muennighoff et al., 2023)</ref>, we use only a combination of permissively licensed data, with an aim to enhance instruction following capabilities of our models, including logical reasoning and problem-solving skills. Specifically, Granite Code Instruct models are trained on the following types of data.</p><p>• Code Commits Dataset: CommitPackFT <ref type="bibr" coords="7,327.14,678.37,112.11,9.58" target="#b50">(Muennighoff et al., 2023)</ref> • Math Datasets: MathInstruct 7 <ref type="bibr" coords="8,279.15,356.71,71.31,9.58" target="#b73">(Yue et al., 2023)</ref> and MetaMathQA <ref type="bibr" coords="8,436.11,356.71,64.75,9.58" target="#b39">(Yu et al., 2023)</ref>;</p><p>• Code Instruction Datasets: Glaive-Code-Assistant-v3 8 , Self-OSS-Instruct-SC2 9 , Glaive-Function-Calling-v2 10 , NL2SQL 11 and few synthetically generated API calling datasets (Basu et al., 2024);</p><p>• Language Instruction Datasets: High-quality datasets like HelpSteer <ref type="bibr" coords="8,450.69,409.59,54.56,9.58;8,143.87,422.30,21.87,9.58" target="#b69">(Wang et al., 2023)</ref>, an open license-filtered version of Platypus 12 <ref type="bibr" coords="8,378.14,422.30,72.28,9.58" target="#b40">(Lee et al., 2023)</ref> including a collection of hardcoded prompts to ensure model generates correct outputs given inquiries about its name or developers.</p><p>For training, we use a cosine scheduler with 250 warmup steps, an initial learning rate 10 -5 , and train for three epochs. Further, we add random, uniform noise with a magnitude of 5 √ Nh</p><p>, where N is the sequence length and h is the embedding dimension, to the embedding vector, as proposed by Jain et al.. The additional noise improved overall answer quality of the instruction model. We use FlashAttention 2 <ref type="bibr" coords="8,342.98,517.54,50.78,9.58" target="#b19">(Dao, 2023;</ref><ref type="bibr" coords="8,397.27,517.54,74.00,9.58" target="#b20">Dao et al., 2022)</ref> with a Padding-Free Transformer 13 implementation to reduce GPU memory usage and redundant FLOPs during finetuning. We also use full activation checkpointing <ref type="bibr" coords="8,399.52,540.54,101.51,9.58">(Korthikanti et al., 2023)</ref>, which allows us to finetune our Granite-20B-Code models with 8K context length within a single node within a few hours on 8×A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We evaluate Granite Code models on a wide variety of tasks, including code generation, code explanation, code fixing, code editing, math reasoning, etc., as shown in Table <ref type="table" coords="8,440.79,624.80,3.66,9.58" target="#tab_3">2</ref>. We compare our models with several open-source code LLMs: StableCode <ref type="bibr" coords="8,390.44,635.76,110.43,9.58" target="#b55">(Pinnaparaju et al., 2024)</ref>, 7 We removed GSM8K-RFT and Camel-Math from MathInstruct due to unknown or NC license. 8 <ref type="url" coords="8,124.14,668.93,268.55,7.47" target="https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v3">https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v3</ref> 9 <ref type="url" coords="8,124.14,679.94,314.49,7.47" target="https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k">https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k</ref> 10 <ref type="url" coords="8,124.14,691.05,275.07,7.47" target="https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2">https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2</ref> 11 <ref type="url" coords="8,124.14,702.16,304.86,7.47" target="https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction">https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction</ref> 12 <ref type="url" coords="8,124.14,713.27,257.76,7.47" target="https://huggingface.co/datasets/garage-bAInd/Open-Platypus">https://huggingface.co/datasets/garage-bAInd/Open-Platypus</ref> 13 <ref type="url" coords="8,124.14,724.38,289.84,7.47" target="https://huggingface.co/blog/mayank-mishra/padding-free-transformer">https://huggingface.co/blog/mayank-mishra/padding-free-transformer</ref> Code Llama <ref type="bibr" coords="9,164.97,85.05,87.76,9.58" target="#b58">(Roziere et al., 2023)</ref>, StarCoder <ref type="bibr" coords="9,307.54,85.05,68.47,9.58">(Li et al., 2023b)</ref>, StarCoder2 <ref type="bibr" coords="9,436.06,85.05,69.18,9.58;9,108.00,97.09,21.57,9.58" target="#b47">(Lozhkov et al., 2024)</ref>, and CodeGemma<ref type="foot" coords="9,214.54,95.10,7.57,7.28" target="#foot_6">14</ref> , including recent high-performing general purpose open LLMs like Mistral <ref type="bibr" coords="9,162.19,109.80,84.75,9.58">(Jiang et al., 2023a)</ref> and LLama-3<ref type="foot" coords="9,309.68,107.81,7.57,7.28" target="#foot_7">15</ref> . For all the benchmarks, we evaluate the baseline models (including ours) using the same script and environment for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Code Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">HumanEvalSynthesize: Multilingual Code Generation in 6 Languages</head><p>While most of the prior code LLMs evaluate code generation capabilities only on Python using HumanEval <ref type="bibr" coords="9,179.22,199.33,75.00,9.58" target="#b15">(Chen et al., 2021)</ref>, we adopt the challenging HumanEvalSynthesize <ref type="bibr" coords="9,473.67,199.33,26.65,9.58;9,108.00,210.29,77.33,9.58" target="#b50">(Muennighoff et al., 2023</ref>) benchmark in our study, which extends Python problems of Humaneval Benchmark to five additional commonly used programming languages, namely JavaScript, Java, Go, C++, Rust. We evaluate all models in a zero-shot manner using greedy decoding with completion format for the base models, and instruction template for the instructiontuned models. In constructing prompts for instruction-tuned models, we adhere to the formats provided in their official examples. We search for a suitable prompt format in the HuggingFace model card, GitHub repository, and formal publications or technical reports.</p><p>Table <ref type="table" coords="9,133.13,292.98,4.90,9.58">3</ref> shows the results on of base and instruct models HumanEvalSynthesize benchmark. Granite-3B-Code-Base is the best performing small model with +3% improvement over CodeGemma-2B. Overall, among base models, Granite Code models achieve the best average performance at 7B-8B scale, 2nd best average performance in 13B-20B size models, and is very close to the best model (falls behind StarCoder2-15B by 0.1%). While CodeLlama-34B achieves better score on HumanEval Python, Granite-34B-Code-Base achieves much better performance on other languages, leading to a 4% improvement on average across 6 languages. Among the instruct models, Granite Code models consistently outperform equivalent size CodeLlama; the 3B, 8B, and 20B models even outperform CodeLlama models that are two times larger. It's worth noting that even our smaller model, Granite-3B-Code-Instruct, surpasses the performance of CodeLlama-34B-Instruct. Further, we can also see that Granite Code models outperform much larger state-of-the-art open-source general-purpose language models, including Gemma, Mixtral, and Llama 3 series models. This shows that domain-specific code models could achieve better performance and efficiency, thus making them more suitable for cost-and performance-sensitive enterprise environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">MultiPL-E: Multilingual Code Generation in 18 Languages</head><p>MultiPL-E <ref type="bibr" coords="9,156.35,491.43,86.78,9.58" target="#b13">(Cassano et al., 2023</ref>) is a canonical benchmark for evaluating code models on a more diverse set of 18 different programming languages. On MultiPL-E, we compare all the base models on 18 languages, by sampling 50 completions per prompt at temperature 0.2 with top-p 0.95, as in <ref type="bibr" coords="9,203.74,524.30,93.72,9.58" target="#b47">(Lozhkov et al., 2024)</ref>. Table <ref type="table" coords="9,331.26,524.30,5.08,9.58" target="#tab_5">4</ref> shows the results of different models on MultiPL-E. As can be seen from the table, there is no single model that works best at every language across all model sizes. In comparison to the similarly sized open-source model CodeLlama-7B, Granite-8B-Code-Base performs the best on 16/18 programming languages. Of the medium models, StarCoder2-15B performs best. Among the large models, Granite-34B-Code-Base does better than CodeLlama-34B on most languages, demonstrating its effectiveness in code generation across a diverse set of languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">MBPP and MBPP+: Code Generation in Python</head><p>MBPP <ref type="bibr" coords="9,137.46,635.08,83.20,9.58" target="#b9">(Austin et al., 2021)</ref> and MBPP+ <ref type="bibr" coords="9,277.48,635.08,73.45,9.58">(Liu et al., 2023a)</ref> are two of the most widely studied benchmarks for evaluating code models. While the prompt for each MBPP problem includes a natural language description followed by a few tests, MBPP+ consists of 35× more tests than the original benchmarks. We use greedy decoding and report the mean pass@1 for all the models. Table <ref type="table" coords="9,186.36,678.92,4.88,9.58" target="#tab_7">5</ref> summarizes the results of different base models. As we can see, Granite-3B-Code-Base significantly outperforms CodeGemma-2B but falls short of StarCoder2-3B on Table <ref type="table" coords="10,134.50,82.76,3.81,9.58">3</ref>: Pass@1 performance on HumanEvalSynthesize benchmark <ref type="bibr" coords="10,415.53,82.76,89.71,9.58;10,108.00,93.72,21.04,9.58" target="#b50">(Muennighoff et al., 2023)</ref>. All models are evaluated using greedy decoding with completion format for the base models, and instruction template for the instruction-tuned models.   <ref type="bibr" coords="11,148.13,521.38,67.56,9.58" target="#b39">(Lai et al., 2023</ref>) is a widely studied benchmark which offers a comprehensive collection of 1,000 data science workflows across seven different libraries, from Matplotlib to TensorFlow. We use temperature 0.2 and top-p 0.95 to generate 40 samples per each library and report mean pass@1 with code completion setting for all the models up to 8B parameters. Results on DS-1000 are summarized in Table <ref type="table" coords="11,299.22,565.21,3.66,9.58" target="#tab_8">7</ref>. Of the small models, StarCoder2-3B performs the best. Granite-3B-Code-Base is in second place, outperforming CodeGemma-2B by more than 12 points on average across 7 libraries. Granite-8B-Code-Base achieves the best average performance of 34.5% outperforming all other models of the similar parameter sizes.</p><p>The Granite Code models achieve relatively high accuracy across all sizes (e.g., outperforming CodeGemma at 2B-3B scale, StarCoder2 at 7B-8B scale and CodeLlama models with half of the sizes). This shows that our Granite Code models are not only capable of generating good code but also of using libraries more accurately in real data science workflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.5">RepoBench, CrossCodeEval: Repository-Level Code Generation</head><p>Code generation in practice often occurs within the context of a repository rather than in isolated files. Thus, we use RepoBench <ref type="bibr" coords="11,283.50,712.13,76.51,9.58">(Liu et al., 2023b)</ref> and CrossCodeEval <ref type="bibr" coords="11,453.37,712.13,51.88,9.58;11,108.00,723.09,21.44,9.58" target="#b23">(Ding et al., 2024)</ref>, to evaluate repository-level code completion capabilities of different models. On RepoBench, we evaluate using level 2k across three settings: cross-file-first (12,000 data points), cross-file-random (5,000 data points), and in-file (7,000 data points). We report the average edit similarity and exact match across the settings. Following <ref type="bibr" coords="12,413.72,524.83,70.55,9.58">Liu et al. (2023b)</ref>, we set generation temperature to 0.2 and the top-p sampling parameter to 0.95 for all models. We constrain the models to generate a maximum of 64 new tokens per prompt, and the first non-empty and non-comment line of the output was selected as the prediction.</p><p>For CrossCodeEval, following <ref type="bibr" coords="12,242.20,574.64,72.26,9.58" target="#b23">Ding et al. (2024)</ref>, we use a max sequence length of 2k using the retrieve-and-generate (RG) method with OpenAI's ada embedding. We set the maximum cross-file context to 512 tokens and the max generation token to 50 tokens for all the models. We use the uniform prompt formatting in the original implementation, with a temperature of 0.2 and top-p of 0.95 for all model generations. Max sequence length was set to 8,192 for all models, with the exception of Granite-3B-Code-Base (2,048) and Granite-8B-Code-Base (4,096), given their respective context lengths.</p><p>Table <ref type="table" coords="12,133.91,657.33,5.06,9.58">6</ref> shows the performance of different models on RepoBench v1.1. Granite-3B-Code-Base demonstrates notable performance among the smaller models, with StarCoderBase-3B achieving the leading performance metrics. Among the medium models, Granite-8B-Code-Base shows very strong performance on Java, while ranks second best one in Python, with CodeGemma-7B being the best performing on both metrics. Among larger models, Granite-20B-Code not only outperforms StarCoder2-15B but also CodeLlama-34B on all 4 metrics across both programming languages. This demonstrates strong repository-level code generation capabilities of the Granite Code models despite being not trained with repo-level file packing as in <ref type="bibr" coords="13,230.63,528.36,92.37,9.58" target="#b47">(Lozhkov et al., 2024;</ref><ref type="bibr" coords="13,325.47,528.36,133.42,9.58" target="#b17">CodeGemma Team et al., 2024)</ref>; we leave this as an interesting future work to further improve performance of our models.</p><p>Results on CrossCodeEval are shown in Table <ref type="table" coords="13,312.34,556.25,3.75,9.58" target="#tab_9">8</ref>. As can be seen from the table, among the similar sized models, CodeGemma-7B is best on Python and TypeScript, while StarCoder2-7B performs best on Java and C#. Likewise, Granite-20B-Code-Base outperforms CodeLlama-13B on 3 programming languages (Python, Java, C#), while falls behind on TypeScript. Across all model sizes and programming languages, there is no single model that is best at all the metrics, similar to the findings in MultiPL-E. This indicates that achieving uniformly high performance across all programming languages remains challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.6">FIM: Infilling Evaluations</head><p>Granite Code models are trained for code completion purposes using FIM objective, as described in Sec. 4.2. We use SantaCoder-FIM benchmark <ref type="bibr" coords="13,369.51,679.25,76.37,9.58">(Allal et al., 2023)</ref>, for infilling evaluations which tests the ability of models to fill in a single line of code in Python, JavaScript, and Java solutions to HumanEval. We use greedy decoding and report the mean exact match for all the models. Table <ref type="table" coords="13,297.12,712.13,4.96,9.58" target="#tab_10">9</ref> shows that Granite Code models significantly outperforms StarCoder and StarCoder2 across all model sizes, demonstrating it to be excellent well-rounded models for code completion use cases. Moreover, we observe no performance improvement in scaling the model sizes from 8B to 34B, indicating that smaller models are often more suitable for FIM code completion tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Code Explanation and Fixing</head><p>While most of the prior code LLMs primarily focus on evaluating performance using code generation benchmarks, users may want to use these models in other challenging scenarios beyond synthesis like explaining and fixing codes. Thus, following <ref type="bibr" coords="14,415.52,387.35,89.73,9.58;14,108.00,398.31,21.29,9.58" target="#b50">(Muennighoff et al., 2023)</ref>, we test the performance of different code models on the code explanation and fixing variants of HumanEvalPack benchmark, spanning 6 different programming languages.</p><p>For both HumanEvalExplain and HumanEvalFix, we evaluate all models in a zero-shot manner using greedy decoding with completion format for the base models, and instruction template for the instruction-tuned models.</p><p>The results of the HumanEvalExplain benchmark are shown in Table <ref type="table" coords="14,408.27,459.08,8.16,9.58" target="#tab_11">10</ref>. Granite Code Base models significantly outperform other SOTA base code LLMs, including StarCoder2 and CodeGemma, by a large margin. Interestingly, Granite-8B-Code-Base beats CodeLlama-34B by 9.3% on average, while being close to CodeLlama-70B. We attribute this performance to our data mixture and base model training decisions. After instruction tuning, performance of all the base models significantly improves across languages. Among code instruct models, Granite-34B-Code-Instruct performs the best reaching the average score of 41.9%, which is very close of 41.1% score of CodeLlama-70B-Instruct. Remarkably, CodeGemma-7B-IT gains the most improvement from instruction tuning but still falls behind Granite-8b-Code-Instruct by 2.5% on average. Mixtral-8x22B-Instruct-v0.1 performs best among all models benchmarked with a significant margin, indicating the potential advantage of bigger models and training on general natural language data could potentially help on this task.</p><p>Table <ref type="table" coords="14,133.04,596.56,9.76,9.58" target="#tab_12">11</ref> reports the results on HumanEvalFix. Like HumanEvalExplain, Granite Code models base models significantly outperform other base models. Notably, Granite-8B-Code-Base again shows impressive performance, making it close to CodeLlama-70B and Llama-3-70B.</p><p>After instruction tuning, we observe a performance improvement on almost all models. Notably, our 8B and 20B instruct models achieve the best performance among models with less that has less than 34B parameters. However, we see a significant performance improvement (about 10 points) after moving to larger models with more than 34B parameters. Among large instruct models, Granite-34B-Code-Instruct performs similarly to other models with at least twice the parameters, thus having a better cost and performance balance.  rize, these results show that both our base and instruct models are capable of generating good code but also in code fixing and explanation, demonstrating their ability to solve diverse coding tasks in enterprise software development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Code Editing and Translation</head><p>CanItEdit is a recent benchmark designed to evaluate code LLMs on instructional code editing tasks. The benchmark contains 105 hand-crafted Python programs where each problem consists of a code snippet accompanied by an instruction of two types: descriptive or lazy. The goal is to modify the code according to the instruction; both lazy and descriptive instructions should lead to the same edit. Following <ref type="bibr" coords="15,350.04,701.17,91.02,9.58" target="#b14">Cassano et al. (2024)</ref>, we compare different instruction-tuned models using their corresponding instruction format, by random sampling with a temperature of 0.2 and a top-p of 0.95, with 20 completions per problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IBM</head><p>Granite Code Models Table <ref type="table" coords="16,134.11,581.21,8.47,9.58" target="#tab_3">12</ref>: Pass@1 and ExcessCode performance on CanItEdit. Pass@1 assesses functional correctness, and ExcessCode assesses conciseness and precision of code edits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Descriptive Lazy</head><p>Pass@1 (↑) ExcessCode (↓) Pass@1 (↑) ExcessCode (↓) Table <ref type="table" coords="17,134.66,82.76,8.47,9.58" target="#tab_0">13</ref>: Performance of different instruction-tuned models on CodeLingua <ref type="bibr" coords="17,457.17,82.76,48.08,9.58;17,108.00,93.72,21.44,9.58" target="#b53">(Pan et al., 2024)</ref>. We report Pass@1 for translation across five languages.  From Table <ref type="table" coords="17,159.17,381.95,8.19,9.58" target="#tab_3">12</ref>, we make the following observations on the performance of different models on CanItEdit benchmark. It shows that Granite Code models have better pass rates, as well as less presence of unnecessary code changes, compared to CodeGemma and CodeLlama. This result shows that Granite Code models can better understand users' intentions and make accurate changes to the existing code in practical situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>CodeLingua <ref type="bibr" coords="17,167.20,442.73,75.84,9.58" target="#b53">(Pan et al., 2024)</ref> is a dataset designed to test model's capabilities in code translation. It contains two sets of programs: one containing 251 programs in Java and 251 prgrams in Python sampled from <ref type="bibr" coords="17,256.57,464.64,118.31,9.58">Avatar (Ahmad et al., 2021)</ref>, and one from CodeNet <ref type="bibr" coords="17,485.19,464.64,18.80,9.58;17,108.00,475.60,52.57,9.58" target="#b56">Puri et al. (2021)</ref> containing 250 programs for each of five languages: C, C++, Go, Java and Python. For each program a set of unit tests in the form of input and expected outputs is provided. The task consists in translating each program from the source language to five target languages (the ones sampled from CodeNet). Pass@1 is used as the metric to evaluate translation accuracy. For every generation, we used greedy decoding and the suggested prompt format for each instruction tuned model. For base models, or cases where the instruction format was not specified, we used the default prompt from the dataset. Basic post-processing is applied to each generation, to remove generation artifacts such as repetition of the input instruction, source language code, target language name and formatting tokens (```, for example).</p><p>Table <ref type="table" coords="17,134.68,591.17,10.16,9.58" target="#tab_0">13</ref> shows the results on the CodeLingua benchmark. For the source languages C, C++ and Go the results reported in the table are taken directly from the runs on Codenet, whereas for Java and Python the results are reported as the average of the runs on Avatar and CodeNet. We report the numbers of Octocoder and CodeLlama from the CodeLingua leaderboard <ref type="foot" coords="17,165.80,634.76,7.57,7.28" target="#foot_8">16</ref> . The Granite Code models performs comparably to CodeGemma. It is worth noticing that the correctness of the translation is not only due to the code generated by the model, but also by the extra metadata and explanation provided as part of the answer. We tested instruction tuned models, as we observed that base models often struggle to understand the request itself to translate code. Instruct models, on the other hand, tend to add additional information besides the translated code as part of the generations. The CodeLLama family seems to suffer especially from this issue, as post-processing the generations to extract only the relevant code constitutes a non-trivial task. The CodeGemma and Granite Models on the other hand, produce a nicely formatted output that can be easily parsed. Interestingly, Go seems to be the hardest target language to translate to, while C is the source language with the highest translation success rate from, for the Granite models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Code Reasoning, Understanding and Execution</head><p>Table <ref type="table" coords="18,134.13,166.60,8.47,9.58" target="#tab_5">14</ref>: Performance on the CRUXEval benchmark. We use temperature 0.2 for pass@1 and temperature 0.8 for pass@5, both using 10 samples, as in <ref type="bibr" coords="18,376.72,177.56,66.72,9.58" target="#b26">(Gu et al., 2024)</ref>. CRUXEval <ref type="bibr" coords="18,157.77,419.48,67.97,9.58" target="#b26">(Gu et al., 2024)</ref> is a benchmark of 800 Python functions and input-output pairs, consisting of two tasks: CRUXEval-I (input prediction) and CRUXEval-O (output prediction). We use temperature 0.2 to report pass@1 and temperature 0.8 to report pass@5, both using 10 samples, as in <ref type="bibr" coords="18,184.59,452.36,90.80,9.58" target="#b47">Lozhkov et al. (2024)</ref>; <ref type="bibr" coords="18,282.08,452.36,65.14,9.58" target="#b26">Gu et al. (2024)</ref>. Table <ref type="table" coords="18,380.68,452.36,10.10,9.58" target="#tab_5">14</ref> shows that Granite Code models perform competitively with other models. Granite-3B-Code-Base outperforms CodeGemma-2B on CRUXEval-I but lags behind on CRUXEval-O. Interestingly, there is not a single model which performs consistently best at 3B parameters. However, at 7B-8B parameters, CodeGemma-7B outperforms all the models on both tasks. For the large models, Granite-34B-Code-Base lags behind CodeLlama-34B on CRUXEval-I but outperforms on CRUXEval-O. Performance on both CRUXEval-I and CRUXEval-O increases as we scale the size of the Granite Code models from 3B to 34B parameters, demonstrating the advantage of larger models for code reasoning and execution tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Math Reasoning</head><p>We use the following four widely used benchmarks to assess the mathematical reasoning capabilities of Granite-8B-Code-Base and various 7B-8B baseline models:</p><p>• MATH <ref type="bibr" coords="18,177.79,615.25,100.85,9.58" target="#b28">(Hendrycks et al., 2021)</ref>   Following <ref type="bibr" coords="19,154.00,509.95,95.76,9.58" target="#b10">Azerbayev et al. (2023)</ref>, we also evaluate models on two tasks that involve solving problems with access to computational tools:</p><p>• MATH+Py solving MATH task by writing a Python program that uses built-in numeric operations, the math module, and SymPy; we use the 5-shot prompt and experiment setting from <ref type="bibr" coords="19,252.50,563.85,98.63,9.58" target="#b10">Azerbayev et al. (2023)</ref>;</p><p>• GSM8K+Py solving GSM8K task by writing a Python program that executes to generate an integer answer; we use the 8-shot prompt and experiment setting from <ref type="bibr" coords="19,143.48,600.91,98.63,9.58" target="#b10">Azerbayev et al. (2023)</ref>.</p><p>Table <ref type="table" coords="19,134.69,621.94,10.16,9.58" target="#tab_18">15</ref> summarizes the results. Despite not being specifically tuned for mathematical reasoning, Granite-8B-Code-Base shows impressive reasoning ability, outperforming most existing 7B to 8B models. While other models may be particularly strong on a few tasks, our model consistently achieves top-1 or top-2 performance on all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Calling Functions and Tools</head><p>We adopt Berkeley Function-Calling Leaderboard (BFCL) <ref type="bibr" coords="19,372.50,701.17,73.37,9.58" target="#b71">(Yan et al., 2024)</ref>, to evaluate LLM's ability to call functions and tools. BFCL is a function-calling dataset with 1700 functions across 4 categories: simple, multiple, parallel, and parallel multiple function calls - Figure <ref type="figure" coords="20,139.45,393.29,5.06,9.58" target="#fig_4">4</ref> shows the results of different Granite Code models on BFCL benchmark. As can be seen from the figure, overall accuracy improves from 25.65% to 57.12% for Granite-3B-Code-Base to Granite-34B-Code-Base, showing the effectiveness of model scaling in function (tool) calling capabilities. We also compare Granite-8B-Code with CodeLlama-7B in Figure <ref type="figure" coords="20,499.12,426.16,4.88,9.58" target="#fig_5">5</ref> and find that Granite-8B-Code-Instruct beats CodeLlama-7B-Instruct by 22%, 14% and 12% on AST Summary, Execution Summary and Overall accuracy respectively. Additionally, Figure <ref type="figure" coords="20,140.23,459.04,5.08,9.58" target="#fig_5">5</ref> shows that instruction tuning consistently improves performance of both base models, with more noticeable improvements in Granite Code models. E.g., +17.88% in overall accuracy from Granite-8B-Code-Base to Granite-8B-Code-Instruct, indicating the effectiveness of our well-curated data mixture in finetuning base models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Model Robustness</head><p>While the performance on canonical code generative tasks is essential, we argue that the evaluation of practical robustness is also necessary to characterize different models systematically. We therefore consider benchmarking the robustness of code synthesis, one of the most representative downstream tasks of source code. ReCode <ref type="bibr" coords="20,383.48,574.64,79.59,9.58" target="#b68">(Wang et al., 2022)</ref> provides 30 different general perturbations on docstrings, function names, and codes to evaluate the robustness of code-generation models. We use the perturbed version of the HumanEval benchmark using greedy generation with 5 seeds, as recommended in <ref type="bibr" coords="20,417.79,607.52,78.48,9.58" target="#b68">(Wang et al., 2022)</ref>.</p><p>Table <ref type="table" coords="20,134.64,624.46,10.16,9.58" target="#tab_0">16</ref> shows the worst-case RP@1 of different models for each perturbation category. While Granite-3B-Code-Base consistently outperforms CodeGemma-2B, Granite-8B-Code-Base lags behind CodeGemma-7B on all categories. Granite Code models obtains much better performance compared to CodeLlama models, showing its generalization in a robust way at every sizes. Our largest model, Granite-34B-Code-Base consistently outperforms CodeLlama-34B on all four categories. This indicates that Granite-34B-Code-Base has more capacity to deal with unseen instances and perturbations. In general, we also observe higher RP@1 for larger models within the Granite Code family (e.g., improved from 40.1% to 52.0% for Granite-3B-Code-Base to Granite-34B-Code-Base on average across all perturbations), showing that larger model helps improve worst-case robustness.</p><p>Table <ref type="table" coords="21,133.76,82.76,8.38,9.58" target="#tab_0">16</ref>: RP@1 performance on the Recode benchmark. Following <ref type="bibr" coords="21,404.85,82.76,79.14,9.58" target="#b68">(Wang et al., 2022)</ref>, we use the perturbed version of the HumanEval benchmark with greedy sampling for all the models to eliminate randomness effect and enable fair comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a family of decoder-only Granite Code models ranging in size from 3 to 34 billion parameters that are highly versatile in their ability to accomplish a wide range of tasks from code generation to fixing bugs, explaining and documenting code, maintaining repositories, and more. These models have proven to be suitable for applications ranging from complex application modernization tasks (IBM, 2023) to on-device memory-constrained use cases. Extensive evaluation demonstrates that Granite Code models consistently reach state-of-the-art performance among open-source code LLMs, matching or exceeding the performance of recently released CodeGemma, StarCoder2, and Llama3 models on average performance across various code-related tasks of code generation, explanation, and bug fixing in a variety of popular programming languages. Our experience and results demonstrate that Granite Code models have a proven ability to better handle different tasks in enterprise software development workflows. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use. We plan to continuously release updates to these models to improve their performance, e.g. leveraging the CodeNet instruction dataset <ref type="bibr" coords="21,191.89,519.82,71.58,9.58" target="#b56">(Puri et al., 2021)</ref>, and in the near future we plan to release long-context as well as Python-and Java-specialized model variants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,108.00,401.87,396.99,9.58;2,108.00,412.83,396.00,9.58;2,108.00,423.79,386.81,9.58;2,108.00,81.86,396.02,304.17"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of Granite-8B-Code (Base/Instruct) with other open source (code) LLMs of similar size on HumanEvalPack (Muennighoff et al., 2023), spanning 3 coding tasks and 6 programming languages. See Tables 3,10,11 for more details. Best viewed in color.</figDesc><graphic coords="2,108.00,81.86,396.02,304.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="14,108.00,254.14,397.24,9.58;14,108.00,265.10,294.35,9.58;14,108.00,81.86,396.00,156.44"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of Granite-8B-Code-Instruct, Mistral-7B-Instruct-v0.2, Gemma-7B-IT, and Llama-3-8B-Instruct on HumanEvalPack. Best viewed in color.</figDesc><graphic coords="14,108.00,81.86,396.00,156.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="14,108.00,701.17,397.65,9.58;14,108.00,712.13,396.00,9.58;14,108.00,723.09,397.66,9.58"><head>Figure 3</head><label>3</label><figDesc>Figure3compares the performance of Granite-8B-Code-Instruct with state-of-the-art opensource instruction-tuned general LLMs. Granite-8B-Code-Instruct consistently outperforms the compared models, emphasizing the need for domain-specific code models. To summa-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="19,108.00,454.65,397.75,9.58;19,108.00,465.61,396.00,9.58;19,108.00,476.57,276.45,9.58;19,127.80,270.52,356.42,168.29"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of Granite Code models on Berkeley Function-Calling Leaderboard. Overall accuracy keeps increasing with increase in model sizes, indicating the advantage of large models for function calling abilities. Best viewed in color.</figDesc><graphic coords="19,127.80,270.52,356.42,168.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="20,108.00,277.30,397.75,9.58;20,108.00,288.26,396.99,9.58;20,108.00,299.22,180.57,9.58;20,127.80,81.86,356.42,179.60"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Granite-8B-Code vs CodeLlama-7B on Berkley Function-Calling Leaderboard. Granite-8B-Code (Base/Instruct) consistently outperforms CodeLlama-7B (Base/Instruct) on all three metrics. Best viewed in color.</figDesc><graphic coords="20,127.80,81.86,356.42,179.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,144.24,340.20,321.02,148.68"><head>Table 1 :</head><label>1</label><figDesc>Model configurations for Granite Code models.</figDesc><table coords="5,144.24,364.79,321.02,124.10"><row><cell>Model</cell><cell>3B</cell><cell>8B</cell><cell>20B</cell><cell>34B</cell></row><row><cell>Batch size</cell><cell>2048</cell><cell>1024</cell><cell>576</cell><cell>532</cell></row><row><cell>Context length</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>8192</cell></row><row><cell>Hidden size</cell><cell>2560</cell><cell>4096</cell><cell>6144</cell><cell>6144</cell></row><row><cell>FFN hidden size</cell><cell>10240</cell><cell>14336</cell><cell>24576</cell><cell>24576</cell></row><row><cell>Attention heads</cell><cell>32</cell><cell>32</cell><cell>48</cell><cell>48</cell></row><row><cell cols="2">Key-Value heads 32 (MHA)</cell><cell>8 (GQA)</cell><cell>1 (MQA)</cell><cell>1 (MQA)</cell></row><row><cell>Layers</cell><cell>32</cell><cell>36</cell><cell>52</cell><cell>88</cell></row><row><cell>Normalization</cell><cell cols="4">RMSNorm RMSNorm LayerNorm LayerNorm</cell></row><row><cell>Activation</cell><cell>swiglu</cell><cell>swiglu</cell><cell>gelu</cell><cell>gelu</cell></row><row><cell>Vocab size</cell><cell>49152</cell><cell>49152</cell><cell>49152</cell><cell>49152</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,108.00,508.89,396.00,9.58"><head>3B: The smallest model in the Granite-code model family is trained with RoPE embedding</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,108.00,691.20,396.00,20.54"><head></head><label></label><figDesc>depth upscaling of the 20B model. Specifically, we first duplicate the 20B code model with 52 layers and then Figure 2: An overview of depth upscaling (Kim et al., 2024) for efficient training of Granite-34B-Code. We utilize the 20B model after 1.6T tokens to start training of 34B model with the same code pretraining data without any changes to the training and inference framework.remove final 8 layers from the original model and initial 8 layers from its duplicate to form two models. Finally, we concatenate both models to form Granite-34B-Code model with 88 layers (see Figure2for an illustration). After the depth upscaling, we observe that the drop in performance compared to 20B model is pretty small contrary to what is observed byKim et al.. This performance is recovered pretty quickly after we continue pretraining of the upscaled 34B model. Similar, to 20B, we use a 8192 token context during pretraining.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,143.87,678.37,361.79,22.18"><head>Table 2 :</head><label>2</label><figDesc>Summary of evaluation tasks.</figDesc><table coords="8,113.98,107.35,388.65,222.73"><row><cell>Task</cell><cell>Benchmark</cell><cell>Reference</cell></row><row><cell>Multilingual code generation</cell><cell cols="2">HumanEvalSynthesize Muennighoff et al. (2023)</cell></row><row><cell>Multilingual code generation</cell><cell>MultiPL-E</cell><cell>Cassano et al. (2023)</cell></row><row><cell>Python code generation</cell><cell>MBPP</cell><cell>Austin et al. (2021)</cell></row><row><cell>Python code generation</cell><cell>MBPP+</cell><cell>Liu et al. (2023a)</cell></row><row><cell>Data science code generation</cell><cell>DS1000</cell><cell>Lai et al. (2023)</cell></row><row><cell>Repository-level code generation</cell><cell>RepoBench</cell><cell>Liu et al. (2023b)</cell></row><row><cell>Repository-level code generation</cell><cell>CrossCodeEval</cell><cell>Ding et al. (2023)</cell></row><row><cell>Fill-in-the-middle code completion</cell><cell>SantaCoder-FIM</cell><cell>Allal et al. (2023)</cell></row><row><cell>Multilingual code explanation</cell><cell>HumanEvalExplain</cell><cell>Muennighoff et al. (2023)</cell></row><row><cell cols="3">Multilingual code fixing Muennighoff et al. (2023) Math reasoning HumanEvalFix MATH Hendrycks et al. (2021)</cell></row><row><cell>Math reasoning</cell><cell>GSM8K</cell><cell>Cobbe et al. (2021)</cell></row><row><cell>Math reasoning</cell><cell>SAT</cell><cell>Azerbayev et al. (2023)</cell></row><row><cell>Math reasoning</cell><cell>OCW</cell><cell>Lewkowycz et al. (2022)</cell></row><row><cell>Function calling</cell><cell>BFCL</cell><cell>Yan et al. (2024)</cell></row><row><cell>Model robustness</cell><cell>ReCode</cell><cell>Wang et al. (2022)</cell></row></table><note coords="7,439.25,678.37,66.41,9.58;7,143.87,690.97,294.14,9.58;7,438.00,688.97,3.79,7.28;7,442.28,690.97,2.49,9.58;8,108.00,28.59,18.87,9.58;8,409.58,28.59,94.43,9.58"><p>, a filtered version of full CommitPack dataset across 92 programming languages 6 ; IBM Granite Code Models</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,107.69,82.76,396.31,425.90"><head>Table 4 :</head><label>4</label><figDesc>Pass@1 results on MultiPL-E averaged over 50 samples for each problem. All models are evaluated at temperature 0.2 and top-p 0.95.</figDesc><table coords="11,108.00,117.49,391.00,391.17"><row><cell>Model</cell><cell cols="2">C++ C#</cell><cell>D</cell><cell>Go</cell><cell cols="3">Java Julia JavaScript</cell><cell>Lua</cell><cell>PHP</cell></row><row><cell>StarCoderBase-3B</cell><cell cols="2">19.9 13.0</cell><cell>12.3</cell><cell>13.3</cell><cell>15.0</cell><cell>16.6</cell><cell>16.7</cell><cell>16.8</cell><cell>17.1</cell></row><row><cell>StableCode-3B</cell><cell cols="2">31.5 15.0</cell><cell>12.3</cell><cell>19.7</cell><cell>23.9</cell><cell>24.9</cell><cell>26.0</cell><cell>23.7</cell><cell>23.8</cell></row><row><cell>StarCoder2-3B</cell><cell cols="2">32.8 23.4</cell><cell>24.5</cell><cell>24.2</cell><cell>27.0</cell><cell>27.5</cell><cell>29.3</cell><cell>27.6</cell><cell>28.0</cell></row><row><cell>CodeGemma-2B</cell><cell cols="2">29.4 18.6</cell><cell>16.8</cell><cell>22.0</cell><cell>19.4</cell><cell>12.5</cell><cell>15.6</cell><cell>11.9</cell><cell>12.8</cell></row><row><cell cols="3">Granite-3B-Code-Base 31.6 21.5</cell><cell>22.8</cell><cell>22.7</cell><cell>25.7</cell><cell>27.0</cell><cell>27.2</cell><cell>26.8</cell><cell>27.1</cell></row><row><cell>StarCoderBase-7B</cell><cell cols="2">24.0 19.6</cell><cell>16.3</cell><cell>19.8</cell><cell>19.5</cell><cell>21.7</cell><cell>21.6</cell><cell>21.9</cell><cell>22.1</cell></row><row><cell>CodeLlama-7B</cell><cell cols="2">29.0 21.6</cell><cell>20.5</cell><cell>21.2</cell><cell>24.4</cell><cell>26.2</cell><cell>26.2</cell><cell>26.9</cell><cell>26.7</cell></row><row><cell>StarCoder2-7B</cell><cell cols="2">39.1 24.7</cell><cell>27.6</cell><cell>22.2</cell><cell>30.5</cell><cell>29.6</cell><cell>31.8</cell><cell>29.8</cell><cell>30.2</cell></row><row><cell>CodeGemma-7B</cell><cell cols="2">43.7 28.2</cell><cell>27.6</cell><cell>27.9</cell><cell>31.3</cell><cell>34.4</cell><cell>35.2</cell><cell>35.3</cell><cell>35.7</cell></row><row><cell cols="3">Granite-8B-Code-Base 44.3 21.5</cell><cell>30.2</cell><cell>28.0</cell><cell>33.1</cell><cell>33.7</cell><cell>35.5</cell><cell>33.4</cell><cell>33.8</cell></row><row><cell cols="3">StarCoderBase-15B 30.2 20.6</cell><cell>20.4</cell><cell>22.0</cell><cell>22.9</cell><cell>24.6</cell><cell>25.2</cell><cell>24.9</cell><cell>25.2</cell></row><row><cell>CodeLlama-13B</cell><cell cols="2">38.8 24.5</cell><cell>27.3</cell><cell>26.7</cell><cell>30.4</cell><cell>31.7</cell><cell>32.5</cell><cell>31.7</cell><cell>31.8</cell></row><row><cell>StarCoder2-15B</cell><cell cols="2">47.4 31.7</cell><cell>35.4</cell><cell>27.3</cell><cell>36.6</cell><cell>37.5</cell><cell>38.5</cell><cell>38.5</cell><cell>31.9</cell></row><row><cell cols="3">Granite-20B-Code-Base 43.3 30.6</cell><cell>16.2</cell><cell>29.4</cell><cell>35.9</cell><cell>31.8</cell><cell>38.9</cell><cell>30.5</cell><cell>38.3</cell></row><row><cell>CodeLlama-34B</cell><cell cols="2">45.9 31.0</cell><cell>30.5</cell><cell>28.4</cell><cell>35.2</cell><cell>36.1</cell><cell>37.0</cell><cell>36.4</cell><cell>37.1</cell></row><row><cell cols="3">Granite-34B-Code-Base 46.7 32.8</cell><cell>18.7</cell><cell>33.2</cell><cell>33.3</cell><cell>31.8</cell><cell>44.5</cell><cell>38.2</cell><cell>42.5</cell></row><row><cell>Model</cell><cell>Perl</cell><cell>R</cell><cell cols="4">Ruby Racket Rust Scala</cell><cell>Bash</cell><cell cols="2">Swift TypeScript</cell></row><row><cell>StarCoderBase-3B</cell><cell cols="2">11.0 16.2</cell><cell>14.2</cell><cell>15.3</cell><cell>14.4</cell><cell>16.4</cell><cell>4.2</cell><cell>13.2</cell><cell>22.4</cell></row><row><cell>StableCode-3B</cell><cell>9.7</cell><cell>22.5</cell><cell>18.7</cell><cell>20.7</cell><cell>19.1</cell><cell>14.6</cell><cell>8.4</cell><cell>17.5</cell><cell>29.5</cell></row><row><cell>StarCoder2-3B</cell><cell cols="2">13.2 26.7</cell><cell>25.2</cell><cell>24.6</cell><cell>25.3</cell><cell>21.1</cell><cell>12.5</cell><cell>23.2</cell><cell>35.5</cell></row><row><cell>CodeGemma-2B</cell><cell>2.5</cell><cell>11.3</cell><cell>9.9</cell><cell>10.9</cell><cell>11.6</cell><cell>25.6</cell><cell>1.6</cell><cell>10.7</cell><cell>1.3</cell></row><row><cell cols="3">Granite-3B-Code-Base 18.8 25.8</cell><cell>24.1</cell><cell>24.1</cell><cell>24.0</cell><cell>26.9</cell><cell>7.0</cell><cell>22.0</cell><cell>31.6</cell></row><row><cell>StarCoderBase-7B</cell><cell cols="2">16.8 21.1</cell><cell>19.9</cell><cell>20.0</cell><cell>20.1</cell><cell>21.2</cell><cell>7.5</cell><cell>18.5</cell><cell>27.6</cell></row><row><cell>CodeLlama-7B</cell><cell cols="2">17.4 25.7</cell><cell>24.7</cell><cell>24.2</cell><cell>24.9</cell><cell>25.5</cell><cell>10.0</cell><cell>22.8</cell><cell>33.7</cell></row><row><cell>StarCoder2-7B</cell><cell cols="2">17.6 29.0</cell><cell>27.3</cell><cell>27.2</cell><cell>27.6</cell><cell>22.0</cell><cell>13.2</cell><cell>25.3</cell><cell>37.0</cell></row><row><cell>CodeGemma-7B</cell><cell cols="2">31.2 34.0</cell><cell>33.1</cell><cell>32.0</cell><cell>33.6</cell><cell>39.0</cell><cell>11.0</cell><cell>30.8</cell><cell>45.2</cell></row><row><cell cols="3">Granite-8B-Code-Base 18.8 32.2</cell><cell>30.4</cell><cell>30.4</cell><cell>30.4</cell><cell>26.9</cell><cell>13.6</cell><cell>27.9</cell><cell>31.6</cell></row><row><cell cols="3">StarCoderBase-15B 16.8 23.9</cell><cell>22.0</cell><cell>22.6</cell><cell>22.3</cell><cell>28.6</cell><cell>11.2</cell><cell>20.4</cell><cell>32.3</cell></row><row><cell>CodeLlama-13B</cell><cell cols="2">22.5 30.1</cell><cell>28.8</cell><cell>28.4</cell><cell>29.0</cell><cell>29.7</cell><cell>13.8</cell><cell>26.6</cell><cell>40.6</cell></row><row><cell>StarCoder2-15B</cell><cell cols="2">36.6 37.2</cell><cell>36.4</cell><cell>35.9</cell><cell>36.6</cell><cell>18.7</cell><cell>18.7</cell><cell>33.5</cell><cell>43.9</cell></row><row><cell cols="3">Granite-20B-Code-Base 26.5 15.5</cell><cell>28.1</cell><cell>17.9</cell><cell>35.7</cell><cell>37.5</cell><cell>16.7</cell><cell>27.6</cell><cell>38.7</cell></row><row><cell>CodeLlama-34B</cell><cell cols="2">28.9 35.4</cell><cell>33.8</cell><cell>33.4</cell><cell>34.3</cell><cell>32.9</cell><cell>16.2</cell><cell>31.5</cell><cell>39.6</cell></row><row><cell cols="3">Granite-34B-Code-Base 31.5 25.4</cell><cell>33.9</cell><cell>18.2</cell><cell>38.9</cell><cell>41.7</cell><cell>19.3</cell><cell>36.5</cell><cell>41.5</cell></row><row><cell>6.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,108.00,499.31,203.14,31.65"><head>.4 DS1000: Data Science Tasks in Python DS</head><label></label><figDesc></figDesc><table /><note coords="11,118.55,521.38,26.36,9.58"><p>-1000</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,107.69,82.76,398.06,226.57"><head>Table 5 :</head><label>5</label><figDesc>Pass@1 on MBPP and MBPP+. Results in the table represent zero-shot evaluation using greedy decoding.</figDesc><table coords="12,113.27,83.35,392.48,225.98"><row><cell></cell><cell></cell><cell></cell><cell cols="5">Table 6: Average exact match (EM) and</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">edit similarity (ES) on RepoBench v1.1.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">All models are evaluated at temperature</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.2 and top-p 0.95.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">MBPP MBPP+</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StarCoderBase-3B</cell><cell>29.4</cell><cell>37.8</cell><cell>Model</cell><cell cols="2">Python</cell><cell cols="2">Java</cell></row><row><cell>StableCode-3B</cell><cell>34.8</cell><cell>43.3</cell><cell></cell><cell>EM</cell><cell>ES</cell><cell>EM</cell><cell>ES</cell></row><row><cell>StarCoder2-3B</cell><cell>42.4</cell><cell>48.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CodeGemma-2B</cell><cell>30.4</cell><cell>30.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Granite-3B-Code-Base</cell><cell>36.0</cell><cell>45.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StarCoderBase-7B</cell><cell>34.8</cell><cell>42.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CodeLlama-7B</cell><cell>39.0</cell><cell>42.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StarCoder2-7B</cell><cell>45.4</cell><cell>46.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CodeGemma-7B</cell><cell>53.0</cell><cell>54.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Granite-8B-Code-Base</cell><cell>42.2</cell><cell>49.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StarCoderBase-15B</cell><cell>37.4</cell><cell>46.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CodeLlama-13B</cell><cell>30.6</cell><cell>30.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StarCoder2-15B</cell><cell>51.2</cell><cell>56.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Granite-20B-Code-Base</cell><cell>43.8</cell><cell>51.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CodeLlama-34B</cell><cell>48.6</cell><cell>53.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Granite-34B-Code-Base</cell><cell>47.2</cell><cell>53.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="12,107.69,329.34,396.31,143.52"><head>Table 7 :</head><label>7</label><figDesc>Mean pass@1 accuracy averaged over 40 samples on DS-1000. All models are evaluated at temperature 0.2 and top-p 0.95.</figDesc><table coords="12,114.87,368.07,382.27,104.79"><row><cell>Model</cell><cell cols="8">Matplotlib NumPy Pandas PyTorch SciPy Scikit-Learn TensorFlow Avg</cell></row><row><cell>StarCoderBase-3B</cell><cell>32.1</cell><cell>16.8</cell><cell>5.3</cell><cell>9.2</cell><cell>13.2</cell><cell>10.5</cell><cell>17.2</cell><cell>14.2</cell></row><row><cell>StableCode-3B</cell><cell>42.5</cell><cell>24.5</cell><cell>16.2</cell><cell>15.4</cell><cell>13.5</cell><cell>20.2</cell><cell>27.7</cell><cell>22.7</cell></row><row><cell>StarCoder2-3B</cell><cell>45.5</cell><cell>27.7</cell><cell>16.2</cell><cell>12.9</cell><cell>15.8</cell><cell>30.8</cell><cell>22.8</cell><cell>25.0</cell></row><row><cell>CodeGemma-2B</cell><cell>30.3</cell><cell>17.7</cell><cell>5.5</cell><cell>4.4</cell><cell>10.3</cell><cell>2.6</cell><cell>4.4</cell><cell>10.7</cell></row><row><cell>Granite-3B-Code-Base</cell><cell>43.3</cell><cell>27.7</cell><cell>11.0</cell><cell>19.1</cell><cell>21.7</cell><cell>16.5</cell><cell>24.4</cell><cell>23.4</cell></row><row><cell>StarCoderBase-7B</cell><cell>38.0</cell><cell>23.0</cell><cell>8.2</cell><cell>13.1</cell><cell>13.7</cell><cell>24.5</cell><cell>14.6</cell><cell>19.1</cell></row><row><cell>CodeLlama-7B</cell><cell>46.3</cell><cell>21.6</cell><cell>13.9</cell><cell>12.2</cell><cell>17.5</cell><cell>16.7</cell><cell>20.6</cell><cell>21.5</cell></row><row><cell>StarCoder2-7B</cell><cell>53.6</cell><cell>33.3</cell><cell>16.9</cell><cell>16.2</cell><cell>20.6</cell><cell>22.2</cell><cell>31.9</cell><cell>27.8</cell></row><row><cell>CodeGemma-7B</cell><cell>56.1</cell><cell>37.2</cell><cell>20.9</cell><cell>20.5</cell><cell>24.5</cell><cell>34.7</cell><cell>31.1</cell><cell>32.1</cell></row><row><cell>Granite-8B-Code-Base</cell><cell>51.6</cell><cell>40.0</cell><cell>23.4</cell><cell>32.4</cell><cell>22.6</cell><cell>29.6</cell><cell>42.2</cell><cell>34.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="13,107.69,82.76,396.31,220.77"><head>Table 8 :</head><label>8</label><figDesc>CrossCodeEval (Ding et al., 2023)  evaluation results. We report Code Match (Edit Similarity) and Identifier Match (F1) results for four languages.</figDesc><table coords="13,115.14,117.59,381.73,185.95"><row><cell></cell><cell>Python</cell><cell></cell><cell>Java</cell><cell></cell><cell cols="2">TypeScript</cell><cell>C#</cell><cell></cell></row><row><cell>Model</cell><cell cols="8">Code ES ID F1 Code ES ID F1 Code ES ID F1 Code ES ID F1</cell></row><row><cell>StarCoderBase-3B</cell><cell>63.5</cell><cell>53.8</cell><cell>63.3</cell><cell>55.7</cell><cell>44.2</cell><cell>40.8</cell><cell>65.3</cell><cell>45.0</cell></row><row><cell>StableCode-3B</cell><cell>65.3</cell><cell>56.1</cell><cell>68.2</cell><cell>61.0</cell><cell>60.9</cell><cell>55.7</cell><cell>59.9</cell><cell>41.7</cell></row><row><cell>StarCoder2-3B</cell><cell>65.5</cell><cell>56.7</cell><cell>64.8</cell><cell>57.3</cell><cell>44.7</cell><cell>41.3</cell><cell>66.0</cell><cell>47.5</cell></row><row><cell>CodeGemma-2B</cell><cell>60.5</cell><cell>50.6</cell><cell>55.1</cell><cell>46.4</cell><cell>55.6</cell><cell>49.0</cell><cell>44.2</cell><cell>27.9</cell></row><row><cell>Granite-3B-Code-Base</cell><cell>65.1</cell><cell>56.0</cell><cell>64.1</cell><cell>56.6</cell><cell>43.2</cell><cell>39.4</cell><cell>65.9</cell><cell>46.6</cell></row><row><cell>StarCoderBase-7B</cell><cell>65.3</cell><cell>56.3</cell><cell>65.4</cell><cell>58.0</cell><cell>46.2</cell><cell>43.2</cell><cell>66.1</cell><cell>47.2</cell></row><row><cell>CodeLlama-7B</cell><cell>64.9</cell><cell>55.4</cell><cell>65.0</cell><cell>57.8</cell><cell>62.1</cell><cell>56.9</cell><cell>65.1</cell><cell>46.9</cell></row><row><cell>StarCoder2-7B</cell><cell>66.5</cell><cell>57.5</cell><cell>67.0</cell><cell>59.8</cell><cell>46.9</cell><cell>43.3</cell><cell>67.2</cell><cell>48.6</cell></row><row><cell>CodeGemma-7B</cell><cell>68.1</cell><cell>59.3</cell><cell>65.9</cell><cell>59.6</cell><cell>63.5</cell><cell>58.5</cell><cell>56.2</cell><cell>42.1</cell></row><row><cell>Granite-8B-Code-Base</cell><cell>66.3</cell><cell>57.8</cell><cell>66.5</cell><cell>59.0</cell><cell>45.1</cell><cell>42.0</cell><cell>66.6</cell><cell>48.7</cell></row><row><cell>StarCoderBase-15B</cell><cell>66.0</cell><cell>57.3</cell><cell>67.0</cell><cell>60.2</cell><cell>46.6</cell><cell>43.3</cell><cell>66.4</cell><cell>47.7</cell></row><row><cell>CodeLlama-13B</cell><cell>66.2</cell><cell>57.4</cell><cell>66.8</cell><cell>59.5</cell><cell>63.5</cell><cell>58.6</cell><cell>65.6</cell><cell>48.3</cell></row><row><cell>StarCoder2-15B</cell><cell>68.1</cell><cell>59.7</cell><cell>68.1</cell><cell>61.5</cell><cell>47.0</cell><cell>43.7</cell><cell>68.5</cell><cell>51.0</cell></row><row><cell>Granite-20B-Code-Base</cell><cell>68.2</cell><cell>58.1</cell><cell>68.4</cell><cell>60.4</cell><cell>48.3</cell><cell>43.1</cell><cell>67.5</cell><cell>48.4</cell></row><row><cell>CodeLlama-34B</cell><cell>69.3</cell><cell>59.6</cell><cell>68.2</cell><cell>61.1</cell><cell>64.4</cell><cell>56.9</cell><cell>67.2</cell><cell>49.6</cell></row><row><cell>Granite-34B-Code-Base</cell><cell>68.3</cell><cell>58.5</cell><cell>68.6</cell><cell>60.8</cell><cell>48.9</cell><cell>43.6</cell><cell>67.5</cell><cell>49.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="13,107.69,320.78,396.69,168.67"><head>Table 9 :</head><label>9</label><figDesc>Exact-match on FIM-task (Allal et al., 2023). All models are evaluated using greedy decoding with maximum new tokens set to 512.</figDesc><table coords="13,182.28,356.29,247.45,133.15"><row><cell>Model</cell><cell cols="4">Java JavaScript Python Avg.</cell></row><row><cell>StarCoderBase-3B</cell><cell>76.0</cell><cell>68.5</cell><cell>53.6</cell><cell>66.0</cell></row><row><cell>StableCode-3B</cell><cell>64.2</cell><cell>74.5</cell><cell>59.6</cell><cell>66.1</cell></row><row><cell>StarCoder2-3B</cell><cell>76.0</cell><cell>73.5</cell><cell>59.4</cell><cell>69.6</cell></row><row><cell>Granite-3B-Code-Base</cell><cell>79.7</cell><cell>71.6</cell><cell>61.8</cell><cell>71.0</cell></row><row><cell>StarCoderBase-7B</cell><cell>81.1</cell><cell>74.5</cell><cell>62.0</cell><cell>72.5</cell></row><row><cell>StarCoder2-7B</cell><cell>82.1</cell><cell>78.4</cell><cell>61.5</cell><cell>74.0</cell></row><row><cell>Granite-8B-Code-Base</cell><cell>83.6</cell><cell>79.9</cell><cell>66.3</cell><cell>76.6</cell></row><row><cell>StarCoderBase-15B</cell><cell>74.6</cell><cell>74.6</cell><cell>63.1</cell><cell>70.8</cell></row><row><cell>StarCoder2-15B</cell><cell>61.1</cell><cell>54.8</cell><cell>48.4</cell><cell>54.8</cell></row><row><cell cols="2">Granite-20B-Code-Base 79.4</cell><cell>82.2</cell><cell>66.8</cell><cell>76.1</cell></row><row><cell cols="2">Granite-34B-Code-Base 80.8</cell><cell>79.4</cell><cell>67.9</cell><cell>76.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="15,113.60,82.76,384.82,480.51"><head>Table 10 :</head><label>10</label><figDesc>Pass@1 performance on HumanEvalExplain.</figDesc><table coords="15,113.60,107.03,384.82,456.24"><row><cell>Model</cell><cell>Prompt</cell><cell cols="3">Python JavaScript Java</cell><cell>Go</cell><cell cols="3">C++ Rust Avg.</cell></row><row><cell></cell><cell></cell><cell>Base Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StarCoderBase-3B</cell><cell>Completion</cell><cell>11.0</cell><cell>10.4</cell><cell cols="3">14.6 11.0 13.4</cell><cell>11.0</cell><cell>11.9</cell></row><row><cell>StableCode-3B</cell><cell>Completion</cell><cell>11.0</cell><cell>7.9</cell><cell>22.0</cell><cell>4.3</cell><cell>14.6</cell><cell>14.0</cell><cell>12.3</cell></row><row><cell>StarCoder2-3B</cell><cell>Completion</cell><cell>12.2</cell><cell>13.4</cell><cell>19.5</cell><cell>6.7</cell><cell>14.0</cell><cell>12.8</cell><cell>13.1</cell></row><row><cell>CodeGemma-2B</cell><cell>Completion</cell><cell>20.7</cell><cell>15.9</cell><cell cols="3">20.7 12.8 17.7</cell><cell>15.9</cell><cell>17.3</cell></row><row><cell>Granite-3B-Code-Base</cell><cell>Completion</cell><cell>25.0</cell><cell>18.9</cell><cell cols="3">29.9 17.1 26.8</cell><cell>14.0</cell><cell>21.9</cell></row><row><cell>StarCoderBase-7B</cell><cell>Completion</cell><cell>14.0</cell><cell>17.1</cell><cell cols="3">17.7 10.4 17.1</cell><cell>12.8</cell><cell>14.8</cell></row><row><cell>CodeLlama-7B</cell><cell>Completion</cell><cell>11.0</cell><cell>14.0</cell><cell>16.5</cell><cell>9.8</cell><cell>17.7</cell><cell>14.6</cell><cell>13.9</cell></row><row><cell>StarCoder2-7B</cell><cell>Completion</cell><cell>4.9</cell><cell>12.8</cell><cell>22.0</cell><cell>4.9</cell><cell>22.0</cell><cell>14.6</cell><cell>13.5</cell></row><row><cell>CodeGemma-7B</cell><cell>Completion</cell><cell>13.1</cell><cell>13.8</cell><cell>2.0</cell><cell>8.0</cell><cell>18.6</cell><cell>18.9</cell><cell>12.4</cell></row><row><cell>Granite-8B-Code-Base</cell><cell>Completion</cell><cell>23.5</cell><cell>32.3</cell><cell cols="3">25.0 23.2 28.0</cell><cell>19.5</cell><cell>26.4</cell></row><row><cell>StarCoderBase-15B</cell><cell>Completion</cell><cell>9.8</cell><cell>15.2</cell><cell>24.4</cell><cell>9.1</cell><cell>20.1</cell><cell>13.4</cell><cell>15.3</cell></row><row><cell>CodeLlama-13B</cell><cell>Completion</cell><cell>13.4</cell><cell>14.0</cell><cell>23.2</cell><cell>9.8</cell><cell>15.9</cell><cell>13.4</cell><cell>15.0</cell></row><row><cell>StarCoder2-15B</cell><cell>Completion</cell><cell>20.1</cell><cell>19.5</cell><cell>7.3</cell><cell>9.8</cell><cell>23.8</cell><cell>21.3</cell><cell>17.0</cell></row><row><cell>Granite-20B-Code-Base</cell><cell>Completion</cell><cell>17.1</cell><cell>18.3</cell><cell cols="3">23.2 10.4 25.6</cell><cell>18.3</cell><cell>18.8</cell></row><row><cell>CodeLlama-34B</cell><cell>Completion</cell><cell>11.6</cell><cell>18.3</cell><cell>22.0</cell><cell>9.8</cell><cell>20.1</cell><cell>20.7</cell><cell>17.1</cell></row><row><cell>Granite-34B-Code-Base</cell><cell>Completion</cell><cell>42.7</cell><cell>26.2</cell><cell cols="3">47.0 26.8 36.6</cell><cell>25.0</cell><cell>34.1</cell></row><row><cell>CodeLlama-70B</cell><cell>Completion</cell><cell>24.4</cell><cell>30.5</cell><cell cols="3">43.9 19.5 31.1</cell><cell>20.1</cell><cell>28.2</cell></row><row><cell>Gemma-2B</cell><cell>Completion</cell><cell>9.8</cell><cell>9.8</cell><cell>14.6</cell><cell>7.9</cell><cell>14.0</cell><cell>9.1</cell><cell>10.9</cell></row><row><cell>Gemma-7B</cell><cell>Completion</cell><cell>10.4</cell><cell>18.3</cell><cell>19.5</cell><cell>9.8</cell><cell>18.3</cell><cell>14.0</cell><cell>15.0</cell></row><row><cell>Mistral-7B-v0.2</cell><cell>Completion</cell><cell>22.0</cell><cell>23.8</cell><cell cols="3">34.8 16.5 14.6</cell><cell>12.8</cell><cell>20.7</cell></row><row><cell>Mixtral-8x7B-v0.1</cell><cell>Completion</cell><cell>17.1</cell><cell>18.3</cell><cell cols="3">35.4 19.5 18.9</cell><cell>15.2</cell><cell>20.7</cell></row><row><cell>Mixtral-8x22B-v0.1</cell><cell>Completion</cell><cell>29.9</cell><cell>20.1</cell><cell cols="3">40.2 17.7 22.0</cell><cell>17.7</cell><cell>24.6</cell></row><row><cell>Llama-3-8B</cell><cell>Completion</cell><cell>15.2</cell><cell>14.0</cell><cell>18.9</cell><cell>5.5</cell><cell>18.3</cell><cell>8.5</cell><cell>13.4</cell></row><row><cell>Llama-3-70B</cell><cell>Completion</cell><cell>12.2</cell><cell>18.9</cell><cell>20.7</cell><cell>9.1</cell><cell>16.5</cell><cell>15.9</cell><cell>15.6</cell></row><row><cell></cell><cell></cell><cell cols="2">Instruct Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CodeGemma-7B-IT</cell><cell>Instruction</cell><cell>48.2</cell><cell>40.9</cell><cell cols="3">51.8 31.1 33.5</cell><cell>25.0</cell><cell>38.4</cell></row><row><cell>CodeLlama-7B-Instruct</cell><cell>Instruction</cell><cell>29.9</cell><cell>29.9</cell><cell cols="3">32.9 19.5 25.0</cell><cell>13.4</cell><cell>25.1</cell></row><row><cell>CodeLlama-13B-Instruct</cell><cell>Instruction</cell><cell>38.4</cell><cell>28.0</cell><cell cols="3">36.0 22.6 26.8</cell><cell>14.6</cell><cell>27.7</cell></row><row><cell>CodeLlama-34B-Instruct</cell><cell>Instruction</cell><cell>42.1</cell><cell>31.1</cell><cell cols="3">39.6 20.1 31.7</cell><cell>17.1</cell><cell>30.3</cell></row><row><cell>CodeLlama-70B-Instruct</cell><cell>Instruction</cell><cell>47.0</cell><cell>40.2</cell><cell cols="3">54.9 34.1 46.3</cell><cell>23.8</cell><cell>41.1</cell></row><row><cell>OctoCoder-15B</cell><cell>Instruction</cell><cell>37.8</cell><cell>26.8</cell><cell cols="3">31.1 18.3 22.6</cell><cell>14.0</cell><cell>25.1</cell></row><row><cell>Granite-3b-Code-Instruct</cell><cell>Instruction</cell><cell>39.6</cell><cell>26.8</cell><cell cols="3">39.0 14.0 23.8</cell><cell>12.8</cell><cell>26.0</cell></row><row><cell>Granite-8b-Code-Instruct</cell><cell>Instruction</cell><cell>53.0</cell><cell>42.7</cell><cell cols="3">52.4 36.6 43.9</cell><cell>16.5</cell><cell>40.9</cell></row><row><cell>Granite-20B-Code-Instruct</cell><cell>Instruction</cell><cell>44.5</cell><cell>42.7</cell><cell cols="3">49.4 32.3 42.1</cell><cell>18.3</cell><cell>38.2</cell></row><row><cell>Granite-34B-Code-Instruct</cell><cell>Instruction</cell><cell>53.0</cell><cell>45.1</cell><cell cols="3">50.6 36.0 42.7</cell><cell>23.8</cell><cell>41.9</cell></row><row><cell>Gemma-2B-IT</cell><cell>Instruction</cell><cell>9.8</cell><cell>12.8</cell><cell>13.4</cell><cell>8.5</cell><cell>13.4</cell><cell>3.0</cell><cell>10.2</cell></row><row><cell>Gemma-7B-IT</cell><cell>Instruction</cell><cell>31.1</cell><cell>23.2</cell><cell cols="3">28.0 14.6 23.8</cell><cell>8.5</cell><cell>21.5</cell></row><row><cell>Mistral-7B-Instruct-v0.2</cell><cell>Instruction</cell><cell>24.4</cell><cell>26.8</cell><cell cols="3">31.1 22.6 25.6</cell><cell>14.0</cell><cell>24.1</cell></row><row><cell>Mixtral-8x7B-Instruct-v0.1</cell><cell>Instruction</cell><cell>47.0</cell><cell>40.9</cell><cell cols="3">48.2 28.0 32.9</cell><cell>25.0</cell><cell>37.0</cell></row><row><cell>Mixtral-8x22B-Instruct-v0.1</cell><cell>Instruction</cell><cell>67.1</cell><cell>56.7</cell><cell cols="3">67.7 44.5 64.0</cell><cell>39.6</cell><cell>56.6</cell></row><row><cell>Llama-3-8B-Instruct</cell><cell>Instruction</cell><cell>36.0</cell><cell>31.7</cell><cell cols="3">40.2 19.5 31.1</cell><cell>15.9</cell><cell>29.1</cell></row><row><cell>Llama-3-70B-Instruct</cell><cell>Instruction</cell><cell>50.6</cell><cell>47.6</cell><cell cols="3">57.9 34.8 48.2</cell><cell>33.5</cell><cell>45.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="16,113.60,84.85,384.82,480.51"><head>Table 11 :</head><label>11</label><figDesc>Pass@1 performance on HumanEvalFix.</figDesc><table coords="16,113.60,109.13,384.82,456.24"><row><cell>Model</cell><cell>Prompt</cell><cell cols="3">Python JavaScript Java</cell><cell>Go</cell><cell cols="3">C++ Rust Avg.</cell></row><row><cell></cell><cell></cell><cell>Base Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StarCoderBase-3B</cell><cell>Completion</cell><cell>12.2</cell><cell>9.8</cell><cell>6.1</cell><cell>7.9</cell><cell>1.8</cell><cell>0.6</cell><cell>6.4</cell></row><row><cell>StableCode-3B</cell><cell>Completion</cell><cell>11.0</cell><cell>7.3</cell><cell cols="3">20.1 10.4 12.8</cell><cell>1.2</cell><cell>10.5</cell></row><row><cell>StarCoder2-3B</cell><cell>Completion</cell><cell>18.3</cell><cell>15.9</cell><cell cols="2">12.8 12.8</cell><cell>5.5</cell><cell>0.6</cell><cell>11.0</cell></row><row><cell>CodeGemma-2B</cell><cell>Completion</cell><cell>4.3</cell><cell>7.3</cell><cell>3.0</cell><cell>9.8</cell><cell>1.8</cell><cell>0.0</cell><cell>4.4</cell></row><row><cell>Granite-3B-Code-Base</cell><cell>Completion</cell><cell>18.3</cell><cell>23.2</cell><cell cols="3">29.9 24.4 16.5</cell><cell>3.7</cell><cell>19.3</cell></row><row><cell>StarCoderBase-7B</cell><cell>Completion</cell><cell>15.9</cell><cell>21.3</cell><cell cols="2">17.1 14.6</cell><cell>5.5</cell><cell>0.6</cell><cell>12.5</cell></row><row><cell>CodeLlama-7B</cell><cell>Completion</cell><cell>15.9</cell><cell>14.0</cell><cell cols="2">23.8 15.2</cell><cell>5.5</cell><cell>11.0</cell><cell>14.2</cell></row><row><cell>StarCoder2-7B</cell><cell>Completion</cell><cell>5.5</cell><cell>13.4</cell><cell cols="2">15.9 11.0</cell><cell>7.3</cell><cell>0.6</cell><cell>8.9</cell></row><row><cell>CodeGemma-7B</cell><cell>Completion</cell><cell>8.5</cell><cell>5.5</cell><cell cols="2">20.1 14.6</cell><cell>7.9</cell><cell>3.7</cell><cell>10.1</cell></row><row><cell>Granite-8B-Code-Base</cell><cell>Completion</cell><cell>22.6</cell><cell>35.4</cell><cell cols="3">38.4 37.2 28.7</cell><cell>15.2</cell><cell>29.6</cell></row><row><cell>StarCoderBase-15B</cell><cell>Completion</cell><cell>10.4</cell><cell>17.7</cell><cell cols="2">17.1 18.9</cell><cell>9.8</cell><cell>3.7</cell><cell>12.9</cell></row><row><cell>CodeLlama-13B</cell><cell>Completion</cell><cell>6.1</cell><cell>9.1</cell><cell>17.1</cell><cell>9.8</cell><cell>6.1</cell><cell>10.4</cell><cell>9.5</cell></row><row><cell>StarCoder2-15B</cell><cell>Completion</cell><cell>9.1</cell><cell>18.9</cell><cell cols="3">25.0 37.2 25.0</cell><cell>16.5</cell><cell>21.9</cell></row><row><cell>Granite-20B-Code-Base</cell><cell>Completion</cell><cell>23.2</cell><cell>23.8</cell><cell cols="3">14.6 26.2 15.2</cell><cell>3.0</cell><cell>17.7</cell></row><row><cell>CodeLlama-34B</cell><cell>Completion</cell><cell>14.0</cell><cell>20.7</cell><cell cols="3">20.1 26.2 31.1</cell><cell>6.1</cell><cell>19.7</cell></row><row><cell>Granite-34B-Code-Base</cell><cell>Completion</cell><cell>20.1</cell><cell>30.5</cell><cell cols="3">40.9 34.1 39.0</cell><cell>12.2</cell><cell>29.5</cell></row><row><cell>CodeLlama-70B</cell><cell>Completion</cell><cell>12.8</cell><cell>31.1</cell><cell cols="3">41.5 42.1 38.4</cell><cell>31.1</cell><cell>32.8</cell></row><row><cell>Gemma-2B</cell><cell>Completion</cell><cell>1.2</cell><cell>2.4</cell><cell>4.3</cell><cell>2.4</cell><cell>1.2</cell><cell>0.0</cell><cell>1.9</cell></row><row><cell>Gemma-7B</cell><cell>Completion</cell><cell>1.8</cell><cell>1.2</cell><cell>26.2</cell><cell>7.3</cell><cell>6.7</cell><cell>1.2</cell><cell>7.4</cell></row><row><cell>Mistral-7B-v0.2</cell><cell>Completion</cell><cell>3.0</cell><cell>2.4</cell><cell>6.1</cell><cell>9.1</cell><cell>5.5</cell><cell>0.6</cell><cell>4.5</cell></row><row><cell>Mixtral-8x7B-v0.1</cell><cell>Completion</cell><cell>11.0</cell><cell>12.8</cell><cell cols="3">18.3 25.0 15.9</cell><cell>3.0</cell><cell>14.3</cell></row><row><cell>Mixtral-8x22B-v0.1</cell><cell>Completion</cell><cell>17.1</cell><cell>30.5</cell><cell cols="3">23.8 30.5 32.3</cell><cell>11.0</cell><cell>24.2</cell></row><row><cell>Llama-3-8B</cell><cell>Completion</cell><cell>2.6</cell><cell>3.7</cell><cell>5.5</cell><cell>3.7</cell><cell>1.3</cell><cell>0.9</cell><cell>2.9</cell></row><row><cell>Llama-3-70B</cell><cell>Completion</cell><cell>31.7</cell><cell>33.5</cell><cell cols="3">39.0 28.0 28.7</cell><cell>12.8</cell><cell>28.9</cell></row><row><cell></cell><cell></cell><cell cols="2">Instruct Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CodeGemma-7B-IT</cell><cell>Instruction</cell><cell>46.3</cell><cell>45.7</cell><cell cols="3">52.4 48.2 43.9</cell><cell>38.4</cell><cell>45.8</cell></row><row><cell>CodeLlama-7B-Instruct</cell><cell>Instruction</cell><cell>19.5</cell><cell>18.9</cell><cell cols="2">13.4 14.6</cell><cell>6.1</cell><cell>4.3</cell><cell>12.8</cell></row><row><cell>CodeLlama-13B-Instruct</cell><cell>Instruction</cell><cell>18.9</cell><cell>18.9</cell><cell cols="2">24.4 22.6</cell><cell>9.8</cell><cell>0.0</cell><cell>15.8</cell></row><row><cell>CodeLlama-34B-Instruct</cell><cell>Instruction</cell><cell>37.8</cell><cell>28.0</cell><cell cols="3">37.2 24.4 24.4</cell><cell>17.7</cell><cell>28.2</cell></row><row><cell>CodeLlama-70B-Instruct</cell><cell>Instruction</cell><cell>64.6</cell><cell>52.4</cell><cell cols="3">57.3 51.8 51.2</cell><cell>40.9</cell><cell>53.0</cell></row><row><cell>OctoCoder-15B</cell><cell>Instruction</cell><cell>28.0</cell><cell>28.7</cell><cell cols="3">34.1 26.8 25.0</cell><cell>15.9</cell><cell>26.4</cell></row><row><cell>Granite-3b-Code-Instruct</cell><cell>Instruction</cell><cell>26.8</cell><cell>28.0</cell><cell cols="3">33.5 27.4 31.7</cell><cell>16.5</cell><cell>27.3</cell></row><row><cell>Granite-8b-Code-Instruct</cell><cell>Instruction</cell><cell>39.6</cell><cell>40.9</cell><cell cols="3">48.2 41.5 39.0</cell><cell>32.9</cell><cell>40.4</cell></row><row><cell>Granite-20B-Code-Instruct</cell><cell>Instruction</cell><cell>43.9</cell><cell>43.9</cell><cell cols="3">45.7 41.5 41.5</cell><cell>29.9</cell><cell>41.1</cell></row><row><cell>Granite-34B-Code-Instruct</cell><cell>Instruction</cell><cell>54.9</cell><cell>47.6</cell><cell cols="3">55.5 51.2 47.0</cell><cell>45.1</cell><cell>50.2</cell></row><row><cell>Gemma-2B-IT</cell><cell>Instruction</cell><cell>18.9</cell><cell>15.9</cell><cell cols="3">25.6 13.4 15.9</cell><cell>10.4</cell><cell>16.7</cell></row><row><cell>Gemma-7B-IT</cell><cell>Instruction</cell><cell>10.3</cell><cell>9.8</cell><cell cols="3">22.0 14.3 16.2</cell><cell>9.8</cell><cell>13.7</cell></row><row><cell>Mistral-7B-Instruct-v0.2</cell><cell>Instruction</cell><cell>33.5</cell><cell>22.6</cell><cell cols="3">27.4 27.4 26.2</cell><cell>23.8</cell><cell>26.8</cell></row><row><cell>Mixtral-8x7B-Instruct-v0.1</cell><cell>Instruction</cell><cell>44.5</cell><cell>37.8</cell><cell cols="3">47.6 38.4 39.0</cell><cell>28.7</cell><cell>39.3</cell></row><row><cell>Mixtral-8x22B-Instruct-v0.1</cell><cell>Instruction</cell><cell>59.1</cell><cell>53.7</cell><cell cols="3">66.5 55.5 56.1</cell><cell>45.7</cell><cell>56.1</cell></row><row><cell>Llama-3-8B-Instruct</cell><cell>Instruction</cell><cell>40.2</cell><cell>25.6</cell><cell cols="3">43.3 29.9 13.4</cell><cell>23.2</cell><cell>29.3</cell></row><row><cell>Llama-3-70B-Instruct</cell><cell>Instruction</cell><cell>57.3</cell><cell>51.2</cell><cell cols="3">54.3 51.8 50.6</cell><cell>49.4</cell><cell>52.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="17,166.62,243.95,278.77,112.22"><head>44.8 52.8 33.4 34.3 41.8 48.4 26.1 53.4</head><label></label><figDesc></figDesc><table coords="17,166.62,243.95,278.77,112.22"><row><cell>Source Language</cell><cell></cell><cell>Java</cell><cell></cell><cell></cell><cell></cell><cell>Python</cell></row><row><cell>Target Language</cell><cell cols="2">C C++</cell><cell>Go</cell><cell>Py</cell><cell cols="2">C C++</cell><cell>Go Java</cell></row><row><cell>Granite-3B-Code-IT</cell><cell cols="6">34.2 63.2 20.1 44.1 18.8 45.6</cell><cell>1.2 15.0</cell></row><row><cell>CodeGemma-7B-IT</cell><cell cols="6">33.7 31.3 10.3 31.3 22.9 33.4</cell><cell>4.6 44.5</cell></row><row><cell>CodeLlama-7B-Instruct</cell><cell cols="6">5.3 17.8 15.8 11.6 2.55 18.0 14.7 28.2</cell></row><row><cell cols="7">Granite-8B-Code-Instruct 49.6 57.3 17.1 57.9 41.3 55.2 10.5 39.7</cell></row><row><cell>CodeLlama-13B-Instruct</cell><cell cols="2">0.0 1.25</cell><cell cols="2">7.6 34.2</cell><cell>0.0</cell><cell>0.9</cell><cell>9.6 13.1</cell></row><row><cell>OctoCoder-15B</cell><cell>0.3</cell><cell cols="3">3.2 18.8 22.9</cell><cell cols="2">0.0 2.65 16.8 32.2</cell></row><row><cell>Granite-20B-Code-Instruct CodeLlama-34B-Instruct</cell><cell>0.0</cell><cell>1.8</cell><cell cols="2">9.8 20.6</cell><cell>0.5</cell><cell>1.0 14.1 10.0</cell></row><row><cell>Granite-34B-Code-Instruct</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="17,266.99,348.76,178.40,7.29"><head>55.0 65.9 28.5 56.3 45.7 53.7 34.1 61.3</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="18,132.85,615.25,372.39,117.43"><head></head><label></label><figDesc><ref type="bibr" coords="18,171.88,665.22,98.63,9.58" target="#b10">Azerbayev et al., 2023)</ref>: a dataset consisting of the 32 math questions with no figures from the May 2023 College Board SAT examination; we use the same experiment setting from<ref type="bibr" coords="18,252.50,687.14,100.29,9.58" target="#b10">Azerbayev et al. (2023)</ref> • OCW<ref type="bibr" coords="18,171.12,701.17,103.97,9.58" target="#b41">(Lewkowycz et al., 2022)</ref>: a collection of undergraduate-level STEM problems harvested from MIT's OpenCourseWare; we use the 4-shot experiment setting from<ref type="bibr" coords="18,143.48,723.09,98.63,9.58" target="#b10">Azerbayev et al. (2023)</ref>.</figDesc><table coords="18,132.85,615.25,372.39,59.56"><row><cell>: a dataset from high-school math competitions; we</cell></row><row><cell>use the 4-shot experiment setting from Gao et al. (2023);</cell></row><row><cell>• GSM8K (Cobbe et al., 2021): a dataset of middle-school level math word problems;</cell></row><row><cell>we use the 5-shot experiment setting from Gao et al. (2023);</cell></row><row><cell>• SAT (</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="19,117.95,82.76,376.11,175.14"><head>Table 15 :</head><label>15</label><figDesc>Performance on 4 chain-of-thought math tasks and 2 tool-aided math tasks. We noticed that Llama-3-8B-Base tends to generate invalid programs given the same prompts as the other model, resulting in very low scores on MATH+Py and GSM8K+Py tasks. Similar issues have been observed in our Python code generation experiment.</figDesc><table coords="19,117.95,107.32,365.97,108.22"><row><cell>Model</cell><cell cols="5">MATH GSM8K SAT OCW MATH+Py</cell><cell>GSM8K+Py</cell></row><row><cell>StarCoderBase-7B</cell><cell>2.4</cell><cell>3.8</cell><cell>18.7</cell><cell>2.2</cell><cell>18.2</cell><cell>15.6</cell></row><row><cell>CodeLlama-7B</cell><cell>4.1</cell><cell>11.9</cell><cell>12.5</cell><cell>2.9</cell><cell>20.8</cell><cell>26.8</cell></row><row><cell>StarCoder2-7B</cell><cell>10.4</cell><cell>27.2</cell><cell>37.5</cell><cell>4.8</cell><cell>28.7</cell><cell>39.4</cell></row><row><cell>CodeGemma-7B</cell><cell>21.8</cell><cell>49.0</cell><cell>53.1</cell><cell>6.9</cell><cell>31.1</cell><cell>60.9</cell></row><row><cell>Granite-8B-Code-Base</cell><cell>21.4</cell><cell>61.9</cell><cell>62.5</cell><cell>8.8</cell><cell>35.4</cell><cell>63.1</cell></row><row><cell>Gemma-7B</cell><cell>24.1</cell><cell>53.3</cell><cell>75.0</cell><cell>7.3</cell><cell>27.4</cell><cell>52.9</cell></row><row><cell>Mistral-7B-v0.2 Llama-3-8B</cell><cell>12.8 15.6</cell><cell>37.2 49.8</cell><cell>53.1 34.4</cell><cell>5.8 9.9</cell><cell>25.7 0.0 ⋆</cell><cell>45.6 2.4 ⋆</cell></row><row><cell>Llemma-7B</cell><cell>17.3</cell><cell>33.7</cell><cell>59.4</cell><cell>7.0</cell><cell>25.6</cell><cell>40.8</cell></row></table><note coords="19,118.04,227.23,3.63,6.37"><p>⋆</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,124.14,724.38,163.26,7.47"><p>https://www.ibm.com/impact/ai-ethics</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,124.14,702.27,257.31,7.47"><p>https://huggingface.co/datasets/codeparrot/github-code-clean</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,124.14,713.27,227.21,7.47"><p>https://huggingface.co/datasets/bigcode/starcoderdata</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,124.14,724.38,159.37,7.47"><p>https://huggingface.co/bigcode/starpii</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,124.14,724.38,105.19,7.47"><p>https://www.clamav.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="7,123.69,713.80,380.31,8.63;7,108.00,723.76,196.19,8.63"><p>We selected 92 programming languages that are common across the original CommitPackFT and our list of 116 languages used during pretraining.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_6" coords="9,124.14,713.27,330.28,7.47"><p>https://storage.googleapis.com/deepmind-media/gemma/codegemma report.pdf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_7" coords="9,124.14,724.38,161.51,7.47"><p>https://github.com/meta-llama/llama3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_8" coords="17,124.14,724.38,193.97,7.47"><p>https://codetlingua.github.io/leaderboard.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to acknowledge the efforts of numerous teams at <rs type="institution">IBM Research AI and Hybrid Cloud Platform, IBM AI Infrastructure team, IBM WatsonX Code Assistant</rs> and platform team. Special thanks to <rs type="institution">IBM Research leaders -Dario Gil</rs>, <rs type="person">Sriram Raghavan</rs>, <rs type="person">Mukesh Khare</rs>, <rs type="person">Danny Barnett</rs>, <rs type="person">Talia Gershon</rs>, <rs type="person">Priya Nagpurkar</rs>, <rs type="person">Nicholas Fuller</rs> for their support.</p><p>Thanks and acknowledgement to <rs type="person">Trent Gray-Donald</rs>, <rs type="person">Keri Olson</rs>, <rs type="person">Alvin Tan</rs>, <rs type="person">Hillery Hunter</rs>, <rs type="person">Dakshi Agrawal</rs>, <rs type="person">Xuan Liu</rs>, <rs type="person">Mudhakar Srivatsa</rs>, <rs type="person">Raghu Kiran Ganti</rs>, <rs type="person">Carlos Costa</rs>, <rs type="person">Darrell Reimer</rs>, <rs type="person">Maja Vukovic</rs>, <rs type="person">Dinesh Garg</rs>, <rs type="person">Akash Srivastava</rs>, <rs type="person">Abhishek Bhandwaldar</rs>, <rs type="person">Aldo Pareja</rs>, <rs type="person">Shiv Sudalairaj</rs>, <rs type="person">Atin Sood</rs>, <rs type="person">Sandeep Gopisetty</rs>, <rs type="person">Nick Hill</rs>, <rs type="person">Ray Rose</rs>, <rs type="person">Tulio Coppola</rs>, <rs type="person">Állysson Oliveira</rs>, <rs type="person">Aadarsh Sahoo</rs>, <rs type="person">Apoorve Mohan</rs>, <rs type="person">Yuan Chi Chang</rs>, <rs type="person">Jitendra Singh</rs>, <rs type="person">Yuya Ong</rs>, <rs type="person">Eric Butler</rs>, <rs type="person">David Brotherton</rs>, <rs type="person">Rakesh Mohan</rs>, <rs type="person">David Kung</rs>, <rs type="person">Dinesh Khandelwal</rs>, <rs type="person">Naigang Wang</rs>, <rs type="person">Nelson Mimura Gonzalez</rs>, <rs type="person">Olivier Tardieu</rs>, <rs type="person">Tuan Hoang Trong</rs>, <rs type="person">Luis Angel Bathen</rs>, <rs type="person">Kevin O'Connor</rs>, <rs type="person">Christopher Laibinis</rs>, <rs type="person">Tatsuhiro Chiba</rs>, <rs type="person">Sunyanan Choochotkaew</rs>, <rs type="person">Robert Walkup</rs>, <rs type="person">Antoni Viros i Martin</rs>, <rs type="person">Adnan Hoque</rs>, <rs type="person">Davis Wertheimer</rs> and <rs type="person">Marquita Ellis</rs>.</p></div>
			</div>
			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code editing CanItEdit Cassano et al. (2024) Code translation CodeLingua Pan et al. (2024) Code execution CruxEval Gu et al. (2024)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="12,341.89,189.27,55.80,7.36" xml:id="b0">
	<monogr>
		<title level="m" type="main" coords="12,341.89,189.27,55.80,7.36">CodeGemma-2B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,332.30,243.58,167.11,7.36" xml:id="b1">
	<monogr>
		<title level="m" type="main" coords="12,332.30,243.58,92.75,7.36">Granite-8B-Code-Base 31</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,337.50,255.83,64.57,7.36" xml:id="b2">
	<monogr>
		<title level="m" type="main" coords="12,337.50,255.83,64.57,7.36">StarCoderBase-15B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,343.29,272.65,52.99,7.36" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Starcoder</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,342.74,293.31,54.09,7.36" xml:id="b4">
	<monogr>
		<title level="m" type="main" coords="12,342.74,293.31,54.09,7.36">CodeLlama-34B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.39,301.72,169.02,7.36;22,108.00,83.79,59.76,11.21;22,108.00,103.28,396.00,9.58;22,117.96,114.11,386.03,9.71;22,117.96,125.07,100.45,9.71" xml:id="b5">
	<monogr>
		<idno type="arXiv">arXiv:2108.11590</idno>
		<title level="m" coords="22,155.12,114.24,279.47,9.58">Avatar: A parallel corpus for java-python program translation</title>
		<editor>
			<persName><forename type="first">References</forename><surname>Wasi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Uddin</forename><surname>Ahmad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Md</forename><surname>Golam Rahman Tushar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Saikat</forename><surname>Chakraborty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,108.00,144.89,397.05,9.58;22,117.96,156.53,120.60,8.30" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><surname>Ai@meta</surname></persName>
		</author>
		<ptr target="https://github.com/meta-llama/llama3/blob/main/MODELCARD.md" />
		<title level="m" coords="22,154.50,144.89,84.58,9.58">Llama 3 model card</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,108.00,175.51,397.25,9.61;22,117.96,186.49,386.04,9.58;22,117.96,197.45,131.25,9.58" xml:id="b7">
	<monogr>
		<title level="m" type="main" coords="22,211.51,186.49,292.49,9.58;22,117.96,197.45,101.75,9.58">Gqa: Training generalized multi-query transformer models from multi-head checkpoints</title>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michiel</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yury</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Lebr Ón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,108.00,217.14,397.65,9.58;22,117.96,228.10,387.78,9.58;22,117.96,238.93,327.30,9.71" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Munoz Ferrandis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.03988</idno>
		<title level="m" coords="22,117.96,238.93,224.36,9.71">Santacoder: don&apos;t reach for the stars! arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,108.00,258.74,396.00,9.58;22,117.96,269.70,386.04,9.58;22,117.96,280.66,193.02,9.58" xml:id="b9">
	<monogr>
		<title level="m" type="main" coords="22,464.76,269.70,39.25,9.58;22,117.96,280.66,163.16,9.58">Program synthesis with large language models</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,108.00,300.35,397.24,9.58;22,117.57,311.31,386.42,9.58;22,117.96,322.14,269.63,9.71" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Zhangir</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><forename type="middle">Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Mcaleer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.10631</idno>
		<title level="m" coords="22,382.76,311.31,121.23,9.58;22,117.96,322.27,99.44,9.58">Llemma: An open language model for mathematics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,108.00,341.95,371.13,9.58;22,108.00,361.64,396.00,9.58;22,117.96,372.60,386.42,9.58;22,117.96,383.56,387.78,9.58;22,117.96,394.39,161.71,9.71" xml:id="b11">
	<analytic>
		<title level="a" type="main" coords="22,361.83,341.95,87.78,9.58">Layer normalization</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ibrahim</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Subhajit</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soham</forename><surname>Chaudhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxwell</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asim</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadhana</forename><surname>Munawar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vinod</forename><surname>Kumaravel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pavan</forename><surname>Muthusamy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><forename type="middle">A</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lastras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.15491</idno>
	</analytic>
	<monogr>
		<title level="m" coords="22,156.36,383.56,345.03,9.58">Api-blend: A comprehensive corpora for training and benchmarking api llms</title>
		<imprint>
			<date type="published" when="2016">2016. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,108.00,414.20,397.25,9.58;22,117.79,425.16,387.45,9.58;22,117.96,436.12,22.42,9.58" xml:id="b12">
	<monogr>
		<title level="m" type="main" coords="22,254.23,425.16,246.17,9.58">Efficient training of language models to fill in the middle</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Mcleavey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,108.00,455.81,397.25,9.58;22,117.96,466.77,387.28,9.58;22,117.96,477.73,387.78,9.58;22,117.96,488.55,205.07,9.71" xml:id="b13">
	<analytic>
		<title level="a" type="main" coords="22,140.84,477.73,360.48,9.58">Multipl-e: a scalable and polyglot approach to benchmarking neural code generation</title>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Gouwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sydney</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luna</forename><surname>Phipps-Costin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Ho</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carolyn</forename><forename type="middle">Jane</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Molly</forename><forename type="middle">Q</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="22,117.96,488.55,175.80,9.50">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,108.00,508.37,397.25,9.58;22,117.96,519.33,386.04,9.58;22,117.57,530.29,386.42,9.58;22,117.96,541.25,113.19,9.58" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luisa</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akul</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Shinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abby</forename><surname>Brennan-Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Ginesin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Chakhnashvili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carolyn</forename><forename type="middle">Jane</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arjun</forename><surname>Guha</surname></persName>
		</author>
		<title level="m" coords="22,176.66,530.29,327.34,9.58;22,117.96,541.25,84.09,9.58">Can it edit? evaluating the ability of large language models to follow code editing instructions</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,108.00,560.93,397.25,9.58;22,117.79,571.89,387.45,9.58;22,117.96,582.85,387.28,9.58;22,117.96,593.81,387.69,9.58;22,117.96,604.77,387.28,9.58;22,117.96,615.73,386.03,9.58;22,117.96,626.69,387.28,9.58;22,117.96,637.65,386.04,9.58;22,117.57,648.61,387.67,9.58;22,117.96,659.56,387.28,9.58;22,117.96,670.52,387.28,9.58;22,117.96,681.48,22.42,9.58" xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code</note>
</biblStruct>

<biblStruct coords="22,108.00,701.17,397.24,9.58;22,117.96,712.13,386.03,9.58;22,117.96,722.96,297.20,9.71" xml:id="b16">
	<monogr>
		<title level="m" type="main" coords="22,428.69,712.13,75.30,9.58;22,117.96,723.09,127.03,9.58">Training verifiers to solve math word problems</title>
		<author>
			<persName coords=""><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="23,108.00,85.05,397.25,9.58;23,117.96,96.01,386.04,9.58;23,117.96,106.97,387.28,9.58;23,117.96,117.93,386.04,9.58;23,117.96,128.89,387.49,9.58;23,117.44,140.53,102.88,8.30" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Codegemma</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ale Jakse</forename><surname>Hartman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heri</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jane</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joe</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mateo</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pratik</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ravin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarmad</forename><surname>Hashmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shubham</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siqi</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tris</forename><surname>Warkentin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhitao</forename></persName>
		</author>
		<ptr target="https://goo.gle/codegemma" />
		<title level="m" coords="23,179.29,128.89,230.83,9.58">Codegemma: Open code models based on gemma</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,108.00,159.33,321.49,9.58" xml:id="b18">
	<analytic>
		<title/>
		<ptr target="https://docs.cohere.com/docs/command-r-plus" />
	</analytic>
	<monogr>
		<title level="j" coords="23,108.00,159.33,95.44,9.58">Cohere. Command r+</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="23,108.00,178.81,397.25,9.58;23,117.96,189.76,22.42,9.58" xml:id="b19">
	<monogr>
		<title level="m" type="main" coords="23,147.62,178.81,353.30,9.58">Flashattention-2: Faster attention with better parallelism and work partitioning</title>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,108.00,209.25,332.63,9.58;23,455.87,209.25,49.78,9.58;23,117.96,220.20,387.28,9.58;23,117.96,231.03,386.04,9.71;23,117.96,241.99,387.69,9.71;23,117.96,253.08,387.08,9.58;23,117.96,264.73,271.70,8.30" xml:id="b20">
	<analytic>
		<title level="a" type="main" coords="23,455.87,209.25,49.78,9.58;23,117.96,220.20,310.49,9.58">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paperfiles/paper/2022/file/67" />
	</analytic>
	<monogr>
		<title level="m" coords="23,450.21,231.03,53.79,9.50;23,117.96,241.99,170.45,9.50">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16344" to="16359" />
		</imprint>
	</monogr>
	<note>d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf</note>
</biblStruct>

<biblStruct coords="23,108.00,283.52,397.46,9.58;23,117.44,295.17,323.48,8.30" xml:id="b21">
	<monogr>
		<title level="m" type="main" coords="23,163.04,283.52,307.35,9.58">Introducing dbrx: A new state-of-the-art open llm -databricks blog</title>
		<author>
			<persName coords=""><surname>Databricks</surname></persName>
		</author>
		<ptr target="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="23,108.00,313.96,397.25,9.58;23,117.96,324.92,386.04,9.58;23,117.63,335.88,388.02,9.58;23,117.66,346.71,386.34,9.71;23,117.65,357.67,351.65,9.71" xml:id="b22">
	<analytic>
		<title level="a" type="main" coords="23,151.12,335.88,354.54,9.58;23,117.66,346.84,29.43,9.58">Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion</title>
		<author>
			<persName coords=""><forename type="first">Yangruibo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uddin</forename><surname>Wasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hantian</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nihal</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishna</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Parminder</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xiang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=wgDcbBMSfh" />
	</analytic>
	<monogr>
		<title level="m" coords="23,166.55,346.71,337.46,9.50;23,117.65,357.67,71.58,9.50">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,108.00,377.28,397.65,9.58;23,117.96,388.24,387.28,9.58;23,117.57,399.06,386.42,9.71;23,117.96,410.02,171.86,9.71" xml:id="b23">
	<analytic>
		<title level="a" type="main" coords="23,439.84,388.24,65.41,9.58;23,117.57,399.19,298.44,9.58">Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion</title>
		<author>
			<persName coords=""><forename type="first">Yangruibo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hantian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nihal</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishna</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Parminder</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="23,424.30,399.06,79.69,9.50;23,117.96,410.02,127.55,9.50">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,108.00,429.63,396.00,9.58;23,117.96,440.59,387.28,9.58;23,117.96,451.55,387.28,9.58;23,117.57,462.51,387.67,9.58;23,117.96,473.47,386.34,9.58;23,117.96,485.12,173.46,8.30" xml:id="b24">
	<monogr>
		<title level="m" type="main" coords="23,192.41,473.47,242.75,9.58">A framework for few-shot language model evaluation</title>
		<author>
			<persName coords=""><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stella</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sid</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurence</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alain</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haonan</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niklas</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laria</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hailey</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aviya</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lintang</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anish</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zou</surname></persName>
		</author>
		<ptr target="https://zenodo.org/records/10256836" />
		<imprint>
			<biblScope unit="page" from="12" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,108.00,503.91,397.25,9.58;23,117.96,514.87,386.03,9.58;23,117.65,525.83,387.59,9.58;23,117.57,536.79,386.43,9.58;23,117.65,547.74,387.59,9.58;23,117.96,558.70,387.28,9.58;23,117.96,569.66,386.04,9.58;23,117.96,580.62,387.28,9.58;23,117.79,591.58,386.60,9.58;23,117.96,602.54,386.42,9.58;23,117.63,613.50,386.36,9.58;23,117.96,624.46,387.28,9.58;23,117.96,635.42,387.28,9.58;23,117.96,646.37,387.28,9.58;23,117.96,657.33,387.28,9.58;23,117.96,668.29,386.04,9.58;23,117.96,679.25,387.28,9.58;23,117.96,690.21,386.04,9.58;23,117.96,701.17,387.29,9.58;23,117.57,712.13,387.67,9.58;23,117.96,723.09,270.10,9.58" xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Gemma-Team</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cassidy</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shreya</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Morgane</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihir</forename><surname>Sanjay Kale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juliette</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pouya</forename><surname>Tafti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Léonard</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giuseppe</forename><surname>Pier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aakanksha</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ambrose</forename><surname>Castro-Ros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amélie</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Héliou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonia</forename><surname>Bulanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Beth</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bobak</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charline</forename><forename type="middle">Le</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clément</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Crepy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daphne</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elena</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geng</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George-Christian</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grigory</forename><surname>Muraru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henryk</forename><surname>Rozhdestvenskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Grishchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jane</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Baptiste</forename><surname>Labanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Mao-Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lars</forename><forename type="middle">Lowe</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><surname>Sjoesund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Machel</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maciej</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mateo</forename><surname>Mikuła</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolai</forename><surname>Sharman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nithum</forename><surname>Chinaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oscar</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oscar</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paige</forename><surname>Wahltinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petko</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rahma</forename><surname>Yotov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramona</forename><surname>Chaabouni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reena</forename><surname>Comanescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rohan</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruibo</forename><surname>Mcilroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mullins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sertan</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sholto</forename><surname>Girgin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shree</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siamak</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soham</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Klimenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vlad</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zafarali</forename><surname>Hui Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhitao</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tris</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ludovic</forename><surname>Warkentin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh</forename><surname>Peran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clément</forename><surname>Giang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koray</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Demis</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zoubin</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joelle</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Barral</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<pubPlace>Fernando Pereira, Eli Collins</pubPlace>
		</imprint>
	</monogr>
	<note>Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology</note>
</biblStruct>

<biblStruct coords="24,108.00,85.05,396.00,9.58;24,117.96,96.01,387.78,9.58;24,117.96,106.84,161.71,9.71" xml:id="b26">
	<monogr>
		<title level="m" type="main" coords="24,179.33,96.01,321.92,9.58">Cruxeval: A benchmark for code reasoning, understanding and execution</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugh</forename><surname>Leather</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armando</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.03065</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="24,108.00,125.14,336.75,9.58" xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note>Gaussian error linear units (gelus</note>
</biblStruct>

<biblStruct coords="24,108.00,143.32,397.25,9.58;24,117.96,154.28,386.04,9.58;24,117.96,165.11,126.31,9.71" xml:id="b28">
	<monogr>
		<title level="m" type="main" coords="24,275.53,154.28,228.47,9.58;24,117.96,165.24,55.09,9.58">Measuring mathematical problem solving with the math dataset</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,108.00,183.42,396.00,9.58;24,117.96,194.37,386.04,9.58;24,117.96,205.20,384.86,9.71" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuge</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chaoqun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yewei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.06395</idno>
		<title level="m" coords="24,359.39,194.37,144.61,9.58;24,117.96,205.33,215.54,9.58">Unveiling the potential of small language models with scalable training strategies</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="24,108.00,223.51,21.79,9.58;24,152.13,223.51,149.52,9.58;24,324.00,223.51,181.04,9.58;24,117.59,235.16,107.31,8.30" xml:id="b30">
	<monogr>
		<ptr target="https://www.ibm.com/products/watsonx-code-assistant" />
		<title level="m" coords="24,108.00,223.51,21.79,9.58;24,152.13,223.51,112.73,9.58">IBM. watsonx code assistant</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,108.00,252.65,396.00,9.58;24,117.96,263.61,387.28,9.58;24,117.96,274.56,386.04,9.58;24,117.96,285.52,163.39,9.58" xml:id="b31">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Neel</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ping</forename><surname>Yeh Chiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Kirchenbauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hong-Min</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gowthami</forename><surname>Somepalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><forename type="middle">R</forename><surname>Bartoldson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aniruddha</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<title level="m" coords="24,372.51,274.56,131.49,9.58;24,117.96,285.52,134.08,9.58">Neftune: Noisy embeddings improve instruction finetuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,108.00,303.70,396.00,9.58;24,117.96,314.66,386.03,9.58;24,117.96,325.49,282.60,9.71" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m" coords="24,183.20,325.62,42.76,9.58">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="24,108.00,343.79,396.00,9.58;24,117.96,354.75,386.03,9.58;24,117.96,365.71,386.04,9.58;24,117.96,376.67,359.38,9.58" xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gianna</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucile</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renard</forename><surname>Lélio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Anne</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teven</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thibaut</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothée</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">El</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sayed</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Mistral 7b, 2023b</note>
</biblStruct>

<biblStruct coords="24,108.00,394.85,397.24,9.58;24,117.96,405.81,386.24,9.58;24,117.63,416.77,386.36,9.58;24,117.96,427.72,386.42,9.58;24,117.96,438.68,217.43,9.58" xml:id="b34">
	<monogr>
		<title level="m" type="main" coords="24,496.48,427.72,7.91,9.58;24,117.96,438.68,188.35,9.58">A study of bfloat16 for deep learning training</title>
		<author>
			<persName coords=""><forename type="first">Dhiraj</forename><surname>Kalamkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naveen</forename><surname>Mellempudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kunal</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sasikanth</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teja</forename><surname>Dharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nataraj</forename><surname>Vooturi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianyu</forename><surname>Jammalamadaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hector</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiyan</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jongsoo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Heinecke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sudarshan</forename><surname>Georganas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhisek</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Misha</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bharat</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pradeep</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dubey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,108.00,456.86,397.25,9.58;24,117.96,467.82,387.28,9.58;24,117.96,478.78,386.04,9.58;24,117.96,489.74,386.03,9.58;24,117.96,500.70,73.47,9.58" xml:id="b35">
	<analytic>
		<title level="a" type="main" coords="24,229.53,489.74,274.46,9.58;24,117.96,500.70,44.15,9.58">Scaling large language models with simple yet effective depth up-scaling</title>
		<author>
			<persName coords=""><forename type="first">Dahyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chanjun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanghoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wonsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wonho</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyeonwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yungi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyeonju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jihoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changbae</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seonghoon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sukyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gyoungjin</forename><surname>Gim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mikyoung</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="24,184.28,489.74,22.04,9.58">Solar</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,108.00,518.87,381.57,9.58" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ba</surname></persName>
		</author>
		<title level="m" coords="24,264.90,518.87,195.21,9.58">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,108.00,537.05,396.00,9.58;24,117.57,548.01,388.08,9.58;24,117.96,558.97,201.84,9.58;24,334.91,558.84,169.08,9.50;24,117.68,569.80,387.36,9.71;24,117.96,581.57,282.32,8.30" xml:id="b37">
	<analytic>
		<title level="a" type="main" coords="24,394.65,548.01,111.01,9.58;24,117.96,558.97,196.79,9.58">Reducing activation recomputation in large transformer models</title>
		<author>
			<persName coords=""><forename type="first">Anand</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sangkug</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Lym</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Mcafee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Andersch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Catanzaro</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/" />
	</analytic>
	<monogr>
		<title level="m" coords="24,334.91,558.84,169.08,9.50;24,117.68,569.80,71.78,9.71">Proceedings of Machine Learning and Systems, 5, 2023</title>
		<meeting>Machine Learning and Systems, 5, 2023</meeting>
		<imprint/>
	</monogr>
	<note>paper files/paper/2023/hash/ e851ca7b43815718fbbac8afb2246bf8-Abstract-mlsys2023.html</note>
</biblStruct>

<biblStruct coords="24,108.00,599.04,396.00,9.61;24,117.96,610.02,386.03,9.58;24,117.96,620.98,387.69,9.58;24,117.96,631.94,387.69,9.58;24,117.96,642.90,186.79,9.58" xml:id="b38">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yannic</forename><surname>Andreas K Öpf</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sotiris</forename><surname>Dimitri Von R Ütte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhi-Rui</forename><surname>Anagnostidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keith</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abdullah</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Barhoum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oliver</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richárd</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nagyfi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">S</forename><surname>Shahul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Glushkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnav</forename><surname>Dantuluri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Mattick</surname></persName>
		</author>
		<title level="m" coords="24,318.56,631.94,187.09,9.58;24,117.96,642.90,157.16,9.58">Openassistant conversations -democratizing large language model alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,108.00,661.07,397.65,9.58;24,117.96,672.03,386.31,9.58;24,117.96,682.86,387.78,9.71;24,117.47,693.95,115.36,9.58" xml:id="b39">
	<analytic>
		<title level="a" type="main" coords="24,318.24,672.03,186.04,9.58;24,117.96,682.99,145.81,9.58">Ds-1000: A natural and reliable benchmark for data science code generation</title>
		<author>
			<persName coords=""><forename type="first">Yuhang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="24,288.08,682.86,194.72,9.50">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18319" to="18345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,108.00,712.13,396.00,9.58;24,117.96,723.09,108.79,9.58" xml:id="b40">
	<monogr>
		<title level="m" type="main" coords="24,330.62,712.13,173.38,9.58;24,117.96,723.09,78.53,9.58">Platypus: Quick, cheap, and powerful refinement of llms</title>
		<author>
			<persName coords=""><forename type="first">Ariel</forename><forename type="middle">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cole</forename><forename type="middle">J</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,108.00,85.05,397.25,9.58;25,117.60,96.01,388.14,9.58;25,117.96,106.84,386.04,9.71;25,117.96,117.80,219.18,9.71" xml:id="b41">
	<analytic>
		<title level="a" type="main" coords="25,117.96,106.97,289.21,9.58">Solving quantitative reasoning problems with language models</title>
		<author>
			<persName coords=""><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anders</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vinay</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ambrose</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cem</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theo</forename><surname>Gutman-Solo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="25,419.65,106.84,84.35,9.50;25,117.96,117.80,127.55,9.50">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3843" to="3857" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,108.00,136.08,397.66,9.58;25,117.96,147.04,386.04,9.58;25,117.96,158.00,387.28,9.58;25,117.79,168.96,387.45,9.58;25,117.57,179.92,388.08,9.58;25,117.96,190.87,387.28,9.58;25,117.96,201.83,386.04,9.58;25,117.96,212.79,387.69,9.58;25,117.96,223.75,386.04,9.58;25,117.96,234.71,386.23,9.58;25,117.96,245.67,387.28,9.58;25,117.96,256.60,387.28,9.61;25,117.65,267.59,386.34,9.58;25,117.96,278.55,118.76,9.58" xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><surname>Chim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mishig</forename><surname>Davaadorj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Lamy-Poirier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">João</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oleh</forename><surname>Shliazhko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Gontier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><surname>Meade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armel</forename><surname>Zebaze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Ho</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Logesh</forename><surname>Kumar Umapathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Lipkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Muhtasham</forename><surname>Oblokulov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rudra</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Stillerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sankalp</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitry</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Abulkhanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manan</forename><surname>Zocca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhihan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nour</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Urvashi</forename><surname>Fahmy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhao</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Swayam</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sasha</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxim</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fedor</forename><surname>Kunakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Zhdanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tony</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nadav</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Timor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claire</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hailey</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mayank</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carolyn</forename><forename type="middle">Jane</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brendan</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danish</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siva</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yacine</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jernite</surname></persName>
		</author>
		<imprint>
			<publisher>Sean Hughes, Thomas Wolf, Arjun Guha</publisher>
		</imprint>
	</monogr>
	<note>Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023a</note>
</biblStruct>

<biblStruct coords="25,108.00,296.70,397.66,9.58;25,117.96,307.66,386.03,9.58;25,117.96,318.49,257.79,9.71" xml:id="b43">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><surname>Chim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06161</idno>
		<title level="m" coords="25,421.01,307.66,82.99,9.58;25,117.96,318.49,149.34,9.71">Starcoder: may the source be with you! arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,108.00,336.77,396.00,9.58;25,117.96,347.73,387.69,9.58;25,117.96,358.56,386.34,9.71;25,117.96,370.33,218.15,8.30" xml:id="b44">
	<analytic>
		<title level="a" type="main" coords="25,405.37,336.77,98.63,9.58;25,117.96,347.73,387.69,9.58;25,117.96,358.69,21.13,9.58">Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation</title>
		<author>
			<persName coords=""><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunqiu</forename><surname>Steven Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lingming</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=1qvx610Cu7" />
	</analytic>
	<monogr>
		<title level="m" coords="25,159.02,358.56,315.92,9.71">Thirty-seventh Conference on Neural Information Processing Systems, 2023a</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="25,108.00,387.80,396.00,9.58;25,117.96,398.63,306.73,9.71" xml:id="b45">
	<monogr>
		<title level="m" type="main" coords="25,316.82,387.80,187.18,9.58;25,117.96,398.76,131.23,9.58">Repobench: Benchmarking repository-level code auto-completion systems</title>
		<author>
			<persName coords=""><forename type="first">Tianyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.03091</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="25,108.00,416.91,397.25,9.58;25,117.96,427.87,386.03,9.58;25,117.96,438.70,387.53,9.71;25,117.96,449.79,85.47,9.58" xml:id="b46">
	<analytic>
		<title level="a" type="main" coords="25,295.39,427.87,208.60,9.58;25,117.96,438.83,130.43,9.58">The flan collection: Designing data and methods for effective instruction tuning</title>
		<author>
			<persName coords=""><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="25,267.95,438.70,184.57,9.50">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22631" to="22648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,108.00,467.94,397.25,9.58;25,117.96,478.90,386.04,9.58;25,117.96,489.73,332.35,9.71" xml:id="b47">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Lamy-Poirier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmytro</forename><surname>Pykhtar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxiang</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.19173</idno>
		<title level="m" coords="25,453.58,478.90,50.42,9.58;25,117.96,489.86,162.54,9.58">Starcoder 2 and the stack v2: The next generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="25,108.00,508.01,396.00,9.58;25,117.96,518.97,386.03,9.58;25,117.47,529.80,388.27,9.71;25,117.96,540.89,233.35,9.58" xml:id="b48">
	<analytic>
		<title level="a" type="main" coords="25,138.75,529.93,105.56,9.58">Mixed precision training</title>
		<author>
			<persName coords=""><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1gs9JgRZ" />
	</analytic>
	<monogr>
		<title level="m" coords="25,263.51,529.80,213.52,9.50">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,108.00,559.04,299.82,9.58" xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Mistralai</surname></persName>
		</author>
		<ptr target="https://mistral.ai/news/mixtral-8x22b/" />
		<imprint>
			<date>8x22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,108.00,577.19,397.25,9.58;25,117.96,588.15,387.28,9.58;25,117.96,599.11,232.85,9.58" xml:id="b50">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armel</forename><surname>Zebaze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qinkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Swayam</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Octopack</surname></persName>
		</author>
		<title level="m" coords="25,117.96,599.11,202.98,9.58">Instruction tuning code large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,108.00,617.26,397.24,9.58;25,117.60,628.22,388.05,9.58;25,117.96,639.18,386.03,9.58;25,117.96,650.01,387.69,9.71;25,117.96,660.97,387.69,9.71;25,117.96,672.06,387.78,9.58;25,117.96,683.02,214.69,9.58" xml:id="b51">
	<analytic>
		<title level="a" type="main" coords="25,312.39,639.18,191.61,9.58;25,117.96,650.14,146.69,9.58;25,490.67,661.10,14.99,9.58;25,117.96,672.06,152.58,9.58">Efficient large-scale language model training on gpu clusters using megatron-lm</title>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458817.3476209</idno>
		<ptr target="https://doi.org/10.1145/3458817.3476209" />
	</analytic>
	<monogr>
		<title level="m" coords="25,282.80,650.01,222.86,9.50;25,117.96,660.97,252.38,9.71">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC &apos;21</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis, SC &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct coords="25,108.00,701.17,397.24,9.58;25,117.96,712.13,386.04,9.58;25,117.66,723.09,109.05,9.58" xml:id="b52">
	<monogr>
		<title level="m" type="main" coords="25,210.89,712.13,293.11,9.58;25,117.66,723.09,79.82,9.58">Codegen: An open large language model for code with multi-turn program synthesis</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,108.00,85.05,397.24,9.58;26,117.96,96.01,387.78,9.58;26,117.96,106.97,386.03,9.58;26,117.96,117.80,387.28,9.71;26,117.96,128.89,238.08,9.58" xml:id="b53">
	<analytic>
		<title level="a" type="main" coords="26,117.96,106.97,386.03,9.58;26,117.96,117.93,18.46,9.58">Lost in translation: A study of bugs introduced by large language models while translating code</title>
		<author>
			<persName coords=""><forename type="first">Rangeet</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reza</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rahul</forename><surname>Ibrahimzada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Divya</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lambert</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Pouguem Wassi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raju</forename><surname>Sobolev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Pavuluri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reyhaneh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jabbarvand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="26,156.24,117.80,349.00,9.50;26,117.96,128.89,35.65,9.58">Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE &apos;24</title>
		<meeting>the IEEE/ACM 46th International Conference on Software Engineering, ICSE &apos;24</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,108.00,148.04,396.00,9.58;26,117.96,158.87,387.73,9.71" xml:id="b54">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><forename type="middle">Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhangir</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06786</idno>
		<title level="m" coords="26,417.92,148.04,86.08,9.58;26,117.96,159.00,221.49,9.58">Openwebmath: An open dataset of high-quality mathematical web text</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="26,108.00,178.16,396.00,9.58;26,117.96,189.12,387.78,9.58;26,117.96,199.95,289.00,9.71" xml:id="b55">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Nikhil</forename><surname>Pinnaparaju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reshinth</forename><surname>Adithyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duy</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Baicoianu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maksym</forename><surname>Zhuravinskyi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dakota</forename><surname>Mahan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Bellagente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.01226</idno>
		<title level="m" coords="26,117.96,200.08,119.47,9.58">Stable code technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="26,108.00,219.23,396.20,9.58;26,117.96,230.19,386.03,9.58;26,117.96,241.15,387.78,9.58;26,117.96,251.98,387.28,9.71;26,117.96,263.07,22.42,9.58" xml:id="b56">
	<monogr>
		<title level="m" type="main" coords="26,117.96,252.11,342.62,9.58">Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks</title>
		<author>
			<persName coords=""><forename type="first">Ruchir</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">S</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geert</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giacomo</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Zolotov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Dolby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihir</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lindsey</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veronika</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Buratti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Pujar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shyam</forename><surname>Ramji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulrich</forename><surname>Finkler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Malaika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frederick</forename><surname>Reiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,108.00,282.23,397.54,9.58" xml:id="b57">
	<monogr>
		<title level="m" type="main" coords="26,333.40,282.23,143.41,9.58">Searching for activation functions</title>
		<author>
			<persName coords=""><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,108.00,301.38,397.24,9.58;26,117.63,312.34,386.36,9.58;26,117.96,323.17,238.40,9.71" xml:id="b58">
	<monogr>
		<title level="m" type="main" coords="26,367.43,312.34,136.57,9.58;26,117.96,323.30,68.46,9.58">Code llama: Open foundation models for code</title>
		<author>
			<persName coords=""><forename type="first">Jonas</forename><surname>Baptiste Roziere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sten</forename><surname>Gloeckle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Itai</forename><surname>Sootla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yossi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingyu</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tal</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jérémy</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rapin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12950</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="26,108.00,342.46,397.25,9.58;26,117.63,353.42,387.61,9.58;26,117.79,364.37,387.46,9.58;26,117.57,375.33,386.42,9.58;26,117.96,386.29,386.03,9.58;26,117.96,397.25,63.30,9.58" xml:id="b59">
	<monogr>
		<title level="m" type="main" coords="26,337.72,386.29,166.28,9.58;26,117.96,397.25,33.75,9.58">Code llama: Open foundation models for code</title>
		<author>
			<persName coords=""><forename type="first">Jonas</forename><surname>Baptiste Rozière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sten</forename><surname>Gloeckle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Itai</forename><surname>Sootla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yossi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingyu</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tal</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jérémy</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Artyom</forename><surname>Rapin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Kozhevnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joanna</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manish</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristian Canton</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhan</forename><surname>Grattafiori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jade</forename><surname>Défossez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faisal</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Synnaeve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,108.00,416.41,396.00,9.58;26,117.54,427.24,284.84,9.71" xml:id="b60">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m" coords="26,348.06,416.41,155.93,9.58;26,117.54,427.37,115.43,9.58">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="26,108.00,446.52,354.47,9.58;26,108.00,465.68,245.80,9.58" xml:id="b61">
	<analytic>
		<title level="a" type="main" coords="26,177.80,446.52,254.98,9.58">Fast transformer decoding: One write-head is all you need</title>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coords="26,108.00,465.68,216.38,9.58">Noam Shazeer. Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,108.00,484.84,396.00,9.58;26,117.54,495.67,247.23,9.71" xml:id="b62">
	<monogr>
		<title level="m" type="main" coords="26,339.07,484.84,164.93,9.58;26,117.54,495.80,77.81,9.58">Jetmoe: Reaching llama2 performance with 0.1 m dollars</title>
		<author>
			<persName coords=""><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07413</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="26,108.00,514.95,396.00,9.58;26,117.96,525.91,386.04,9.58;26,117.96,536.74,247.55,9.71" xml:id="b63">
	<monogr>
		<title level="m" type="main" coords="26,197.03,525.91,306.97,9.58;26,117.96,536.87,77.94,9.58">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="26,108.00,556.03,397.05,9.58;26,117.96,567.67,277.72,8.30" xml:id="b64">
	<monogr>
		<title level="m" type="main" coords="26,163.80,556.03,174.63,9.58">Snowflake arctic -llm for enterprise ai</title>
		<author>
			<persName coords=""><surname>Snowflake</surname></persName>
		</author>
		<ptr target="https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="26,108.00,586.14,397.25,9.58;26,117.96,597.10,269.33,9.58" xml:id="b65">
	<monogr>
		<title level="m" type="main" coords="26,460.56,586.14,44.69,9.58;26,117.96,597.10,239.15,9.58">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName coords=""><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,108.00,616.26,397.25,9.58;26,117.65,627.22,388.09,9.58;26,117.96,638.05,387.28,9.71;26,117.96,649.14,22.42,9.58" xml:id="b66">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m" coords="26,151.34,638.18,208.34,9.58">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="26,108.00,668.29,396.00,9.58;26,117.96,679.25,387.28,9.58;26,117.96,690.21,387.69,9.58;26,117.96,701.04,387.69,9.71;26,117.96,712.13,387.08,9.58;26,117.96,723.77,213.23,8.30" xml:id="b67">
	<analytic>
		<title level="a" type="main" coords="26,328.68,679.25,110.79,9.58">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/" />
	</analytic>
	<monogr>
		<title level="m" coords="26,170.37,701.04,220.64,9.50">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>paper files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</note>
</biblStruct>

<biblStruct coords="27,108.00,85.05,397.25,9.58;27,117.96,96.01,387.28,9.58;27,117.96,106.97,387.69,9.58;27,117.96,117.93,386.34,9.58;27,117.96,129.57,155.45,8.30" xml:id="b68">
	<monogr>
		<title level="m" type="main" coords="27,387.92,106.97,117.74,9.58;27,117.96,117.93,157.16,9.58">Recode: Robustness evaluation of code generation models</title>
		<author>
			<persName coords=""><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haifeng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingyue</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samson</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishna</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2212.10264</idno>
		<ptr target="https://arxiv.org/abs/2212.10264" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,108.00,147.81,396.00,9.58;27,117.96,158.77,386.04,9.58;27,117.96,169.73,325.78,9.58" xml:id="b69">
	<monogr>
		<title level="m" type="main" coords="27,164.68,169.73,249.76,9.58">Helpsteer: Multi-attribute helpfulness dataset for steerlm</title>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virginia</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Narsimhan</forename><surname>Makesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sreedhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Egert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jane</forename><forename type="middle">Polak</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neel</forename><surname>Scowcroft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oleksii</forename><surname>Swope</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kuchaiev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,108.00,188.66,396.00,9.58;27,117.96,199.62,387.69,9.58;27,117.96,210.45,386.04,9.71;27,117.96,221.41,386.03,9.71;27,117.63,232.37,387.41,9.71;27,117.69,244.14,94.02,8.30" xml:id="b70">
	<analytic>
		<title level="a" type="main" coords="27,344.03,199.62,161.62,9.58;27,117.96,210.58,85.72,9.58">On layer normalization in the transformer architecture</title>
		<author>
			<persName coords=""><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v119/xiong20b.html" />
	</analytic>
	<monogr>
		<title level="m" coords="27,405.90,210.45,98.10,9.50;27,117.96,221.41,183.70,9.50">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,108.00,262.38,397.65,9.58;27,117.96,273.34,388.07,9.58;27,117.96,284.30,320.16,9.58" xml:id="b71">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Fanjia</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huanzhi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charlie</forename><surname>Cheng-Jie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianjun</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Shishir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://gorilla.cs.berkeley.edu/blogs/8berkeleyfunctioncallingleaderboard.html" />
		<title level="m" coords="27,249.68,273.34,167.21,9.58">Berkeley function calling leaderboard</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,108.00,303.23,396.31,9.58;27,117.96,314.19,386.04,9.58;27,117.96,325.02,387.78,9.71" xml:id="b72">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Longhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weisen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jincheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhengying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Metamath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.12284</idno>
		<title level="m" coords="27,413.73,314.19,90.27,9.58;27,117.96,325.15,218.33,9.58">Bootstrap your own mathematical questions for large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="27,108.00,344.08,396.00,9.58;27,117.96,355.04,387.78,9.58;27,117.96,365.87,161.71,9.71" xml:id="b73">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xingwei</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05653</idno>
		<title level="m" coords="27,148.08,355.04,353.03,9.58">Mammoth: Building math generalist models through hybrid instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="27,108.00,384.93,335.18,9.58" xml:id="b74">
	<monogr>
		<title level="m" type="main" coords="27,247.15,384.93,166.51,9.58">Root mean square layer normalization</title>
		<author>
			<persName coords=""><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,108.00,403.85,396.00,9.58;27,117.96,414.81,193.26,9.58" xml:id="b75">
	<monogr>
		<title level="m" type="main" coords="27,232.58,403.85,271.42,9.58;27,117.96,414.81,163.50,9.58">A curated collection of 2 million mathematical questions and answers sourced from stack exchange</title>
		<author>
			<persName coords=""><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stackmathqa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,108.00,83.79,157.76,11.21;28,107.61,109.00,397.63,9.58;28,108.00,119.95,397.25,9.58;28,108.00,130.91,397.65,9.58;28,108.00,141.87,397.25,9.58;28,108.00,152.83,397.25,9.58;28,107.83,163.79,397.83,9.58;28,108.00,174.75,397.25,9.58;28,108.00,185.71,397.25,9.58;28,108.00,196.67,397.24,9.58;28,108.00,207.63,397.25,9.58;28,107.69,218.58,397.96,9.58;28,108.00,229.54,243.91,9.58" xml:id="b76">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Abap</forename><surname>Programming Languages</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ada</forename><surname>Agda</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Alloy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Antlr</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Applescript</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Arduino</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Asp</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Assembly</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Augeas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Awk</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Batchfile</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bluespec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C-Sharp</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C++</forename><surname>Clojure</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cmake</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cobol</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Coffeescript</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Common-Lisp</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Css</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cucumber</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cuda</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cython</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dart</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dockerfile</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Eagle</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Elixir</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Elm</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Emacs-Lisp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F-Sharp</forename><surname>Erlang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fortran</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Glsl</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Go</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gradle</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Graphql</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Groovy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Haskell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Haxe</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hcl</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Html</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><surname>Idris</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Java</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Java-Server-Pages</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Javascript</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jsoniq</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jsonld</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jsx</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Julia</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jupyter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lean</forename><surname>Kotlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Literate-Coffeescript</forename><surname>Literate-Agda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="28,436.59,218.58,69.06,9.58;28,108.00,229.54,41.06,9.58">Web-Ontology-Language</title>
		<imprint>
			<publisher>Literate-Haskell</publisher>
			<pubPlace>Lua, Makefile, Maple, Markdown, Mathematica, Matlab, Objective-C++, OCaml, OpenCL, Pascal, Perl, PHP, PowerShell, Prolog, Protocol-Buffer, Python, Python-traceback, R, Racket, RDoc, Restructuredtext, RHTML, RMarkdown, Ruby, Rust, SAS, Scala, Scheme, Shell, Smalltalk, Solidity, SPARQL, SQL, Stan, Standard-ML, Stata, Swift, SystemVerilog, Tcl, Tcsh, Tex, Thrift, Twig, TypeScript, Verilog, VHDL, Visual-Basic, Vue; WebAssembly, XML, XSLT, Yacc, YAML, Zig</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
