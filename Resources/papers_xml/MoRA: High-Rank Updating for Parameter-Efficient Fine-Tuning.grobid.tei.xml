<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coords="1,95.39,79.85,404.51,12.90">MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-20">20 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,91.87,120.94,54.60,10.75"><forename type="first">Ting</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.92,120.94,82.06,10.75"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.43,120.94,72.75,10.75"><forename type="first">Shengyue</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.63,120.94,66.78,10.75"><forename type="first">Zihan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,421.86,120.94,80.05,10.75"><forename type="first">Haizhen</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName coords="1,78.61,135.00,48.70,10.75"><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,140.77,135.00,66.63,10.75"><forename type="first">Weiwei</forename><surname>Deng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.84,135.00,47.87,10.75"><forename type="first">Feng</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.16,135.00,48.84,10.75"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName coords="1,344.44,135.00,69.30,10.75"><forename type="first">Deqing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,431.46,135.00,80.72,10.75"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coords="1,95.39,79.85,404.51,12.90">MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-20">20 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">3A5200D394923BBE81032CE449CE682C</idno>
					<idno type="arXiv">arXiv:2405.12130v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-11T09:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low-rank adaptation (LoRA) is a popular parameter-efficient fine-tuning (PEFT) method for large language models (LLMs). In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memoryintensive tasks and achieves comparable performance on other tasks. Our code will be available at <ref type="url" coords="1,116.65,545.47,147.03,8.00" target="https://github.com/kongds/MoRA">https://github.com/kongds/MoRA</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the size of language models increases, parameter-efficient fine-tuning (PEFT) <ref type="bibr" coords="1,248.10,603.13,41.42,9.46;1,70.87,616.67,51.41,9.46" target="#b13">(Houlsby et al., 2019)</ref> has emerged as a popular technique to adapt these models to specific downstream tasks. Compared to Full Fine-Tuning (FFT), which updates all model parameters, PEFT modifies only a small part of the parameters. For example, it can achieve similar performance with FFT by updating less than 1% of the parameters in some tasks <ref type="bibr" coords="1,271.86,697.97,17.27,9.46;1,70.87,711.52,49.89,9.46" target="#b14">(Hu et al., 2021)</ref>, which significantly reduces the memory requirements for the optimizer and facilitates the storage and deployment of fine-tuned models.</p><p>Among the existing PEFT methods, Low-Rank Adaptation (LoRA) <ref type="bibr" coords="1,163.40,765.73,77.27,9.46" target="#b14">(Hu et al., 2021)</ref> is particu- larly prevalent for LLMs. LoRA enhances performance over other PEFT methods such as prompt tuning <ref type="bibr" coords="1,337.92,534.05,89.11,9.46" target="#b20">(Lester et al., 2021)</ref> or adapters <ref type="bibr" coords="1,483.38,534.05,41.42,9.46;1,306.14,547.60,52.92,9.46" target="#b13">(Houlsby et al., 2019)</ref> by updating parameters via low-rank matrices. These matrices can be merged into the original model parameters, thereby avoiding additional computational costs during inference. There are numerous methods that aim to improve LoRA for LLMs. However, most methods primarily validate their efficiency based on GLUE <ref type="bibr" coords="1,469.73,628.89,56.05,9.46;1,306.14,642.44,23.95,9.46" target="#b34">(Wang et al., 2018)</ref>, either by achieving better performance or by requiring fewer trainable parameters. Recent methods <ref type="bibr" coords="1,346.39,669.54,73.99,9.46" target="#b23">(Liu et al., 2024;</ref><ref type="bibr" coords="1,423.10,669.54,80.74,9.46" target="#b26">Meng et al., 2024;</ref><ref type="bibr" coords="1,506.57,669.54,17.84,9.46;1,306.14,683.09,53.03,9.46" target="#b39">Zhu et al., 2024)</ref> leverage instruction tuning task such as Alpaca <ref type="bibr" coords="1,353.34,696.64,86.64,9.46" target="#b35">(Wang et al., 2024)</ref> or reasoning tasks like GSM8K <ref type="bibr" coords="1,367.87,710.19,91.51,9.46" target="#b3">(Cobbe et al., 2021)</ref> to better evaluate their performance on LLMs. However, the diverse settings and datasets used in the evaluation complicate the understanding of their progression.</p><p>In this paper, we conduct a comprehensive eval-uation of LoRA across various tasks under the same settings, including instruction tuning, mathematical reasoning, and continual pretraining. We find that LoRA-like methods demonstrate similar performance across these tasks and they perform comparably to FFT in instruction tuning but fall short in mathematical reasoning and continual pretraining. Among these tasks, instruction tuning primarily focuses on interacting with the format, rather than acquiring knowledge and capabilities, which are learned almost entirely during pretraining <ref type="bibr" coords="2,88.90,223.76,84.22,9.46" target="#b38">(Zhou et al., 2024)</ref>. We observe that LoRA is easily adapted to follow response formats in instruction tuning but struggles with other tasks that require enhancing knowledge and capabilities through fine-tuning.</p><p>One plausible explanation for this limitation observed with LoRA could be its reliance on low-rank updates <ref type="bibr" coords="2,107.08,323.13,82.92,9.46" target="#b21">(Lialin et al., 2023)</ref>. The low-rank update matrix, ∆W , struggles to estimate the full-rank updates in FFT, particularly in memory-intensive tasks like continual pretraining that require memorizing domain-specific knowledge. Since the rank of ∆W is significantly smaller than the full rank, this limitation restricts capacity to store new information via fine-tuning. Moreover, current variants of LoRA cannot alter the inherent characteristic of low-rank updates. To validate this, we conducted a memorization task using pseudo-data to assess the performance of LoRA in memorizing new knowledge. We found that LoRA performed significantly worse than FFT, even with a large rank such as 256.</p><p>Given these observations, we introduce a method called MoRA, which employs a square matrix as opposed to low-rank matrices, aiming to maximize the rank in ∆W while maintaining the same number of trainable parameters. For instance, when utilizing 8 rank with the hidden size 4096, LoRA employs two low-rank matrices A ∈ R 4096×8 and B ∈ R 8×4096 , with rank(∆W ) ≤ 8. Under same number of parameters, our method uses a square matrix M ∈ R 256×256 , with rank(∆W ) ≤ 256, as depicted in Figure <ref type="figure" coords="2,168.17,652.82,4.17,9.46" target="#fig_0">1</ref>. Notably, our method exhibits a greater capacity than LoRA with a large rank. To decrease the input dimension and increase the output dimension for M , we develop corresponding non-parameter operators. Furthermore, these operators and M can be substituted by a ∆W , ensuring our method can be merged back into LLM like LoRA.</p><p>Our contributions are as follows:</p><p>1. We introduce MoRA, a novel method that employs a square matrix instead of low-rank matrices in LoRA to achieve high-rank updating, while maintaining the same number of trainable parameters.</p><p>2. We discuss four kinds of non-parameter operators of MoRA to reduce the input dimension and increase the output dimension for the square matrix, while ensures that the weight can be merged back into LLMs.</p><p>3. We evaluate MoRA across five tasks: memory, instruction tuning, mathematical reasoning, continual pretraining, and pretraining.</p><p>Our method outperforms LoRA on memoryintensive tasks and achieves comparable performance on other tasks, which demonstrates the effectiveness of high-rank updating.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LoRA</head><p>LoRA is one of the most popular PEFT methods for fine-tuning LLM, owing to its broad applicability and robust performance in comparison to other methods. To approximate the updated weight ∆W in FFT, LoRA employs two low-rank matrices for its decomposition. By adjusting the rank of these two matrices, LoRA can accordingly modify the trainable parameters. Benefit from it, LoRA can merge these matrices after fine-tuning without the inference latency compared to FFT. There are many methods to further improve LoRA, particularly for the application in LLMs. DoRA <ref type="bibr" coords="2,444.88,522.18,80.26,9.46" target="#b23">(Liu et al., 2024)</ref> further decomposes the original weight into magnitude and direction components and uses LoRA to update the direction component. LoRA+ <ref type="bibr" coords="2,488.88,562.83,35.54,9.46;2,306.14,576.38,49.48,9.46" target="#b10">(Hayou et al., 2024</ref>) employs different learning rates for the two low-rank matrices to improve learning efficiency. ReLoRA <ref type="bibr" coords="2,381.62,603.48,96.15,9.46" target="#b21">(Lialin et al., 2023)</ref> integrates LoRA into the LLM during training to increase the rank of the final ∆W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-Tuning with LLMs</head><p>Despite the impressive performance of LLMs with in-context learning, certain scenarios still necessitate fine-tuning, which can be broadly categorized into three types. The first type, instruction tuning, aims to better align LLMs with end tasks and user preferences, without significantly enhancing the knowledge and capabilities of LLMs <ref type="bibr" coords="2,471.66,752.18,54.11,9.46;2,306.14,765.73,23.95,9.46" target="#b38">(Zhou et al., 2024)</ref>. This approach simplifies the process of dealing with varied tasks and understanding complex instructions. The second type involves complex reasoning tasks such as mathematical problemsolving <ref type="bibr" coords="3,105.19,115.37,88.05,9.46" target="#b4">(Collins et al., 2023;</ref><ref type="bibr" coords="3,195.99,115.37,78.51,9.46" target="#b15">Imani et al., 2023;</ref><ref type="bibr" coords="3,277.24,115.37,11.89,9.46;3,70.87,128.92,48.71,9.46" target="#b36">Yu et al., 2023)</ref>, where general instruction tuning often falls short in handling complex, symbolic, multistep reasoning tasks. To improve the reasoning abilities of LLMs, the majority of research focuses on creating corresponding training datasets, either by leveraging larger teacher models like GPT-4 <ref type="bibr" coords="3,274.28,196.67,14.85,9.46;3,70.87,210.22,52.74,9.46" target="#b7">(Fu et al., 2023)</ref>, or by rephrasing questions along a reasoning path <ref type="bibr" coords="3,140.34,223.76,72.26,9.46" target="#b36">(Yu et al., 2023)</ref>. The third type, continual pretraining <ref type="bibr" coords="3,169.38,237.31,92.13,9.46" target="#b2">(Cheng et al., 2023;</ref><ref type="bibr" coords="3,265.65,237.31,23.49,9.46;3,70.87,250.86,55.23,9.46" target="#b0">Chen et al., 2023;</ref><ref type="bibr" coords="3,130.24,250.86,77.90,9.46" target="#b9">Han et al., 2023;</ref><ref type="bibr" coords="3,212.27,250.86,73.43,9.46" target="#b22">Liu et al., 2023)</ref>, aims to enhance the domain-specific capabilities of LLMs. Unlike instruction tuning, it necessitates fine-tuning to augment the corresponding domainspecific knowledge and capabilities. However, most variants of LoRA <ref type="bibr" coords="3,242.65,318.61,46.48,9.46;3,70.87,332.16,55.12,9.46" target="#b19">(Kopiczko et al., 2023;</ref><ref type="bibr" coords="3,130.06,332.16,85.79,9.46" target="#b21">Lialin et al., 2023;</ref><ref type="bibr" coords="3,219.93,332.16,70.57,9.46;3,70.87,345.71,25.30,9.46" target="#b6">Dettmers et al., 2024;</ref><ref type="bibr" coords="3,98.89,345.71,73.56,9.46" target="#b39">Zhu et al., 2024)</ref> predominantly employ instruction tuning or text classification tasks from GLUE <ref type="bibr" coords="3,102.03,372.81,81.93,9.46" target="#b34">(Wang et al., 2018)</ref> to validate their efficacy on LLMs. Given that instruction tuning requires the least capacity for fine-tuning compared to other types, it may not accurately reflect the effectiveness of LoRA variants. To better evaluate their methods, recent works <ref type="bibr" coords="3,154.67,440.55,87.69,9.46" target="#b26">(Meng et al., 2024;</ref><ref type="bibr" coords="3,246.04,440.55,44.45,9.46;3,70.87,454.10,25.10,9.46" target="#b23">Liu et al., 2024;</ref><ref type="bibr" coords="3,98.71,454.10,69.43,9.46" target="#b31">Shi et al., 2024;</ref><ref type="bibr" coords="3,170.88,454.10,118.99,9.46" target="#b28">Renduchintala et al., 2023)</ref> have employed reasoning tasks to test their methods. But the training sets used are often too small for LLMs to effectively learn reasoning. For instance, some methods <ref type="bibr" coords="3,166.25,508.30,80.76,9.46" target="#b26">(Meng et al., 2024;</ref><ref type="bibr" coords="3,249.38,508.30,36.37,9.46;3,70.87,521.85,76.77,9.46" target="#b28">Renduchintala et al., 2023)</ref> utilize the GSM8K <ref type="bibr" coords="3,233.38,521.85,57.12,9.46;3,70.87,535.40,24.94,9.46" target="#b3">(Cobbe et al., 2021)</ref> with only 7.5K training samples. Compare to the SOTA method with 395K training samples <ref type="bibr" coords="3,273.58,548.95,15.55,9.46;3,70.87,562.49,49.81,9.46" target="#b36">(Yu et al., 2023)</ref>, this small training set achieves worse performance on reasoning and makes it hard to evaluate the effectiveness of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis the Influence of Low-rank Updating</head><p>The key idea of LoRA <ref type="bibr" coords="3,174.86,646.41,74.26,9.46" target="#b14">(Hu et al., 2021)</ref> involves the use of low-rank updates to estimate full-rank updates in FFT. Formally, given a pretrained parameter matrix W 0 ∈ R d×k , LoRA employs two low-rank matrices to calculate the weight update ∆W :</p><formula xml:id="formula_0" coords="3,102.52,732.82,187.35,10.63">h = W 0 x + ∆W x = W 0 x + BAx (1)</formula><p>where A ∈ R r×k and B ∈ R d×r represent the lowrank matrices in LoRA. To ensure that ∆W = 0  <ref type="bibr" coords="3,454.13,398.26,71.19,9.46" target="#b23">(Liu et al., 2024;</ref><ref type="bibr" coords="3,306.14,411.81,75.07,9.46" target="#b26">Meng et al., 2024)</ref>. However, for tasks like complex reasoning or continual pretraining, LoRA tends to show worse performance <ref type="bibr" coords="3,418.15,438.91,71.66,9.46" target="#b22">(Liu et al., 2023)</ref>.</p><p>Based on these observations, we propose a hypothesis that low-rank updating is easy to leverage original knowledge and capabilities of LLM to solve task, but it is struggle to handle tasks that require enhancing knowledge and capabilities of LLM.</p><p>To substantiate this hypothesis, we examine the differences between LoRA and FFT in terms of memorizing new knowledge through fine-tuning. In order to circumvent leveraging the original knowledge of the LLM, we randomly generate 10K pairs of Universally Unique Identifiers (UUIDs), each pair comprising two UUIDs with 32 hexadecimal values. The task requires the LLM to generate the corresponding UUID based on the input UUID. For instance, given a UUID such as "205f3777-52b6-4270-9f67-c5125867d358", the model should generate the corresponding UUID based on 10K training pairs. This task can also be viewed as a question-answering task, while the knowledge indispensable for accomplishing it is exclusively from the training datasets rather than the LLM itself.</p><p>For the training settings, we employ LLaMA-2 7B as base model, utilizing 1,000 pairs per batch and conducting 100 epochs. For the LoRA, we apply low-rank matrices to all linear layers and search learning rate from {1e-4,2e-4,3e-4} to enhance performances. We conduct the experiment on LoRA using various ranks r ∈ {8, 16, 32, 64, 128, 256}.</p><p>For the FFT, we directly use a learning rate of 3e-5.</p><p>Based on Figure <ref type="figure" coords="4,145.49,169.57,4.12,9.46" target="#fig_1">2</ref>, we observe low-rank updating are hard to memorizing new knowledge compared to FFT. Although constantly increasing the rank of LoRA can alleviate this problem, the gap still exists.</p><p>In contrast to the memory task, we also evaluate the performance gap between LoRA and FFT on instruction tuning, which merely introduces new knowledge. Similar to previous results <ref type="bibr" coords="4,236.69,277.96,53.81,9.46;4,70.87,291.51,25.35,9.46" target="#b26">(Meng et al., 2024;</ref><ref type="bibr" coords="4,99.77,291.51,74.20,9.46" target="#b39">Zhu et al., 2024)</ref>, we also find that LoRA matches the performance of FFT with small rank r = 8 in Table <ref type="table" coords="4,142.19,318.61,4.17,9.46" target="#tab_1">1</ref>. This indicates that LoRA can easily leverage the original knowledge of LLMs by fine-tuning like FFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>Based on the above analysis, we propose a new method to alleviate the negative effects of low-rank updating. The main idea of our method is to utilize the same trainable parameters as much as possible to achieve a higher rank in ∆W . Consider to the pretrained weight W 0 ∈ R d×k , LoRA uses two low-rank matrices A and B with (d + k)r total trainable parameters for rank r. Under same trainable parameters, a square matrix M ∈ R r×r where r = ⌊ (d + k)r⌋ can achieve the highest rank due to r ≪ min(d, k).</p><p>To accomplish this, we need to reduce the input dimension and increase the output dimension for M . Formally,</p><formula xml:id="formula_1" coords="4,104.91,587.05,184.96,10.82">h = W 0 x + f decomp M f comp (x)<label>(2)</label></formula><p>where f comp : R k → R r denotes the function that decreases the input dimension of x from k to r, and f decomp : R r → R d represents the function that enhances the output dimension from r to d. Furthermore, these two functions ought to be nonparameterized operators and expected to execute in linear time corresponding to the dimension. They should also have corresponding function, f comp : R r×r → R r×k and f decomp : R r×k → R d×k , to transform M into ∆W . For any x, the following should hold:</p><formula xml:id="formula_2" coords="4,84.23,762.89,205.64,13.32">f decomp M f comp (x) = ∆W x, ∀x ∈ R k (3)</formula><p>where ∆W = f decomp f comp (M ) . If Eq. 3 holds, M can be losslessly expanded to ∆W based on f comp and f decomp . This allows our method to merge back into the LLM like LoRA.</p><p>For the design of f comp and f comp , we explore several methods to implement these functions. One straightforward method is truncating the dimension and subsequently add it in corresponding dimension. Formally, this can be represented as:</p><formula xml:id="formula_3" coords="4,375.30,206.60,149.84,40.65">f comp (x) = x 1:r f decomp (x) = x 0 (4)</formula><p>and the corresponding ∆W is:</p><formula xml:id="formula_4" coords="4,377.79,284.97,147.35,23.15">∆W = M 0 0 0 (5)</formula><p>However, this method leads to a significant loss of information during compression and only modifies a segment of the output by appending a zero vector during decompression. To improve it, we can share the rows and columns of M to achieve a more efficient compression and decompression. Formally, this can be represented as:</p><formula xml:id="formula_5" coords="4,351.97,423.06,173.17,39.90">f comp (x) = j∈g i x j r i=1 f decomp (x) = x g ′ i d i=1<label>(6)</label></formula><p>Here, g and g ′ represent predefined groups that share the same row and column in M , respectively. The j ∈ g i indicates that the j-th dimension belongs to the i-th group in g. The term g ′ i is the reverse of g ′ i , referring to the i-th dimension associated with the g ′ i -th group in g ′ . The corresponding ∆W is as follows:</p><formula xml:id="formula_6" coords="4,379.22,582.74,145.92,13.57">∆W i,j = M g ′ i , g j (7)</formula><p>Sharing rows and columns can be efficient for larger ranks such as r = 128 or r = 256, as only a few rows or columns in ∆W share a common row or column. For instance, considering to ∆W ∈ R 4096×4096 for r = 128, which has r = 1024 and M ∈ R 1024×1024 . In this situation, only 4 rows or columns share the same row or column. Conversely, for smaller ranks such as r = 8, where r = 256, it requires average 16 rows or columns in a group to share the same row or column in M . It can lead to inefficiencies due to the significant information loss during compression in Eq. 6.</p><p>To enhance performance for smaller ranks, we reshape x instead of directly compressing it, to preserve the input information. In this context, f comp (x) : R k → R n×r and f decomp : R n×r → R d . Corresponding f comp , f decomp and ∆W are as follows:</p><formula xml:id="formula_7" coords="5,94.06,161.10,195.67,73.93">fcomp (x) = x 1:r x r:2r • • • x (n-1)r:nr fdecomp (x) = concat(x) ∆W =     M 0 • • • 0 0 M • • • 0 . . . . . . . . . . . . 0 0 • • • M     (8)</formula><p>where concat(x) refers to concatenate the rows of x into a vector. For simplicity, we omit the padding and truncation operators in above functions and focus on the case where d = k. In comparison to sharing columns and rows, this method incurs additional computational overhead by reshaping x into R n×r instead of R r. However, given that the size of M is significantly smaller than W 0 , this additional computation is very small for rank like 8. For instance, when fine-tuning the 7B model with rank of 8 (r = 256), this method is only 1.03 times slower than previous methods. Inspired by RoPE <ref type="bibr" coords="5,163.64,410.12,69.33,9.46" target="#b33">(Su et al., 2024)</ref>, we can further refine this method by incorporating rotation operators into f comp to augment the expressiveness of M by enable it to differentiate between various x ir:(i+1)r by rotating them. We can modify Eq. 8 as follows:</p><formula xml:id="formula_8" coords="5,109.15,495.07,180.58,63.46">fcomp (x) = a 1 a 2 • • • a n-1 ∆W =      P 1 0 • • • 0 0 P 2 • • • 0 . . . . . . . . . . . . 0 0 • • • P n-1     <label>(9)</label></formula><p>where a i and P i represent the corresponding values of x ir:(i+1)r and M post-rotation, respectively. Following RoPE, we use a r × r block diagonal matrix to achieve the rotation. However, our method use rotation information to enable M distinguish the x ir:(i+1)r instead of token position in RoPE. We can define a i and P i as follows:</p><formula xml:id="formula_9" coords="5,88.00,671.58,135.24,51.93">a i =      R θ 1 ,i 0 • • • 0 0 R θ 2 ,i • • • 0 . . . . . . . . . . . . 0 0 • • • R θ r 2 ,i     </formula><p>x ir:(i+1)r where θ j = 10000 -2(j-1)/r and R θ j ,i ∈ R 2×2 is a rotation matrix: R θ j ,i = cos iθ j -sin iθ j sin iθ j cos iθ j (11)</p><formula xml:id="formula_10" coords="5,85.72,720.94,204.02,58.34">P i = M      R θ 1 ,i 0 • • • 0 0 R θ 2 ,i • • • 0 . . . . . . . . . . . . 0 0 • • • R θ r 2 ,i      (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>We evaluate our method on various tasks to understand the influence of high-rank updating. In Section 5.1, we evaluate our method with LoRA and our method on memorizing UUID pairs to show the benefit of high-rank updating on memorizing. In Section 5.2, we reproduce LoRA, LoRA variants and FFT on three fine-tuning tasks: instruction tuning, mathematical reasoning and continual pretraining. In Section 5.3, we compare our method with LoRA and ReLoRA on pretraining by training transformer from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Memorizing UUID Pairs</head><p>We first compare our method with LoRA and FFT on memorizing UUID pairs to demonstrate improvements through high-rank updating. Following the training settings in Section 3, we search learning rate from {5e-5,1e-4,2e-4} and use decompress and compress functions in Eq. 8, sharing rows and columns in M . Due to use one matrix M instead of two matrices A and B, we can directly initialize M with zeros. For the predefined groups g and g ′ , we group every adjacent r rows or columns together. The training loss is presented in Figure3.</p><p>Our method shows significant improvements over LoRA with the same number of trainable parameters, benefiting from high-rank updating. We also report character-level accuracy at various training steps in  LoRA. Compared to FFT, MoRA with 256 rank can achieve similar performance and both method can memorize all UUID pairs in 500 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fine-tuning Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Setup</head><p>We evaluate our method across three fine-tuning tasks for large language models (LLMs): instruction tuning, mathematical reasoning, and continual pretraining. For these tasks, we select highquality corresponding datasets to test both LoRA and our method. In instruction tuning, we utilize Tülu v2 <ref type="bibr" coords="6,135.27,643.79,93.29,9.46" target="#b16">(Ivison et al., 2023)</ref>, a blend of several high-quality instruction datasets, containing 326k filtered samples. We assess instruction performance using the MMLU <ref type="bibr" coords="6,237.23,684.44,51.91,9.46;6,70.87,697.99,55.59,9.46" target="#b11">(Hendrycks et al., 2020)</ref> in both zero-shot and five-shot settings. For mathematical reasoning, we employ the MetaMath <ref type="bibr" coords="6,135.58,725.09,71.12,9.46" target="#b36">(Yu et al., 2023)</ref> with its 395k samples to enhance mathematical reasoning capabilities and also use GSM8K <ref type="bibr" coords="6,184.08,752.18,86.63,9.46" target="#b3">(Cobbe et al., 2021)</ref> and MATH <ref type="bibr" coords="6,105.05,765.73,107.40,9.46" target="#b12">(Hendrycks et al., 2021)</ref> for further evalu-ation. In continual pretraining, we adapt an LLM to the biomedicine and finance using PubMed abstracts from the Pile <ref type="bibr" coords="6,396.36,377.74,76.58,9.46" target="#b8">(Gao et al., 2020)</ref> and finicial news, complemented by data preprocessing methods from AdaptLLM <ref type="bibr" coords="6,400.15,404.84,86.76,9.46" target="#b2">(Cheng et al., 2023)</ref> to boost performance. We report the average performance of corresponding tasks for continual pretraining. More details can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Baselines and Implements</head><p>For LoRA-like methods and MoRA, we conducted experiments at r = 8 and r = 256, and reproduce following methods across three tasks: FFT, LoRA, LoRA+ <ref type="bibr" coords="6,345.63,535.40,95.14,9.46" target="#b10">(Hayou et al., 2024)</ref>, AsyLoRA <ref type="bibr" coords="6,502.78,535.40,21.63,9.46;6,306.14,548.95,49.14,9.46" target="#b39">(Zhu et al., 2024</ref><ref type="bibr" coords="6,355.28,548.95,148.88,9.46">), ReLoRA (Lialin et al., 2023)</ref> and DoRA <ref type="bibr" coords="6,339.20,562.49,76.74,9.46" target="#b23">(Liu et al., 2024)</ref>. LoRA+ enhances the learning rate of matrix B in LoRA to facilitate efficient feature learning based on theoretical analysis. We search the corresponding the hyperparameter λ from {2,4}. AsyLoRA also analyzes asymmetry in the A and B matrices, and we adopted their initialization strategy. ReLoRA proposes a method to merge low-rank matrices into the model during training to increase the rank of ∆W . we search merge steps from {1k, 2k} and use 50 steps restarts warmup. DoRA leverages weight decomposition to enhance performance as a robust baseline. For FFT, we follow the settings proposed by corresponding datasets. For MoRA, we employed rotation operators as outlined in Eq. 9 to implement compression and decompression for r = 8, and for r = 256, we  utilized shared rows and columns as specified in Eq. 6 and group every adjacent r rows or columns together. The details hyperparameters about finetuning can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Results and Analysis</head><p>We present the results of fine-tuning tasks in Table 1. We report the results of MMLU with zeroshot and 5-shot settings for instruction tuning, GSM8K and MATH for mathematical reasoning, and average performance on biomedical tasks and financial tasks for continual pretraining.</p><p>MoRA shows on par performances with LoRA on instruction tuning and mathematical reasoning. Benefit from high-rank updating to memorize new knowledge, MoRA outperforms LoRA on both biomedical and financial domains for continual pretraining.</p><p>We also find that LoRA variants exhibit similar performances on these fine-tuning tasks as compared to LoRA. Although AsyLoRA achieves the best performance in instruction tuning, it demonstrates poor performance in mathematical reasoning. For ReLoRA, merging low-rank matrices during training can harm performance, particularly at the the high rank like 256.</p><p>Consider the difference between three tasks, they show different requirements for fine-tuning capabilities. For instruction tuning, which does not learn new knowledge from fine-tuning, rank 8 is enough to achieve performance similar to FFT. For mathematical reasoning, rank 8 is unable to match FFT performance. However, increasing the rank from 8 to 256 can eliminate the performance gap. For 250M 1.3B continual pretraining, LoRA with rank 256 still underperforms FFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Pretraining</head><p>To understand the influence of high-rank updating, we train transformer from scratch on the C4 datasets <ref type="bibr" coords="7,383.18,520.62,92.05,9.46" target="#b27">(Raffel et al., 2020)</ref>. For the model architeture, we train LLaMA-based model with RMSNorm <ref type="bibr" coords="7,387.59,547.72,133.39,9.46" target="#b37">(Zhang and Sennrich, 2019)</ref>, SwiGLU <ref type="bibr" coords="7,350.98,561.26,72.34,9.46" target="#b30">(Shazeer, 2020)</ref> and RoPE <ref type="bibr" coords="7,479.35,561.26,46.42,9.46;7,306.14,574.81,25.96,9.46" target="#b33">(Su et al., 2024)</ref> on 250M and 1.3B sizes. For the hyperparameters, we use 10k steps, 1024 batch size, 512 sequence length and follow Lialin et al., using rank r = 128 for LoRA and our methods and also keep modules without applying LoRA-like layernorm or embeddings unfreezed. We compare our method with LoRA and ReLoRA. To better show the difference between high-rank and low-rank updating, we reproduce ReLoRA and other methods without full-rank training warmup. For MoRA, we use compression and decompression functions in Eq. 6 by sharing columns and rows.</p><p>We also combine merge-and-reint in ReLoRA with our method called ReMoRA by merging M back into the original parameters during training to increase the rank of ∆W . However, if we directly merge M with g and g ′ in Eq. 6, the final rank of ∆W is unchanged due to the same expand pattern. To solve this problem, we can change g and g ′ after merging to ensure the rank of ∆W increasing. More details about ReMoRA can be found in Appendix B. For the hyperparameters corresponding to ReLoRA and ReMoRA, we merge every 2k steps and use 50 steps restarts warmup with optimizer reseting and jagged scheduler.</p><p>We show pretraining loss in Figure <ref type="figure" coords="8,237.92,427.09,5.49,9.46" target="#fig_4">4</ref> and corresponding perplexity on C4 validation dataset in Table <ref type="table" coords="8,99.38,454.19,4.17,9.46" target="#tab_3">3</ref>. Our method show better performance on pretraining compared to LoRA and ReLoRA with same amount of trainable parameters. Benefiting from high-rank updating, ReMoRA also achieves more improvements on MoRA compared to ReLoRA, which demonstrates the effectiveness of merge-and-reint strategy in ReMoRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">High-rank Updating</head><p>To demonstrate the impact of high-rank updating on the rank of ∆W , we analyzed the spectrum of singular values for the learned ∆W on 250M pretraining 250M model. We present the average count of singular values exceeding 0.1 across all layers for ∆W q , ∆W k , ∆W v , ∆W o , ∆W up , ∆W down , and ∆W gate in Figure <ref type="figure" coords="8,150.79,684.44,5.45,9.46" target="#fig_5">5</ref> following <ref type="bibr" coords="8,203.84,684.44,82.52,9.46" target="#b21">(Lialin et al., 2023)</ref>. MoRA and ReMoRA exhibit a substantially higher number of significant singular values compared to LoRA and ReLoRA, highlighting the effectiveness of our methods in increasing the rank of ∆W . We find the quantity of singular values shown in Figure 5 can be correlated with the perplexity metrics listed in Table <ref type="table" coords="8,368.38,74.72,4.01,9.46" target="#tab_3">3</ref>. Moreover, MoRA, without mergeand-reint strategy in ReLoRA and ReMoRA, can achieve a lower perplexity than ReLoRA along with a higher significant singular values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Influence of Decompression and Compression</head><p>To explore the impact of decompression and compression functions in MoRA, we report the performance on GSM8K using various methods: truncation, sharing, decoupling, and rotation in Table <ref type="table" coords="8,517.97,213.99,4.17,9.46" target="#tab_4">4</ref>. Among these methods, truncation shows the worst performance due to the significant information loss during compression. Sharing can achieve better performance than truncation by leveraging the shared rows or columns to preserve the input information. But in the case of r = 8, sharing shows worse performance than decouple and rotation due to the large number of sharing rows or columns, as we discussed in Section 4. Rotation is more efficient than decouple, due to the rotation information can help the square matrix to distinguish the input information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we analyze the impact of low-rank updating through LoRA and observe that such updating struggles for memory-intensive tasks, which also limits current LoRA variants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head><p>We propose hyperparameters in Table <ref type="table" coords="11,237.90,95.69,4.09,9.46">5</ref>. Table <ref type="table" coords="11,204.10,312.39,3.88,8.64">5</ref>: Hyperparameters for fine-tuning on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation of ReMoRA</head><p>We introduce detial implementation of ReMoRA in pretraining. In this case, we simply define two kinds of g. The first kind is grouping every adjacent r rows or columns together following the defined in fine-tuning, the first groups can be represented as {1, 2, . . . , r}. The second kind is grouping every neighboring k of the rows or columns together, the first groups can be represented as {1, 1 + k, . . . , 1 + rk}. We propose a example code about compression and decompression functions in Algorithm 1 and 2. After merging, we can change the group type from 0 to 1 or 1 to 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Downstream Tasks of Continual Pretraining</head><p>For biomedcine, we use PubMedQA <ref type="bibr" coords="12,233.82,95.64,69.52,9.46" target="#b18">(Jin et al., 2019)</ref>, RCT <ref type="bibr" coords="12,334.82,95.64,125.84,9.46" target="#b5">(Dernoncourt and Lee, 2017)</ref>, USMLE <ref type="bibr" coords="12,507.96,95.64,16.45,9.46;12,70.87,109.19,50.49,9.46" target="#b17">(Jin et al., 2021)</ref>, and selecting biomedicine subjects from MMLU to evaluate the performance. For finance, following <ref type="bibr" coords="12,116.64,122.73,148.96,9.46">BloombergGPT (Wu et al., 2023)</ref>,we use ConvFinQA <ref type="bibr" coords="12,359.01,122.73,76.33,9.46" target="#b1">(Chen et al., 2022</ref><ref type="bibr" coords="12,435.34,122.73,85.93,9.46;12,70.59,136.28,82.43,9.46">), NER (Salinas Alvarado et al., 2015)</ref>, Headline <ref type="bibr" coords="12,202.65,136.28,118.84,9.46" target="#b32">(Sinha and Khandait, 2021)</ref>, FiQA SA <ref type="bibr" coords="12,373.25,136.28,81.03,9.46" target="#b24">(Maia et al., 2018)</ref> and FPB <ref type="bibr" coords="12,497.70,136.28,26.72,9.46;12,70.87,149.83,50.14,9.46" target="#b25">(Malo et al., 2014)</ref>. We report the detail performance of these tasks following: </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,306.14,398.66,218.27,8.64;1,306.14,410.23,216.88,9.03;1,306.14,422.25,218.26,8.96;1,306.14,434.21,218.27,8.96;1,306.14,446.48,218.27,8.64;1,306.14,458.44,218.27,8.64;1,306.14,470.39,194.47,8.64;1,416.86,224.89,100.73,141.79"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of our method compared to LoRA under same number of trainable parameters. W is the frozen weight from model. A and B are trainable low-rank matrices in LoRA. M is the trainable matrix in our method. Gray parts are non-parameter operators to reducing the input dimension and increasing the output dimension. r represents the rank in two methods.</figDesc><graphic coords="1,416.86,224.89,100.73,141.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,306.14,242.96,218.27,8.64;3,306.14,254.91,163.83,8.64;3,307.23,70.87,216.08,160.24"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of memorizing UUID pairs through fine-tuning with FFT and LoRA.</figDesc><graphic coords="3,307.23,70.87,216.08,160.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,306.14,242.96,218.27,8.64;5,306.14,254.91,166.85,8.64;5,307.23,70.87,216.08,160.24"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of memorizing UUID pairs with LoRA and our method on rank 8 and 256.</figDesc><graphic coords="5,307.23,70.87,216.08,160.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,118.24,236.34,132.35,7.77;7,346.55,236.34,128.62,7.77"><head></head><label></label><figDesc>(a) Pretraining loss at 250M models. (b) Pretraining loss at 1.3B models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,70.87,260.95,453.90,8.64;7,70.87,272.59,453.90,8.96;7,70.87,284.54,248.19,8.96;7,75.28,75.85,218.27,152.79"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pretraining loss with LoRA and MoRA on 250M and 1B models from scratch. Both LoRA and MoRA use same amount of trainable parameters with r = 128. ReMoRA and ReLoRA refer to merge MoRA or LoRA back to the model during training to increase the rank of ∆W .</figDesc><graphic coords="7,75.28,75.85,218.27,152.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,70.87,243.38,216.88,8.96;8,70.87,255.66,127.01,8.64;8,70.87,70.86,218.27,160.99"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The number of singular values &gt; 0.1 in ∆W on the 250M pretraining model.</figDesc><graphic coords="8,70.87,70.86,218.27,160.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,306.14,752.18,218.27,23.01"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="5,306.14,752.18,218.27,23.01"><row><cell>. MoRA requires fewer training</cell></row><row><cell>steps to memorize these UUID pairs compared to</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,70.56,304.83,453.85,110.44"><head>Table 1 :</head><label>1</label><figDesc>Performance of FFT, LoRA, LoRA variants and our method on instruction tuning, mathematical reasoning and continual pretraining tasks.</figDesc><table coords="6,96.81,351.98,166.38,63.29"><row><cell></cell><cell cols="2">Rank 300</cell><cell>500</cell><cell>700</cell><cell>900</cell></row><row><cell>FFT</cell><cell>-</cell><cell>42.5</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>LoRA</cell><cell>8</cell><cell>9.9</cell><cell cols="3">10.0 10.7 54.2</cell></row><row><cell>MoRA</cell><cell>8</cell><cell cols="4">10.1 15.7 87.4 100</cell></row><row><cell>LoRA</cell><cell>256</cell><cell>9.9</cell><cell cols="2">70.6 100</cell><cell>100</cell></row><row><cell>MoRA</cell><cell>256</cell><cell cols="2">41.6 100</cell><cell>100</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,70.56,431.00,218.58,32.55"><head>Table 2 :</head><label>2</label><figDesc>Character-level accuracy of memorizing UUID pairs by generating the value of corresponding key in 300, 500, 700 and 900 training steps.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,326.46,340.55,177.33,75.13"><head>Table 3 :</head><label>3</label><figDesc>Perplexity on C4 validation dataset.</figDesc><table coords="7,341.21,340.55,148.15,50.11"><row><cell>LoRA</cell><cell>33.40 28.56</cell></row><row><cell>MoRA (Ours)</cell><cell>28.54 25.25</cell></row><row><cell>ReLoRA</cell><cell>32.19 27.80</cell></row><row><cell cols="2">ReMoRA (Ours) 26.74 23.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,305.83,402.34,218.58,89.94"><head>Table 4 :</head><label>4</label><figDesc>Influence of decompression and compression functions on r = 8 and r = 256 on GSM8K.</figDesc><table coords="8,323.64,418.29,175.87,37.66"><row><cell>Truncation</cell><cell>Eq. 4</cell><cell>59.5</cell><cell>66.6</cell></row><row><cell>Sharing</cell><cell>Eq. 6</cell><cell>62.5</cell><cell>67.9</cell></row><row><cell>Decouple</cell><cell>Eq. 8</cell><cell>63.6</cell><cell>67.8</cell></row><row><cell>Rotation</cell><cell>Eq. 9</cell><cell>64.2</cell><cell>67.9</cell></row></table><note coords="8,374.62,402.34,132.29,8.35"><p>fcomp, f decomp r = 8 r = 256</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="12,99.52,176.10,385.02,171.24"><head>Table 6 :</head><label>6</label><figDesc>Performance on biomedical tasks.</figDesc><table coords="12,99.52,176.10,385.02,171.24"><row><cell></cell><cell>r</cell><cell>PubMedQA</cell><cell cols="2">USMLE</cell><cell cols="2">BioMMLU</cell><cell>RCT</cell><cell>Avg.</cell></row><row><cell>FFT</cell><cell>-</cell><cell>74.1</cell><cell>41.2</cell><cell></cell><cell>47.5</cell><cell></cell><cell>62.7</cell><cell>56.4</cell></row><row><cell>LoRA</cell><cell>8</cell><cell>73.1</cell><cell>34.9</cell><cell></cell><cell>45.3</cell><cell></cell><cell>54.9</cell><cell>51.9</cell></row><row><cell>MoRA</cell><cell>8</cell><cell>73.3</cell><cell>34.7</cell><cell></cell><cell>45.3</cell><cell></cell><cell>59.9</cell><cell>53.3</cell></row><row><cell>LoRA</cell><cell>256</cell><cell>73.8</cell><cell>39.7</cell><cell></cell><cell>46.0</cell><cell></cell><cell>56.9</cell><cell>54.1</cell></row><row><cell>MoRA</cell><cell>256</cell><cell>74.4</cell><cell>40.4</cell><cell></cell><cell>46.1</cell><cell></cell><cell>60.6</cell><cell>55.4</cell></row><row><cell></cell><cell>r</cell><cell cols="2">ConvFinQA FiQA SA</cell><cell cols="2">Headline</cell><cell>NER</cell><cell>FPB</cell><cell>Avg.</cell></row><row><cell>FFT</cell><cell>-</cell><cell>44.4</cell><cell>78.8</cell><cell>82.3</cell><cell></cell><cell>68.1</cell><cell>74.3</cell><cell>69.6</cell></row><row><cell>LoRA</cell><cell>8</cell><cell>44.5</cell><cell>76.2</cell><cell>72.4</cell><cell></cell><cell>61.6</cell><cell>65.1</cell><cell>64.0</cell></row><row><cell>MoRA</cell><cell>8</cell><cell>45.8</cell><cell>76.6</cell><cell>76.3</cell><cell></cell><cell>68.9</cell><cell>68.2</cell><cell>67.1</cell></row><row><cell>LoRA</cell><cell>256</cell><cell>41.4</cell><cell>78.3</cell><cell>83.0</cell><cell></cell><cell>66.8</cell><cell>66.7</cell><cell>67.3</cell></row><row><cell>MoRA</cell><cell>256</cell><cell>47.7</cell><cell>76.3</cell><cell>83.4</cell><cell></cell><cell>68.0</cell><cell>68.1</cell><cell>68.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="12,220.66,363.07,153.64,8.64"><head>Table 7 :</head><label>7</label><figDesc>Performance on finicial tasks.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,70.87,94.40,219.52,8.64;9,81.78,105.36,207.36,8.64;9,81.42,116.32,209.37,8.64;9,81.78,127.27,207.36,8.64;9,81.78,138.05,207.36,8.82;9,81.78,149.01,75.27,8.59" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zefei</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xianyin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhongtian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bingxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiarong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.15205</idno>
		<title level="m" coords="9,268.78,116.32,22.01,8.64;9,81.78,127.27,207.36,8.64;9,81.78,138.23,144.12,8.64">Discfinllm: A chinese financial large language model based on multiple experts fine-tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,70.87,171.46,219.51,8.64;9,81.78,182.41,209.01,8.64;9,81.53,193.37,207.61,8.64;9,81.78,204.15,207.36,8.82;9,81.78,215.11,110.05,8.59" xml:id="b1">
	<monogr>
		<title level="m" type="main" coords="9,271.16,182.41,19.63,8.64;9,81.53,193.37,207.61,8.64;9,81.78,204.33,177.24,8.64">Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering</title>
		<author>
			<persName coords=""><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charese</forename><surname>Smiley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiqiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameena</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03849</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,70.87,237.55,220.01,8.64;9,81.42,248.51,209.37,8.64;9,81.78,259.29,171.74,8.82" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Daixuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.09530</idno>
		<title level="m" coords="9,81.42,248.51,209.37,8.64;9,81.78,259.47,29.30,8.64">Adapting large language models via reading comprehension</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,70.87,281.74,219.52,8.64;9,81.78,292.70,207.36,8.64;9,81.78,303.65,207.36,8.64;9,81.78,314.61,207.36,8.64;9,81.42,325.39,200.70,8.82" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m" coords="9,165.06,314.61,124.08,8.64;9,81.42,325.57,58.04,8.64">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,70.87,347.84,219.52,8.64;9,81.78,358.79,207.36,8.64;9,81.78,369.75,208.60,8.64;9,81.31,380.71,207.83,8.64;9,81.78,391.49,207.36,8.82;9,81.78,402.45,110.05,8.59" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Katherine M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lionel</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miri</forename><surname>Zilka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Umang</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01694</idno>
		<title level="m" coords="9,204.08,380.71,85.05,8.64;9,81.78,391.67,177.74,8.64">Evaluating language models for mathematics through interactions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,70.87,424.89,218.26,8.64;9,81.78,435.85,209.01,8.64;9,81.78,446.63,207.36,8.82;9,81.78,457.59,75.27,8.59" xml:id="b5">
	<monogr>
		<title level="m" type="main" coords="9,256.11,424.89,33.02,8.64;9,81.78,435.85,209.01,8.64;9,81.78,446.81,128.62,8.64">Pubmed 200k rct: a dataset for sequential sentence classification in medical abstracts</title>
		<author>
			<persName coords=""><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lee</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06071</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,70.87,480.03,218.27,8.64;9,81.78,490.99,207.36,8.64;9,81.78,501.77,207.35,8.82;9,81.47,512.73,95.29,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coords="9,183.65,490.99,105.48,8.64;9,81.78,501.95,68.93,8.64">Qlora: Efficient finetuning of quantized llms</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,158.30,501.77,130.83,8.59;9,81.47,512.73,76.09,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.87,535.17,218.27,8.64;9,81.47,546.13,207.67,8.64;9,81.78,556.91,209.01,8.82;9,81.78,567.87,207.36,8.82;9,81.03,579.01,89.39,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coords="9,166.27,546.13,122.87,8.64;9,81.78,557.09,155.88,8.64">Specializing smaller language models towards multi-step reasoning</title>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Litu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m" coords="9,267.85,556.91,22.94,8.59;9,81.78,567.87,175.87,8.59">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10421" to="10430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.87,601.27,219.92,8.64;9,81.78,612.23,209.01,8.64;9,81.78,623.19,209.10,8.64;9,81.47,634.15,209.32,8.64;9,81.78,644.93,204.67,8.82" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m" coords="9,81.47,634.15,209.32,8.64;9,81.78,645.11,61.95,8.64">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,70.87,667.37,219.92,8.64;9,81.78,678.33,207.53,8.64;9,81.78,689.29,209.10,8.64;9,81.78,700.25,207.36,8.64;9,81.78,711.03,207.36,8.82;9,81.78,721.99,110.05,8.59" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Tianyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jens-Michalis</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Oberhauser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Löser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Truhn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keno</forename><forename type="middle">K</forename><surname>Bressem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08247</idno>
		<title level="m" coords="9,81.78,700.25,207.36,8.64;9,81.78,711.21,174.34,8.64">Medalpaca-an open-source collection of medical conversational ai models and training data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,70.87,744.43,220.01,8.64;9,81.78,755.39,207.36,8.64;9,81.78,766.35,43.44,8.64" xml:id="b10">
	<monogr>
		<title level="m" type="main" coords="9,81.78,755.39,207.36,8.64;9,81.78,766.35,27.75,8.64">LoRA+: Efficient Low Rank Adaptation of Large Models</title>
		<author>
			<persName coords=""><forename type="first">Soufiane</forename><surname>Hayou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikhil</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,306.14,75.34,219.51,8.64;9,317.05,86.30,209.10,8.64;9,317.05,97.26,209.02,8.64;9,317.05,108.04,174.51,8.82" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03300</idno>
		<title level="m" coords="9,343.07,97.26,183.00,8.64;9,317.05,108.22,32.23,8.64">Measuring massive multitask language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,128.20,218.26,8.64;9,316.69,139.16,209.37,8.64;9,317.05,150.11,209.01,8.64;9,317.05,160.89,207.36,8.82;9,317.05,171.85,75.27,8.59" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<title level="m" coords="9,404.79,150.11,121.27,8.64;9,317.05,161.07,138.92,8.64">Measuring mathematical problem solving with the math dataset</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,192.01,219.52,8.64;9,317.05,202.97,207.36,8.64;9,317.05,213.93,209.10,8.64;9,317.05,224.71,209.01,8.82;9,317.05,235.67,207.36,8.82;9,317.05,246.81,79.43,8.64" xml:id="b13">
	<analytic>
		<title level="a" type="main" coords="9,317.05,224.89,176.86,8.64">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m" coords="9,522.11,224.71,3.95,8.59;9,317.05,235.67,177.39,8.59">ternational conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,306.14,266.79,218.27,8.64;9,316.69,277.74,208.96,8.64;9,317.05,288.70,209.01,8.64;9,317.05,299.48,207.35,8.82;9,317.05,310.44,75.27,8.59" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m" coords="9,430.99,288.70,95.07,8.64;9,317.05,299.66,133.28,8.64">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,330.60,220.01,8.64;9,317.05,341.56,207.35,8.64;9,317.05,352.34,207.99,8.82" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Shima</forename><surname>Imani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harsh</forename><surname>Shrivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05398</idno>
		<title level="m" coords="9,317.05,341.56,207.35,8.64;9,317.05,352.52,65.26,8.64">Mathprompter: Mathematical reasoning using large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,372.50,219.52,8.64;9,317.05,383.46,208.60,8.64;9,316.86,394.42,209.20,8.64;9,317.05,405.38,209.01,8.64;9,317.05,416.15,207.36,8.82;9,317.05,427.11,75.27,8.59" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentina</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.10702</idno>
		<title level="m" coords="9,385.12,405.38,140.94,8.64;9,317.05,416.33,138.95,8.64">Camels in a changing climate: Enhancing lm adaptation with tulu 2</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,447.27,219.51,8.64;9,317.05,458.23,207.35,8.64;9,317.05,469.19,207.36,8.64;9,317.05,479.97,209.01,8.82;9,317.05,490.93,113.44,8.82" xml:id="b17">
	<analytic>
		<title level="a" type="main" coords="9,472.72,458.23,51.69,8.64;9,317.05,469.19,207.36,8.64;9,317.05,480.15,186.63,8.64">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eileen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nassim</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,511.86,479.97,14.20,8.59;9,317.05,490.93,55.12,8.59">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">6421</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,306.14,511.09,218.27,8.64;9,317.05,522.05,207.35,8.64;9,317.05,532.83,209.01,8.82;9,317.05,543.79,209.01,8.59;9,317.05,554.74,209.02,8.59;9,317.05,565.70,207.36,8.59;9,316.74,576.66,199.06,8.82" xml:id="b18">
	<analytic>
		<title level="a" type="main" coords="9,440.96,522.05,83.45,8.64;9,317.05,533.01,172.38,8.64">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName coords=""><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,508.20,532.83,17.87,8.59;9,317.05,543.79,209.01,8.59;9,317.05,554.74,209.02,8.59;9,317.05,565.70,207.36,8.59;9,316.74,576.66,118.89,8.59">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2567" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,306.14,596.82,218.27,8.64;9,316.47,607.78,124.70,8.64;9,461.39,607.78,64.67,8.64;9,317.05,618.56,207.35,8.82;9,317.05,629.52,75.27,8.59" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tijmen</forename><surname>Kopiczko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Asano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.11454</idno>
		<title level="m" coords="9,461.39,607.78,64.67,8.64;9,317.05,618.74,134.27,8.64">Vera: Vectorbased random matrix adaptation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,649.68,220.01,8.64;9,316.74,660.64,207.67,8.64;9,317.05,671.42,166.22,8.82" xml:id="b20">
	<monogr>
		<title level="m" type="main" coords="9,316.74,660.64,207.67,8.64;9,317.05,671.59,23.96,8.64">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,691.57,219.92,8.64;9,317.05,702.53,207.36,8.64;9,317.05,713.49,209.01,8.64;9,317.05,724.27,161.22,8.82" xml:id="b21">
	<analytic>
		<title level="a" type="main" coords="9,469.35,691.57,56.71,8.64;9,317.05,702.53,104.13,8.64">Sherin Muckatira, and Anna Rumshisky</title>
		<author>
			<persName coords=""><forename type="first">Namrata</forename><surname>Vladislav Lialin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shivagunde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.05695</idno>
	</analytic>
	<monogr>
		<title level="m" coords="9,453.96,702.53,70.45,8.64;9,317.05,713.49,209.01,8.64;9,317.05,724.45,19.14,8.64">Stack more layers differently: High-rank training through low-rank updates</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,744.43,218.27,8.64;9,317.05,755.39,207.36,8.64;9,316.69,766.35,207.71,8.64;10,81.78,75.34,209.01,8.64;10,81.78,86.12,207.36,8.82;10,81.78,97.08,75.27,8.59" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Mingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teodor-Dumitru</forename><surname>Ene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathaniel</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rongjian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Himyanshu</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanmitra</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismet</forename><surname>Bayraktaroglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.00176</idno>
		<title level="m" coords="10,203.37,75.34,87.41,8.64;10,81.78,86.30,127.13,8.64">Chipnemo: Domainadapted llms for chip design</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.87,120.33,218.27,8.64;10,81.78,131.29,207.36,8.64;10,81.78,142.25,209.01,8.64;10,81.78,153.03,207.35,8.82;10,81.78,163.99,75.27,8.59" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Shih-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chien-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09353</idno>
		<title level="m" coords="10,231.38,142.25,59.41,8.64;10,81.78,153.21,135.81,8.64">Dora: Weightdecomposed low-rank adaptation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.87,187.24,219.52,8.64;10,81.78,198.20,207.36,8.64;10,81.42,209.16,209.10,8.64;10,81.78,220.12,207.36,8.64;10,81.45,230.90,207.69,8.59;10,81.53,241.86,97.40,8.82" xml:id="b24">
	<analytic>
		<title level="a" type="main" coords="10,187.69,209.16,102.83,8.64;10,81.78,220.12,191.23,8.64">Www&apos;18 open challenge: financial opinion mining and question answering</title>
		<author>
			<persName coords=""><forename type="first">Macedo</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">André</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manel</forename><surname>Zarrouk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,81.45,230.90,207.69,8.59;10,81.53,241.86,17.93,8.59">Companion proceedings of the the web conference 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1941" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,265.11,219.92,8.64;10,81.78,276.07,207.36,8.64;10,81.78,287.03,207.36,8.64;10,81.78,297.81,207.35,8.82;10,81.53,308.76,162.36,8.82" xml:id="b25">
	<analytic>
		<title level="a" type="main" coords="10,249.53,265.11,41.25,8.64;10,81.78,276.07,96.52,8.64;10,215.53,276.07,73.61,8.64;10,81.78,287.03,207.36,8.64;10,81.78,297.98,17.99,8.64">Good debt or bad debt: Detecting semantic orientations in economic texts</title>
		<author>
			<persName coords=""><forename type="first">Pekka</forename><surname>Malo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ankur</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pekka</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,110.93,297.81,178.20,8.59;10,81.53,308.76,93.82,8.59">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="782" to="796" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Jyrki Wallenius, and Pyry Takala</note>
</biblStruct>

<biblStruct coords="10,70.87,332.02,219.52,8.64;10,81.78,342.98,208.60,8.64;10,81.78,353.94,209.10,8.64;10,81.78,364.89,207.36,8.64;10,81.78,375.67,209.10,8.82" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Xiangdi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiyao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16141</idno>
		<title level="m" coords="10,81.78,364.89,207.36,8.64;10,81.78,375.85,66.78,8.64">Periodiclora: Breaking the low-rank bottleneck in lora optimization</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.87,398.93,218.27,8.64;10,81.78,409.89,208.60,8.64;10,81.31,420.84,209.48,8.64;10,81.78,431.80,207.36,8.64;10,81.78,442.58,208.60,8.82;10,81.78,453.72,56.73,8.64" xml:id="b27">
	<analytic>
		<title level="a" type="main" coords="10,214.19,420.84,76.59,8.64;10,81.78,431.80,207.36,8.64;10,81.78,442.76,45.28,8.64">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,134.96,442.58,151.28,8.59">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,476.79,218.27,8.64;10,81.78,487.75,209.01,8.64;10,81.78,498.53,207.36,8.82;10,81.78,509.49,75.27,8.59" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Adithya</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tugrul</forename><surname>Konuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.09578</idno>
		<title level="m" coords="10,151.66,487.75,139.13,8.64;10,81.78,498.71,138.27,8.64">Tied-lora: Enhacing parameter efficiency of lora with weight tying</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.87,532.74,219.92,8.64;10,81.78,543.70,207.36,8.64;10,81.78,554.66,209.10,8.64;10,81.78,565.44,209.01,8.82;10,81.78,576.40,208.60,8.82;10,81.78,587.54,87.00,8.64" xml:id="b29">
	<analytic>
		<title level="a" type="main" coords="10,174.64,543.70,114.49,8.64;10,81.78,554.66,204.76,8.64">Domain adaption of named entity recognition to support credit risk assessment</title>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salinas</forename><surname>Alvarado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,92.87,565.44,197.92,8.59;10,81.78,576.40,122.90,8.59">Proceedings of the Australasian Language Technology Association Workshop</title>
		<meeting>the Australasian Language Technology Association Workshop<address><addrLine>Parramatta, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="84" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,610.61,220.01,8.64;10,81.78,621.39,134.67,8.59" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m" coords="10,158.20,610.61,128.71,8.64">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.87,644.65,218.27,8.64;10,81.78,655.60,207.36,8.64;10,81.78,666.56,209.01,8.64;10,81.78,677.34,207.36,8.82;10,81.78,688.30,110.05,8.59" xml:id="b31">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Shuhua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minghui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haizhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18039</idno>
		<title level="m" coords="10,234.45,666.56,56.33,8.64;10,81.78,677.52,174.85,8.64">Reslora: Identity residual mapping in low-rank adaption</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.87,711.55,218.27,8.64;10,81.78,722.51,209.10,8.64;10,81.78,733.29,209.01,8.82;10,81.47,744.25,207.67,8.59;10,81.45,755.21,207.69,8.82;10,81.78,766.35,76.10,8.64" xml:id="b32">
	<analytic>
		<title level="a" type="main" coords="10,249.51,711.55,39.62,8.64;10,81.78,722.51,205.54,8.64">Impact of news on the commodity market: Dataset and results</title>
		<author>
			<persName coords=""><forename type="first">Ankur</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tanmay</forename><surname>Khandait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,94.56,733.29,196.23,8.59;10,81.47,744.25,207.67,8.59;10,81.45,755.21,138.21,8.59">Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="589" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,75.34,219.52,8.64;10,316.58,86.30,209.48,8.64;10,317.05,97.26,209.10,8.64;10,316.72,108.04,121.59,8.82" xml:id="b33">
	<analytic>
		<title level="a" type="main" coords="10,463.69,86.30,62.37,8.64;10,317.05,97.26,204.39,8.64">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName coords=""><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Murtadha</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,316.72,108.04,64.41,8.59">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,128.14,218.52,8.64;10,317.05,139.10,209.10,8.64;10,317.05,150.06,207.36,8.64;10,317.05,160.84,207.36,8.82;10,317.05,171.80,75.27,8.59" xml:id="b34">
	<monogr>
		<title level="m" type="main" coords="10,317.05,150.06,207.36,8.64;10,317.05,161.02,140.63,8.64">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,306.14,191.90,218.52,8.64;10,317.05,202.86,209.01,8.64;10,317.05,213.82,208.60,8.64;10,317.05,224.78,207.36,8.64;10,317.05,235.56,209.01,8.82;10,317.05,246.52,208.60,8.59;10,317.05,257.66,12.45,8.64;10,306.14,277.58,219.51,8.64;10,317.05,288.54,209.01,8.64;10,317.05,299.50,209.10,8.64;10,317.05,310.46,209.10,8.64;10,317.05,321.24,134.67,8.59" xml:id="b35">
	<analytic>
		<title level="a" type="main" coords="10,367.38,224.78,157.03,8.64;10,317.05,235.56,209.01,8.82;10,317.05,246.52,204.26,8.59">How far can camels go? exploring the state of instruction tuning on open resources. Advances in Neural Information Processing Systems</title>
		<author>
			<persName coords=""><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kelsey</forename><surname>Macmillan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17564</idno>
	</analytic>
	<monogr>
		<title level="m" coords="10,317.05,310.46,205.14,8.64">Bloomberggpt: A large language model for finance</title>
		<editor>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Steven</forename><surname>Lu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vadim</forename><surname>Dabravolski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Prabhanjan</forename><surname>Kambadur</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Rosenberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2024. 2023</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,306.14,341.34,219.52,8.64;10,317.05,352.30,209.01,8.64;10,317.05,363.26,209.10,8.64;10,317.05,374.22,209.01,8.64;10,317.05,385.00,207.35,8.82;10,317.05,395.96,75.27,8.59" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Longhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weisen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jincheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhengying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.12284</idno>
		<title level="m" coords="10,317.05,374.22,209.01,8.64;10,317.05,385.18,133.28,8.64">Metamath: Bootstrap your own mathematical questions for large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,306.14,416.06,218.27,8.64;10,317.05,426.84,207.36,8.82;10,316.74,437.80,95.29,8.82" xml:id="b37">
	<analytic>
		<title level="a" type="main" coords="10,454.81,416.06,69.60,8.64;10,317.05,427.02,74.37,8.64">Root mean square layer normalization</title>
		<author>
			<persName coords=""><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,399.10,426.84,125.31,8.59;10,316.74,437.80,76.09,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,457.91,219.52,8.64;10,316.86,468.86,207.55,8.64;10,316.47,479.82,209.59,8.64;10,317.05,490.60,207.36,8.82;10,316.80,501.56,48.98,8.82" xml:id="b38">
	<analytic>
		<title level="a" type="main" coords="10,411.36,479.82,114.70,8.64;10,317.05,490.78,18.29,8.64">Lima: Less is more for alignment</title>
		<author>
			<persName coords=""><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Puxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,344.73,490.60,179.68,8.59;10,316.80,501.56,29.78,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,521.67,219.52,8.64;10,317.05,532.62,209.01,8.64;10,317.05,543.58,207.36,8.64;10,316.47,554.54,208.29,8.64;10,317.05,565.32,207.36,8.82;10,317.05,576.28,110.05,8.59" xml:id="b39">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jiacheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristjan</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kimia</forename><surname>Nadjahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haitz</forename><surname>Sáez De Ocáriz Borde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rickard</forename><surname>Brüel Gabrielsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mikhail</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16842</idno>
		<title level="m" coords="10,476.78,554.54,47.98,8.64;10,317.05,565.50,174.24,8.64">Asymmetry in low-rank adapters of foundation models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
