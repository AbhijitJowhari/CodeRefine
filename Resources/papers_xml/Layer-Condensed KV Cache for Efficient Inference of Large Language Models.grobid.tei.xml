<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coords="1,132.93,79.85,329.42,12.90;1,223.19,95.79,148.90,12.90">Layer-Condensed KV Cache for Efficient Inference of Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-17">17 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.97,131.60,51.55,10.75"><forename type="first">Haoyi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">ShanghaiTech University Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.74,131.60,48.08,10.75"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">ShanghaiTech University Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coords="1,132.93,79.85,329.42,12.90;1,223.19,95.79,148.90,12.90">Layer-Condensed KV Cache for Efficient Inference of Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-17">17 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">1BF48F674A82FF9F36DDB56499CCBD88</idno>
					<idno type="arXiv">arXiv:2405.10637v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-11T09:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Huge memory consumption has been a major bottleneck for deploying high-throughput large language models in real-world applications. In addition to the large number of parameters, the key-value (KV) cache for the attention mechanism in the transformer architecture consumes a significant amount of memory, especially when the number of layers is large for deep language models. In this paper, we propose a novel method that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput. Our experiments on large language models show that our method achieves up to 26× higher throughput than standard transformers and competitive performance in language modeling and downstream tasks. In addition, our method is orthogonal to existing transformer memory-saving techniques, so it is straightforward to integrate them with our model, achieving further improvement in inference efficiency. Our code is available at <ref type="url" coords="1,87.87,510.86,147.03,8.00" target="https://github.com/whyNLP/LCKV">https://github.com/whyNLP/LCKV</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>High throughput and low latency are essential for deploying large language models (LLMs) in realworld applications <ref type="bibr" coords="1,153.76,584.01,80.92,9.46" target="#b28">(Tillet et al., 2019;</ref><ref type="bibr" coords="1,237.41,584.01,53.09,9.46;1,70.87,597.56,23.95,9.46" target="#b14">Kwon et al., 2023)</ref>. However, the huge memory consumption of LLMs has been a major bottleneck, preventing a large batch size and high throughput generation. Among the memory-consuming components, the key-value (KV) cache is one of the most significant parts <ref type="bibr" coords="1,95.94,665.30,83.28,9.46" target="#b21">(Pope et al., 2023;</ref><ref type="bibr" coords="1,182.65,665.30,86.35,9.46">Zhang et al., 2023)</ref> that takes over 30% of the GPU memory during deployment <ref type="bibr" coords="1,96.14,692.40,85.41,9.46" target="#b14">(Kwon et al., 2023)</ref>. The KV cache is used to store the keys and values in each transformer layer during generation to avoid re-computation. Its memory consumption is proportional to both the sequence length and the number of layers. There have been substantial works on reducing the memory consumption of the KV cache in LLMs. Most of them focus on compressing the KV cache by reducing the length of the cached KV sequence. For example, <ref type="bibr" coords="1,366.50,492.28,82.54,9.46">Jiang et al. (2023a)</ref>; <ref type="bibr" coords="1,456.42,492.28,64.27,9.46" target="#b15">Li et al. (2023)</ref>; <ref type="bibr" coords="1,306.14,505.83,76.78,9.46" target="#b20">Mu et al. (2023)</ref> compress the prompts to save the memory consumption. <ref type="bibr" coords="1,424.30,519.38,73.63,9.46" target="#b24">Ren et al. (2023)</ref> incrementally compress a specified span of tokens into compact ones to reduce the KV cache length. <ref type="bibr" coords="1,503.60,546.48,20.80,9.46;1,306.14,560.02,54.51,9.46" target="#b32">Xiao et al. (2024)</ref>; <ref type="bibr" coords="1,369.11,560.02,78.41,9.46" target="#b10">Han et al. (2023)</ref> propose to store only the KVs of initial and recent tokens in the KV cache. <ref type="bibr" coords="1,337.16,587.12,83.80,9.46">Zhang et al. (2023)</ref> propose a dynamic KV cache eviction policy to only keep a small portion of the KV cache in memory.</p><p>In this paper, we propose to reduce the memory consumption of the KV cache from a novel perspective that is orthogonal to previous efforts: reducing the number of layers. Specifically, we present a new variant of transformer decoders in which queries of all layers are paired with keys and values of just the top layer, as illustrated in Figure <ref type="figure" coords="1,518.30,711.54,4.01,9.46" target="#fig_0">1</ref>. In this way, we only need to cache the keys and values of one layer (vs. tens of layers in a typical LLM), significantly saving memory consumption while introducing no computation overheads dur-ing inference. In fact, since we only need the keys and values of the top layer, we can omit the keyvalue computation and discard the key-value parameters for all the other layers, further improving the throughput and reducing the memory consumption as well as the mode size.</p><p>We draw our inspiration from the interpretation of the stacking layer structure of a transformer as an iterative process of improving token representation <ref type="bibr" coords="2,99.81,196.67,84.66,9.46" target="#b31">(Wu and Tu, 2023)</ref>. In this interpretation, the representation at the top layer is the most informative, so it makes sense to attend only to the top layer. We also note the similarity of our idea to the cross-attention mechanism in a standard transformer encoder-decoder, in which all the decoder layers attend to the top encoder layer. However, applying this idea to a decoder has never been attempted before as far as we know.</p><p>Although our model presented above achieves very efficient inference, its performance in language modeling and downstream tasks degrades in comparison with standard transformers. Therefore, we further propose to retain standard attention for a small number of layers in our model, which slightly diminishes our saving of the KV cache memory consumption but leads to almost no performance degradation.</p><p>Another challenge faced by our model is training. When training a standard transformer decoder, the computation of all the tokens can be fully parallelized. In our model, however, the computation at each token depends on the top layer of the previous token, creating sequential dependencies that spoil parallel training. We address the challenge by deriving an approximate training method that supports parallel training.</p><p>Our experiments on Llama <ref type="bibr" coords="2,197.72,562.49,87.16,9.46">(Touvron et al., 2023</ref>) show that our model achieves up to 32× larger batch sizes and up to 26× higher throughput than standard transformers for LLMs of 1B-30B parameters; at the same time, our model has competitive performance to standard transformers in language modeling and downstream tasks. We further empirically demonstrate that it is straightforward to integrate our model with other memory-saving techniques like StreamingLLM <ref type="bibr" coords="2,208.71,684.44,77.14,9.46" target="#b32">(Xiao et al., 2024)</ref>, achieving further improvements in inference efficiency.</p><p>We summarize our contributions as follows: 1) we propose a new variant of transformer decoders that reduces the memory consumption of the KV cache by dramatically reducing the number of cached layers; 2) we make parallel training of our model feasible by designing a novel approximate training method; 3) we conduct extensive experiments to verify and analyze the effectiveness and efficiency of our method.</p><p>2 Layer-Condensed KV Cache</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model</head><p>As shown in Figure <ref type="figure" coords="2,399.26,195.54,4.26,9.46" target="#fig_0">1</ref>(b), we pair the queries of all layers with KVs of only the top layer, so that we do not have to cache or even compute KVs for layers other than the top layer, saving both memory consumption and computation. Furthermore, since we no longer need to compute KVs for these layers, nor do we need to keep the weights W K , W V that map hidden representations to KVs for these layers, thus also saving model parameters.</p><p>One problem with this method is that, since each token also attends to itself, we need its top-layer KVs for its attention computation at lower layers, but the top-layer cannot be computed until we finish the computation of all the lower layers. A straightforward solution to this cyclic dependency problem is to drop the attention of each token to itself, which is equivalent to masking the diagonal of the attention matrix. Now the first token of the sequence has nothing to attend to, so we just use zero vectors as dummy KVs in its attention computation. Note that even without self-attention of each token, its information can still be incorporated in its bottom-up computation thanks to residual connections. Empirically, we find that the diagonal mask of the attention matrix does not affect the performance of the model.</p><p>It has been previously observed that transformers tend to attend to syntactic information in lower layers and semantic information in higher layers <ref type="bibr" coords="2,305.78,589.59,91.25,9.46">(Clark et al., 2019b)</ref>. Intuitively, applying KVs of the same layer to queries of all layers might break this pattern. Empirically, we do find that our method as described above underperforms standard transformers in language modeling and downstream tasks. A simple yet effective solution to this problem is to retain standard attention for a small number of layers, which we call warmup layers, and only apply our method to the rest of the layers. Inspired by the sandwich module of <ref type="bibr" coords="2,482.28,711.54,44.04,9.46;2,305.78,725.09,26.94,9.46" target="#b23">Reid et al. (2021)</ref>, we propose to keep the top w/2 layers and the bottom w/2 layers as warmup layers. We empirically find that such a sandwich configuration outperforms alternative configurations and leads to almost no performance degradation compared with standard transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>Though the inference process of our model is straightforward and almost identical to that of a standard transformer, i.e., decoding one token at a time from left to right, the training process of our model is more complicated. Since each token relies on the KVs of the top layer of previous tokens, it is impossible to train on a sequence of tokens in parallel as in a standard transformer. Below we derive a parallel training process of our model in three steps. For simplicity, we assume no warmup layers (i.e., w = 0). It is straightforward to add warmup layers in the derived training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">From Sequential to Parallel Training</head><p>During training, we minimize the cross entropy loss for each token. In our model, the computation of each token depends on the top-layer KVs of its previous tokens. Therefore, for a token sequence of length n, training is done sequentially on a computation graph consisting of n passes of bottom-up transformer computation, as shown in Figure <ref type="figure" coords="3,269.66,394.25,16.23,9.46" target="#fig_2">2(a)</ref>.</p><p>In the following, we present a different training computation graph and show its equivalence to the original computation graph. Specifically, we perform n iterations of bottom-up transformer computation on all tokens in parallel, and in each iteration, we pair the queries of all layers with KVs of the top layer from the previous iteration, thus breaking the sequential dependencies within the iteration. We compute the cross entropy loss only after the last iteration. Note that the first iteration has no previous iteration, and we pair its queries with dummy KVs which are zero vectors. We leave the proof of the theorem to Appendix A. Here, we provide an intuitive explanation. We say the computation of a token (w.r.t. its hidden representations and top-layer KVs) is correct in an iteration of the second computation graph if and only if it is identical to that in the first computation graph. Since we have masked the diagonal of the attention matrix, the computation of the first token does not rely on any key or value (except for dummy zero vectors) and thus is correct in every iteration. For the second token, its computation relies on the KVs of the first token and thus is correct starting from the second iteration. In general, the computation of the i-th token relies on the KVs of the first i -1 tokens and by induction, it is correct starting from the i-th iteration. Therefore, after n iterations, all the tokens are correctly computed. As a result, the computation sub-graphs of the crossentropy losses of all the tokens are identical in the two graphs, and hence training processes following the two graphs are equivalent. Essentially, the second computation graph replaces horizontal dependencies in the first graph with vertical dependencies, and it does not change the length of the longest dependency chain. Consequently, although the second computation graph supports parallel training over all the tokens, it still requires the same n iterations as the first graph. Next, we will trim the iterations first in terms of backpropagation and then in terms of forward propagation.</p><formula xml:id="formula_0" coords="3,320.51,74.63,179.91,159.07">𝑥𝑥 1 𝑥𝑥 2 𝑥𝑥 3 𝐿𝐿 1 𝐿𝐿 2 𝐿𝐿 3 (a) 𝑥𝑥 1 𝑥𝑥 2 𝑥𝑥 3 𝐿𝐿 1 𝐿𝐿 2 𝐿𝐿 3 (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Backpropagation: Gradient Stopping</head><p>We compute the cross entropy loss after the last iteration, which backpropagates through n iterations, resulting in a large computation graph that is impossible to fit into GPU memory for large n.</p><p>To solve the issue, we follow the practice of gradient stopping in Transformer-XL <ref type="bibr" coords="3,450.13,697.01,75.02,9.46" target="#b5">(Dai et al., 2019)</ref> and backpropagate the loss only through the last b iterations (b ≪ n).</p><p>Notice that the KVs used in the last iteration come from the second-to-last iteration, so if we set b = 1, then backpropagation would not reach the KVs and hence the model parameters used to calculate the KVs are not trained at all, which would result in a large performance degradation. We empirically find that with b ≥ 2, the performance of our model is comparable with that of a standard transformer. To reduce memory consumption, we set b = 2 by default, which means we only backpropagate through two iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Forward Propagation: Fast Convergence of KV</head><p>With gradient stopping, the first n -b iterations are solely used in forward propagation to compute KVs that are fed into the last b iterations. When n is large, it is still too costly to run n-b iterations of forward propagation. Fortunately, we observe that the values of KVs converge very fast over iterations and hence we do not have to run n -b iterations to obtain the final KVs. Figure <ref type="figure" coords="4,205.72,506.36,5.37,9.46" target="#fig_3">3</ref> shows the convergence of KVs of a randomly initialized model with the same configuration as TinyLlama <ref type="bibr" coords="4,233.13,533.46,57.36,9.46;4,70.87,547.01,23.95,9.46">(Zhang et al., 2024)</ref>. The input is a randomly picked segment of text from the MiniPile (Kaddour, 2023) dataset with 2048 tokens (i.e., n = 2048). We measure the change of KVs over consecutive iterations using the mean squared error. As can be seen, KVs converge after only a few tens of iterations, with more warmup layers leading to faster convergence. Therefore, we use m iterations (m ≪ n) to approximate the KVs of n -b iterations. We empirically find that m = 7 is sufficient for model training and larger values of m do not further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference with Prompts</head><p>It is straightforward to employ our model for autoregressive generation. However, our model cannot do parallel encoding of prompts like standard transformers for the same reason as it cannot do parallel training. Fortunately, since iterative computation of KVs is fast to converge, we can just perform iterative computation for the prompts for m + b iterations. Typically, m + b is far less than the number of tokens to generate, and thus the extra time spent in encoding the prompts is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We empirically verify the effectiveness of our method on the Llama model <ref type="bibr" coords="4,429.97,218.82,91.74,9.46">(Touvron et al., 2023)</ref>.</p><p>We show that our method achieves significant memory reduction and throughput improvement as well as competitive performance in language modeling and downstream tasks compared with standard transformers. We also show that our method could effectively integrate with other memory-saving techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generation Throughput</head><p>We test our method with 1.1B, 7B, and 30B parameters on an NVIDIA GeForce RTX 3090 (24GB) GPU and an NVIDIA A100 (80GB) GPU respectively. The 1.1B model configuration follows that of TinyLlama <ref type="bibr" coords="4,373.74,411.91,93.87,9.46">(Zhang et al., 2024)</ref> and the 7B and 30B model configuration follows that of the original Llama <ref type="bibr" coords="4,378.42,439.01,100.59,9.46">(Touvron et al., 2023)</ref>. We set m = 7, b = 2 and w = {2, 10}. Our implementation is based on HuggingFace Transformers <ref type="bibr" coords="4,499.26,466.11,25.15,9.46;4,306.14,479.66,51.40,9.46" target="#b30">(Wolf et al., 2020)</ref> with kernel replacement with FlashAttention 2 <ref type="bibr" coords="4,351.00,493.21,53.16,9.46" target="#b6">(Dao, 2023)</ref>, fused RMS norm, fused cross-entropy and fused SwiGLU. Following FlexGen <ref type="bibr" coords="4,401.89,521.08,81.09,9.46" target="#b26">(Sheng et al., 2023)</ref>, the evaluation is conducted in an end-to-end fashion. For a prompt of length s, we let the model generate output sequences of length n with batch size b. The latency t is defined as the total time in seconds spent in processing the prompts and generating all the bn tokens. The generation throughput is defined as bn/t tokens per second.</p><p>Table <ref type="table" coords="4,343.60,630.24,5.42,9.46" target="#tab_0">1</ref> compares the maximum batch sizes and throughput of standard Llama models and our models on the two types of GPUs. Note that some of the sequence lengths exceed the maximum input lengths that the models are trained on, but that does not affect batch size and throughput measurement. We still benchmark the batch sizes and throughput to show the potential of our method on other models allowing larger sequence lengths. It can be seen that our method achieves significantly larger batch sizes and higher throughput on all of the settings. Notice that the maximum throughput is not necessarily achieved using the maximum batch size. Figure <ref type="figure" coords="5,101.05,630.24,5.35,9.46">4</ref> shows the throughput of 7B Llama and our model on an A100 GPU w.r.t. the batch size. The prompt length and generation length are both 2048. We find that when the batch size grows larger than 32, the throughput no longer increases and even decreases with a batch size of 128. This may indicate that the model operations are turning from memory-bound into compute-bound <ref type="bibr" coords="5,238.63,725.09,51.86,9.46;5,70.87,738.63,25.45,9.46" target="#b7">(Dao et al., 2022)</ref> and the throughput is limited by the computation power. Further improvement in throughput may require more efficient computation kernels</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev ppl.  instead of larger batch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Performance</head><p>To evaluate the performance of our model in language modeling and downstream tasks, we pretrain from scratch two 1.1B models with m = 7, b = 2 and w = {2, 10}. We use TinyLlama as our baseline, whose size is also 1.1B. We pre-train the models on a 100B subset of the SlimPajama dataset <ref type="bibr" coords="5,340.69,670.89,101.13,9.46" target="#b27">(Soboleva et al., 2023)</ref>. The training details are consistent with those of TinyLlama <ref type="bibr" coords="5,493.53,684.44,30.88,9.46;5,306.14,697.99,50.48,9.46">(Zhang et al., 2024)</ref>. All models are trained with AdamW <ref type="bibr" coords="5,305.78,711.54,135.08,9.46" target="#b17">(Loshchilov and Hutter, 2019)</ref> with β 1 = 0.9 and β 2 = 0.95. The batch size is 2M tokens. We use a cosine learning rate schedule with a maximum learning rate of 4.0 × 10 -4 and a warmup of 200 steps. The final learning rate is 4.0 × 10 -5 . We  During evaluation, we perform inference in the standard left-to-right fashion instead of using the method of Section 2.3. We report the perplexity on a 10M subset of the development set of SlimPajama. We also test the zero-shot performance on commonsense reasoning tasks following Zhang et al. ( <ref type="formula" coords="6,130.64,395.36,18.41,9.46">2024</ref>), including Hellaswag <ref type="bibr" coords="6,254.76,395.36,34.37,9.46;6,70.87,408.90,48.37,9.46">(Zellers et al., 2019)</ref>, OpenBookQA <ref type="bibr" coords="6,190.26,408.90,95.64,9.46" target="#b19">(Mihaylov et al., 2018)</ref>, WinoGrande <ref type="bibr" coords="6,130.07,422.45,103.75,9.46" target="#b25">(Sakaguchi et al., 2021)</ref>, ARC-Easy and ARC-Challenge <ref type="bibr" coords="6,165.90,436.00,84.91,9.46" target="#b4">(Clark et al., 2018)</ref>, BoolQ <ref type="bibr" coords="6,70.51,449.55,85.43,9.46">(Clark et al., 2019a)</ref>, and PIQA <ref type="bibr" coords="6,209.96,449.55,76.39,9.46" target="#b1">(Bisk et al., 2020)</ref>.</p><p>The tests are conducted using the lm-eval-harness framework <ref type="bibr" coords="6,121.68,476.65,75.52,9.46" target="#b9">(Gao et al., 2023)</ref>. For these tasks, we encode the prompts with the same number of iterations (m + b = 9) as in training.</p><p>Table <ref type="table" coords="6,107.80,533.12,5.35,9.46" target="#tab_1">2</ref> and<ref type="table" coords="6,133.68,533.12,5.35,9.46" target="#tab_2">3</ref> show the results. The performance of our models is comparable to that of TinyLlama. In particular, our model with w = 10 has almost no performance degradation, while achieving significantly higher generation throughput as evaluated in Section 3.1. Our model with w = 2 has a small but noticeable decrease in performance for most of the tasks, but it achieves even higher increase in throughput.</p><p>Despite the competitive performance and higher inference efficiency of our models, we note that pre-training our model costs about 3 times the time of pre-training TinyLlama with the same amount of data due to the iterative training process. Nevertheless, we believe that in most scenarios, a speedup in inference is worth a slowdown in training which is a one-time process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integration with StreamingLLM</head><p>We have previously mentioned that our method is orthogonal to other memory-saving techniques and can be easily integrated with them. Here we integrate our method with StreamingLLM <ref type="bibr" coords="6,474.70,534.73,51.08,9.46;6,306.14,548.28,23.95,9.46" target="#b32">(Xiao et al., 2024)</ref>. StreamingLLM employs an attention sink that only preserves the KV cache of the first few tokens (four by default) and recent tokens, which empowers LLMs to process infinite-length inputs. As shown in Figure <ref type="figure" coords="6,423.45,602.81,4.17,9.46" target="#fig_5">5</ref>, the integration of StreamingLLM and our model (w = 10) achieves lower latency and memory consumption compared to the original StreamingLLM on different cache sizes (numbers of cached tokens).</p><p>We further showcase that integration with our method does not hinder the ability of StreamingLLM to process infinite-length tokens. Specifically, we integrate StreamingLLM into our model (w = 10) trained in Section 3.2 with different cache sizes and find that the integrated model achieves even lower perplexities than StreamingLLM as shown in Figure <ref type="figure" coords="6,463.92,765.73,4.15,9.46" target="#fig_7">6</ref>. We also let the model handle inputs with a sequence length of four million tokens. As shown in Figure <ref type="figure" coords="7,264.40,404.44,4.17,9.46">7</ref>, the integrated model can effectively process the input with the perplexity remaining stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analyses</head><p>In this section, we empirically analyze design choices in our method. For experiment details, please refer to Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Sandwich Configuration</head><p>In Section 2.1, we propose to add some warmup layers to improve model performance. Note that inference efficiency is determined by the number of warmup layers w and not by their placement. Here we ask the following question: with the number of warmup layers fixed, where should we place the warmup layers to achieve the best performance?</p><p>Table <ref type="table" coords="7,110.24,657.34,5.56,9.46">4</ref> shows the model performance in language modeling with two warmup layers (i.e., w = 2) that are placed at the bottom, at the top, and with the default sandwich configuration. It can be seen that the sandwich style performs the best. A possible explanation is that top and bottom layers serve different functionalities (e.g., semantic vs. syntactic) and it makes more sense to warm up both than just one of them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Number of Warmup Layers</head><p>Warmup layers serve as a bridge between the standard transformer and our model. The more warmup layers we keep, the more similar it is to the standard transformer, and the less memory we save. In this section, we ask the following question: how does the number of warmup layers w affect the model performance and throughput?</p><p>We test the model with 1.1B (22 layers) parameters and different numbers of warmup layers (Figure <ref type="figure" coords="7,323.37,464.56,4.02,9.46" target="#fig_8">8</ref>). Surprisingly, we find that the log dev perplexity does not monotonically decrease with the number of warmup layers. Without warmup layers (the red region), the model performance is significantly worse than that of the standard transformer. With only a few warmup layers (the yellow region), the model performance is greatly improved and close to that of the standard transformer, but its throughput is decreased at the same time. With more warmup layers (the green region), the model even outperforms the standard transformer, while suffering from further decrease in throughput.</p><p>The results point to a trade-off between model performance and throughput that is controllable by the number of warmup layers. If pursuing a high throughput, one could keep only a few warmup layers (the yellow region). If good performance is crucial, one could keep more warmup layers (the left part of the green region).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Convergence of KV</head><p>In Section 2.2.3, we have shown that KVs converge very fast for a random model and hence we use We measure the convergence of KVs of a 50M Llama (Figure <ref type="figure" coords="8,141.81,435.81,4.17,9.46" target="#fig_9">9</ref>, bottom) and the 1.1B model (w = 2) pre-trained in Section 3.2 (Figure <ref type="figure" coords="8,273.43,449.35,8.81,9.46" target="#fig_10">10</ref>). It can be seen that while a random model requires 15-20 iterations to converge, a trained model requires far fewer iterations. This hints at a small value of m especially during late stages of training. We then evaluate model performance when trained with different values of m (Figure <ref type="figure" coords="8,226.86,530.65,4.17,9.46" target="#fig_9">9</ref>, top). It can be seen that the model performance converges with m ≥ 6, with more warmup layers leading to faster convergence. This justifies our default choice of m = 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Extensive research has been done on reducing KV cache for efficient inference of LLMs. With the exception of vLLM <ref type="bibr" coords="8,156.54,657.34,80.84,9.46" target="#b14">(Kwon et al., 2023)</ref>, which proposes paged attention to reduce memory fragmentation of the KV cache from a system perspective, most recent works focus on compressing the KV cache by reducing the length of the cached KV sequence. <ref type="bibr" coords="8,117.85,725.09,93.51,9.46">Jiang et al. (2023a,b)</ref> accelerate model inference by compressing document-level prompts into short prompts. <ref type="bibr" coords="8,164.49,752.18,70.17,9.46" target="#b15">Li et al. (2023)</ref>   <ref type="bibr" coords="8,305.78,457.28,29.64,9.46">(2023)</ref> propose a KV cache eviction policy based on the summation of attention scores to only keep a small portion of the KV cache in memory. Unlike these previous methods, our method reduces the memory consumption of the KV cache by reducing the number of layers, which is orthogonal to these methods and can potentially be combined with these methods to further reduce the KV cache and improve inference efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel method to reduce the memory consumption and improve the throughput of LLMs by reducing the number of layers whose keys and values need to be computed and cached. We empirically show that our method achieves significant memory reduction and throughput improvement with negligible performance degradation. We also show that our method could effectively integrate with other memory-saving techniques like StreamingLLM. Future work includes designing more efficient training approaches, developing large-batch-friendly kernels, and verifying our method on larger and more complex LLMs. We hope that our work could provide a new perspective for improving inference efficiency of LLMs and inspire more research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Though our method achieves impressive memory reduction and throughput improvement, we would like to point out its limitations from the following aspects:</p><p>• Due to the iterative training, our model requires about 3× the time to pre-train a model with the same amount of data. In other words, our method improves the inference efficiency at the cost of the training efficiency. A potential remedy is that if one has a pre-trained model, one could use it to initialize our model, which is empirically found to speed up the process of training.</p><p>• Since our method requires iteratively processing the prompts, the throughput degrades when the prompts are much longer than the generation length, e.g., in document summarization. Generally, our method is more suitable for tasks with a large generation length, such as translation, dialogue, question answering, CoT problem solving, etc.</p><p>International Conference on Learning Representations.</p><p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, Florence, Italy. Association for Computational Linguistics.</p><p>Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385.</p><p>Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. 2023. H2o: Heavy-hitter oracle for efficient generative inference of large language models. In Thirty-seventh Conference on Neural Information Processing Systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of The Training Theorem</head><p>Here we formally prove Theorem 1 in Section 2.2. Remember that we need to compute the loss for each token sequentially, but we propose to train all the tokens in parallel with n iterations.</p><p>Theorem 1. The two computation graphs are equivalent in terms of model training.</p><p>To prove the theorem, we first introduce the following lemma.</p><p>Lemma 1. In the first computation graph, denote the final hidden representation of the i-th token as h i = f 1,i (x 1 , x 2 , . . . , x i ). In the second computation graph, denote the final hidden representation of the i-th token in iteration t as h</p><formula xml:id="formula_1" coords="11,70.87,510.48,220.18,31.98">(t) i = f (t) 2,i (x 1 , x 2 , . . . , x i ). We have f 1,i = f (t) 2,i , ∀t ≥ i,</formula><p>no matter what the initial KVs are.</p><p>Proof. In the first computation graph, denote the KVs of the i-th token as KV i and that under parallel training with iteration t as KV (t) i . Since the basic networks are the same for the two computation graphs, we further denote g i as h i = f 1,i (x 1 , x 2 , . . . , x i ) = g i (x 1 , x 2 , . . . , x i , KV 1 , KV 2 , . . . , KV i-1 ), as well as h</p><formula xml:id="formula_2" coords="11,70.87,661.04,218.27,33.39">(t) i = f (t) 2,i (x 1 , x 2 , . . . , x i ) = g i (x 1 , x 2 , . . . , x i , KV (t-1) 1 , . . . , KV (t-1)</formula><p>i-1 ). The only difference in the computation of the final hidden representations lies in the KVs.</p><p>We prove the lemma by induction. For the base case, since we have removed the diagonal of the attention matrix, h 1 and h (t) 1 does not rely on any key or value. So we have h</p><formula xml:id="formula_3" coords="11,175.68,71.40,301.86,704.62">1 = f 1,1 (x 1 ) = g i (x 1 ) = f (t) 2,1 (x 1 ) = h (t) 1 , ∀t. That is f 1,1 = f (t)</formula><p>2,1 , ∀t. Since the computation graphs of the first token are the same, KV 1 and KV (t) 1 are the same for all t. For the inductive step, we assume that ∀i ≤</p><formula xml:id="formula_4" coords="11,306.14,128.67,169.18,16.00">T, f 1,i = f (T ) 2,i = g i , KV i = KV (T )</formula><p>i . Then for iteration T + 1: ∀i ≤ T + 1, the computation of the i-th token relies on the KVs of the first i -1 tokens. That is, h</p><formula xml:id="formula_5" coords="11,306.14,182.87,218.65,104.45">(T +1) i = f (T +1) 2,i (x 1 , x 2 , . . . , x i ) = g i (x 1 , x 2 , . . . , x i , KV (T ) 1 , . . . , KV (T ) i-1 ). Since i ≤ T + 1, we have i -1 ≤ T . By induction we have KV (T ) i-1 = KV i-1 . Thus, we have g i (x 1 , x 2 , . . . , x i , KV (T ) 1 , . . . , KV (T ) i-1 ) = g i (x 1 , x 2 , . . . , x i , KV 1 , . . . , KV i-1 ) = f 1,i (x 1 , x 2 , . . . , x i ) = h i .</formula><p>The computation graphs are the same for the first T + 1 tokens, then we have</p><formula xml:id="formula_6" coords="11,316.71,301.58,118.01,30.36">KV i = KV (T +1) i . Thus, we have f 1,i = f (t)</formula><p>2,i , ∀i ≤ t. The proof does not rely on initial KVs, so the conclusion holds for any initial KVs.</p><p>Let t = n, we have the entire computation graphs are equivalent. Therefore, we have proved Theorem 1.</p><p>From the lemma, we could learn one more thing: The parallel training has the same computation graph as the sequential training when the iteration t ≥ n. This indicates that the parallel training is theoretically guaranteed to converge to the same solution as the sequential training. Once it is converged, it will not diverge. Our work finds that the KVs converge much faster than the theoretical n iterations, which significantly reduces the training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model and Training Details</head><p>We provide the model configurations and training details in Table <ref type="table" coords="11,399.80,589.59,5.56,9.46" target="#tab_5">5</ref> and<ref type="table" coords="11,430.07,589.59,4.17,9.46" target="#tab_6">6</ref>. The 7B and 30B model configurations are consistent with those of the original Llama <ref type="bibr" coords="11,395.32,616.69,99.27,9.46">(Touvron et al., 2023)</ref>. The 1.1B model configuration follows that of <ref type="bibr" coords="11,495.24,630.24,25.82,9.46;11,306.14,643.79,112.45,9.46">TinyLlama (Zhang et al., 2024)</ref>. We use the WikiText-103 <ref type="bibr" coords="11,324.96,657.34,90.42,9.46" target="#b18">(Merity et al., 2017)</ref> (licensed under CC-BY-SA 3.0), MiniPile <ref type="bibr" coords="11,384.96,670.89,71.29,9.46" target="#b13">(Kaddour, 2023)</ref> (licensed under MIT) and SlimPajama <ref type="bibr" coords="11,405.36,684.44,97.63,9.46" target="#b27">(Soboleva et al., 2023)</ref> (various licenses depending on the data source) as our datasets. Our use of the datasets is consistent with their intended use.</p><p>The training of TinyLlama in Section 3 takes 14:42:59, while our models take 1 day, 16:44:16 (2.77x, w = 2) and 1 day, 15:52:38 (2.71x, w =  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Analyses</head><p>In this section, we provide more analyses beyond those in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Initialize with Pre-trained Models</head><p>Since our model structure resembles that of the standard transformer, we could initialize our model with pre-trained models. For W K , W V of the middle layers, we just ignore the parameters. Experiments show that initialization with pre-trained models could effectively speed up the training pro-cess (Table <ref type="table" coords="12,358.70,560.50,4.02,9.46">7</ref>).  Experiments on a 1.1B model (Table <ref type="table" coords="13,253.38,514.73,4.63,9.46" target="#tab_8">8</ref>) show that the perplexity increases with b and the training time also increases. From the training curve, we find that larger b leads to more unstable training, thus the model is harder to converge. Therefore, we set b = 2 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 KV Loss for Less Iterations</head><p>In Section 4.3 we have shown that the KVs converge very fast for a trained model, yet we still want to make it converge even faster, saving both  training and inference time costs. An intuitive idea is to add an MSE loss to the KVs before and after the last iteration to force the KVs to converge. We call this term the "KV Loss". Our experiments (Table 9) show that for small data with small w, the KV loss could lead to better performance. While for large data or large w, the KV loss hurts the performance. This is probably because the KVs are not converged at the beginning of training and the KV loss helps the KVs converge. However, when the KVs are already converged, the KV loss could slow down the training process. In our method, we do not use the KV loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Encode Prompts with Different Number of Iterations</head><p>Though we set m = 7, b = 2 during training, it does not necessarily mean that we have to encode the prompts with 9 iterations. Is it possible to encode the prompts with less iterations? What if we encode the prompts with more iterations?</p><p>We treat the token segments as prompts and test the model trained in Section 3.2 with different numbers of iterations during encoding. As shown in Figure <ref type="figure" coords="13,337.57,643.79,9.16,9.46" target="#fig_2">12</ref>, the perplexity increases when the number of iterations is reduced, but still in a reasonable scale if we only reduce one or two iterations. The more warmup layers there are, the more stable the performance is. Increasing the number of iterations does not noticeably affect the performance. Thus, one could make a trade-off to set the proper number of iterations during inference to balance the time encoding prompts and the quality of generation texts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Model Performance With Respect to Token Position</head><p>The long-context performance of LLMs highly relies on KV cache <ref type="bibr" coords="14,154.51,475.01,84.07,9.46" target="#b32">(Xiao et al., 2024;</ref><ref type="bibr" coords="14,242.49,475.01,48.00,9.46;14,70.87,488.56,24.35,9.46" target="#b10">Han et al., 2023;</ref><ref type="bibr" coords="14,97.79,488.56,80.02,9.46" target="#b0">Adnan et al., 2024)</ref>. To verify that the performance of our model does not degrade under long context, we test the perplexity of our model with different token positions on PG19 <ref type="bibr" coords="14,215.80,529.20,70.64,9.46" target="#b22">(Rae et al., 2020)</ref>. Despite the fact that we only compute the KVs of a few layers, the performance of our model does not degrade with the token position (Figure <ref type="figure" coords="14,246.03,569.85,9.71,9.46" target="#fig_13">13</ref>) and is comparable to that of TinyLlama. Due to the limitation of computational resources, we only train models with context length 2048.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,306.14,351.47,218.43,8.64;1,306.14,363.42,219.92,8.64;1,306.14,375.38,218.26,8.64;1,306.14,387.01,218.26,8.96;1,306.14,398.97,38.19,8.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of a standard transformer decoder and our model. Each node represents one layer of transformer computation of one token. Each horizontal edge a → b denotes that the queries at b are paired with the KVs at a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,195.71,556.84,93.42,9.46;3,70.87,570.39,105.47,9.46;3,70.51,586.34,218.63,9.81;3,70.87,600.13,164.52,9.39"><head></head><label></label><figDesc>Figure 2(b) shows the new computation graph. Theorem 1. The two computation graphs are equivalent in terms of model training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,306.14,246.60,219.92,8.64;3,306.14,258.24,218.27,8.96;3,306.14,270.19,220.01,8.96;3,306.14,282.47,218.27,8.64;3,306.14,294.42,218.27,8.64;3,306.14,306.38,70.30,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two equivalent computation graphs for training our model with two layers. (a) Sequential over n tokens. (b) Parallel over n tokens with n iterations. Sub-graphs with the same color (except grey) represent identical data flow. Sub-graphs in grey are unused in loss computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,70.87,204.39,219.92,8.96;4,70.87,216.67,218.27,8.64;4,70.87,228.62,50.82,8.64"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MSE of the KV before and after the ith iteration. The model is randomly initialized and tested with 2048 tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,70.87,211.46,454.21,8.96;6,70.87,223.73,233.07,8.64"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of latency per token and memory consumption of StreamingLLM and our model (w = 10) integrated with StreamingLLM w.r.t. different cache sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="6,306.14,393.59,218.27,8.64;6,306.14,405.55,220.01,8.64;6,305.67,417.50,218.73,8.64;6,306.14,429.46,219.52,8.64;6,306.14,441.41,25.73,8.64"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of StreamingLLM and our model integrated with StreamingLLM w.r.t. the cache size.We use 4 initial tokens for all settings. The results are collected on the first text sample of PG19<ref type="bibr" coords="6,480.71,429.46,44.95,8.64;6,306.14,441.41,21.44,8.64" target="#b22">(Rae et al., 2020)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,306.14,216.55,218.27,8.64;7,306.14,228.51,218.27,8.64;7,306.14,240.14,218.27,8.96;7,306.14,252.42,219.92,8.64;7,306.14,264.37,218.27,8.64;7,306.14,276.33,219.92,8.64;7,306.14,288.28,67.80,8.64"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effect of the number of warmup layers on model performance and throughput. The right-most point (w = 22) denotes that all the layers are warmup layers and hence our model becomes exactly the standard transformer (the baseline). The throughput is tested on an RTX 3090 GPU with prompt length 5 and generation length 2043.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="8,70.87,297.98,218.27,8.64;8,70.87,309.61,218.26,8.96;8,70.87,321.57,218.27,8.96;8,70.87,333.84,111.41,8.64"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: (Top) Experiment on a Llama (50M) with different values of m during training. (Bottom) MSE of the KV before and after the ith iteration. The models are tested with 1024 tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="8,306.14,219.32,218.27,8.96;8,306.14,231.60,199.45,8.64"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: MSE of the KV before and after the ith iteration. The models are tested with 2048 tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="13,70.87,384.46,218.27,8.96;13,70.87,396.73,105.04,8.64"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Effect of b on a 50M model with different number of warmup layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="13,97.15,690.57,28.48,9.46;13,164.98,690.57,97.88,9.46;13,97.15,709.47,55.88,9.81;13,168.63,709.82,30.00,9.46;13,233.09,709.82,10.91,9.46;13,97.15,723.02,55.88,9.81;13,168.63,723.36,30.00,9.46;13,230.36,723.36,16.36,9.46;13,97.15,736.56,55.88,9.81;13,168.63,736.91,30.00,9.46;13,230.36,736.91,16.36,9.46"><head>Model</head><label></label><figDesc>Dev ppl. Train Time Ours (b = 2) 10.390 8h Ours (b = 3) 10.476 10h Ours (b = 4) 10.885 13h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="14,70.87,202.07,26.53,8.64;14,118.13,202.07,171.00,8.64;14,70.87,214.02,171.56,8.64"><head>Figure 13 :</head><label>13</label><figDesc>Figure Performance of the models with different numbers of iterations for prompt encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,70.56,76.07,453.85,294.54"><head>Table 1 :</head><label>1</label><figDesc>Maximum generation batch size and throughput on an RTX 3090 (24GB) and an A100 (80GB) GPU respectively with different sequence lengths. Following Zhang et al. (2023), we use "x + y" to denote a prompt length of x and a generation length of y.</figDesc><table coords="5,82.79,76.07,429.69,294.54"><row><cell cols="3">GPU Model Size Seq. Length</cell><cell></cell><cell>Batch Size</cell><cell></cell><cell></cell><cell cols="2">Throughput (tokens/s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Llama</cell><cell>Ours w = 2</cell><cell>Ours w = 10</cell><cell>Llama</cell><cell></cell><cell>Ours w = 2</cell><cell>Ours w = 10</cell></row><row><cell></cell><cell>1.1B</cell><cell>5+8187 5+2043</cell><cell cols="6">48 239 1150 (4.8×) 289 (1.2×) 5142.86 10033.40 (2.0×) 7239.92 (1.4×) 384 (8×) 119 (2.5×) 1424.96 4113.37 (2.9×) 2374.05 (1.7×)</cell></row><row><cell>RTX 3090</cell><cell>7B</cell><cell>5+8187 2048+2048 5+2043</cell><cell>1 2 5</cell><cell cols="3">12 (12×) 23 (11.5×) 64 (12.8×) 16 (3.2×) 140.88 4 (4×) 32.02 8 (4×) 56.98</cell><cell cols="2">151.91 (4.7×) 171.65 (3.0×) 534.02 (3.8×)</cell><cell>83.80 (2.6×) 119.68 (2.1×) 315.38 (2.2×)</cell></row><row><cell></cell><cell></cell><cell>512+512</cell><cell>9</cell><cell cols="3">95 (10.6×) 32 (3.6×) 225.31</cell><cell cols="2">378.89 (1.7×)</cell><cell>380.60 (1.7×)</cell></row><row><cell></cell><cell></cell><cell>512+1024</cell><cell>7</cell><cell cols="3">72 (10.3×) 16 (2.3×) 174.11</cell><cell cols="2">401.92 (2.3×)</cell><cell>310.05 (1.8×)</cell></row><row><cell cols="2">30B (CPU-offload)</cell><cell>512+1024</cell><cell>4</cell><cell cols="2">83 (20.8×) 23 (5.8×)</cell><cell>0.23</cell><cell></cell><cell>5.99 (26.0×)</cell><cell>1.63 (7.1×)</cell></row><row><cell>A100</cell><cell>7B</cell><cell>2048+2048</cell><cell>15</cell><cell cols="3">128 (8.5×) 42 (2.8×) 141.10</cell><cell cols="2">421.02 (3.0×)</cell><cell>315.09 (2.2×)</cell></row><row><cell></cell><cell>30B</cell><cell>2048+2048</cell><cell>1</cell><cell>32 (32×)</cell><cell>8 (8×)</cell><cell>14.10</cell><cell cols="2">108.29 (7.7×)</cell><cell>77.65 (5.5×)</cell></row><row><cell>Model</cell><cell cols="8">HellaSwag Obqa WinoGrande ARC-c ARC-e BoolQ PIQA Avg</cell></row><row><cell cols="2">TinyLlama</cell><cell>44.58</cell><cell>30.2</cell><cell>50.99</cell><cell>25.00</cell><cell cols="2">46.38</cell><cell>60.46 68.93 46.65</cell></row><row><cell cols="2">Ours (w = 2)</cell><cell>42.22</cell><cell>30.6</cell><cell>49.64</cell><cell>24.74</cell><cell cols="2">43.10</cell><cell>61.38 66.49 45.45</cell></row><row><cell cols="2">Ours (w = 10)</cell><cell>44.74</cell><cell>31.0</cell><cell>51.70</cell><cell>24.83</cell><cell cols="2">46.38</cell><cell>61.38 67.90 46.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,70.87,387.00,351.22,181.79"><head>Table 2 :</head><label>2</label><figDesc>Zero-shot accuracy on commonsense reasoning tasks.</figDesc><table coords="5,78.59,418.81,190.40,116.83"><row><cell>Thoughput (tokens/s)</cell><cell>100 150 200 250 300 350 400</cell><cell>Llama ours</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16 32 64 128 Batch Size</cell></row></table><note coords="5,70.87,548.19,220.01,8.64;5,70.87,560.15,56.72,8.64"><p>Figure 4: Throughput of 7B Llama and our model w.r.t. the batch size.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,305.83,495.64,218.57,20.59"><head>Table 3 :</head><label>3</label><figDesc>Perplexity on a 10M subset of the development set of SlimPajama.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,111.55,77.31,372.17,432.94"><head>Table 5 :</head><label>5</label><figDesc>Model configurations.</figDesc><table coords="12,111.55,77.31,372.17,432.94"><row><cell>Model Size</cell><cell></cell><cell cols="2">50M 1.1B</cell><cell>7B</cell><cell>30B</cell></row><row><cell>Hidden Size</cell><cell></cell><cell cols="3">512 2048 4096</cell><cell>6656</cell></row><row><cell cols="2">Intermediate Size</cell><cell cols="4">1024 5632 11008 17920</cell></row><row><cell cols="4">Max Trained Length 1024 2048</cell><cell>-</cell><cell>-</cell></row><row><cell># Layers</cell><cell></cell><cell>8</cell><cell>22</cell><cell>32</cell><cell>60</cell></row><row><cell cols="2"># Attention Heads</cell><cell>8</cell><cell>32</cell><cell>32</cell><cell>52</cell></row><row><cell># KV Heads</cell><cell></cell><cell>4</cell><cell>4</cell><cell>32</cell><cell>52</cell></row><row><cell cols="2">RMS Norm eps</cell><cell cols="2">1e-5 1e-5</cell><cell>1e-6</cell><cell>1e-6</cell></row><row><cell>Vocab Size</cell><cell></cell><cell></cell><cell cols="2">32000</cell></row><row><cell>Section</cell><cell></cell><cell>4.1</cell><cell></cell><cell cols="2">4.2</cell><cell>4.3</cell></row><row><cell>Model Size</cell><cell>50M</cell><cell></cell><cell>1.1B</cell><cell cols="2">1.1B</cell><cell>50M</cell></row><row><cell>m</cell><cell>7</cell><cell></cell><cell>7</cell><cell>7</cell><cell>-</cell></row><row><cell>b</cell><cell>2</cell><cell></cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>w</cell><cell>2</cell><cell></cell><cell>2</cell><cell>-</cell><cell>2</cell></row><row><cell>lr scheduler</cell><cell></cell><cell></cell><cell cols="2">cosine</cell></row><row><cell>max. lr</cell><cell>3e-4</cell><cell></cell><cell>3e-4</cell><cell cols="2">4e-4</cell><cell>3e-4</cell></row><row><cell>min. lr</cell><cell>0</cell><cell></cell><cell>0</cell><cell cols="2">4e-5</cell><cell>0</cell></row><row><cell>optimizer</cell><cell></cell><cell></cell><cell cols="2">AdamW</cell></row><row><cell>β 1</cell><cell>0.9</cell><cell></cell><cell>0.9</cell><cell cols="2">0.9</cell><cell>0.9</cell></row><row><cell>β 2</cell><cell>0.999</cell><cell></cell><cell>0.999</cell><cell cols="2">0.95</cell><cell>0.999</cell></row><row><cell>batch size (tokens)</cell><cell>16K</cell><cell></cell><cell>256K</cell><cell cols="2">256K</cell><cell>16K</cell></row><row><cell>warmup ratio</cell><cell>0.015</cell><cell></cell><cell>0.015</cell><cell cols="2">0.024</cell><cell>0.015</cell></row><row><cell>weight decay</cell><cell>6.6e-6</cell><cell></cell><cell>6.6e-6</cell><cell cols="2">1e-1</cell><cell>6.6e-6</cell></row><row><cell>gradient clipping</cell><cell>1.0</cell><cell></cell><cell>1.0</cell><cell cols="2">1.0</cell><cell>1.0</cell></row><row><cell>initialize from pre-trained</cell><cell>yes</cell><cell></cell><cell>yes</cell><cell>no</cell><cell>no</cell></row><row><cell>epochs</cell><cell>3</cell><cell></cell><cell>1</cell><cell cols="2">2B tokens</cell><cell>3</cell></row><row><cell>Data</cell><cell cols="5">WikiText-103 MiniPile SlimPajama WikiText-103</cell></row><row><cell>GPU</cell><cell cols="3">RTX 3090x1 A100x8</cell><cell cols="2">A800x16</cell><cell>RTX 3090x1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,70.32,526.64,304.23,83.72"><head>Table 6 :</head><label>6</label><figDesc>Training details for Section 4.</figDesc><table coords="12,70.32,560.15,218.82,50.22"><row><cell>10), respectively. The training took place before</cell></row><row><cell>we optimize our codes, so the training time could</cell></row><row><cell>be further reduced, especially for the model with</cell></row><row><cell>w = 10.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,105.99,76.96,394.46,694.44"><head>Table 8 :</head><label>8</label><figDesc>Effect of b on a 1.1B model.</figDesc><table coords="13,330.10,76.96,170.35,102.50"><row><cell cols="4">Model Size w KV Loss Dev ppl.</cell></row><row><cell></cell><cell>0</cell><cell>no</cell><cell>15.965</cell></row><row><cell>50M</cell><cell>0 2</cell><cell>yes no</cell><cell>15.610 15.004</cell></row><row><cell></cell><cell>2</cell><cell>yes</cell><cell>15.065</cell></row><row><cell>1.1B</cell><cell>2 2</cell><cell>no yes</cell><cell>9.746 10.073</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="13,305.83,195.86,218.58,44.50"><head>Table 9 :</head><label>9</label><figDesc>Effect of the KV loss on a 50M and a 1.1B model. The 50M model is trained on WikiText-103 with 3 epochs and the 1.1B model is trained on a 100B subset of SlimPajama.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,70.87,521.92,219.52,8.64;9,81.78,532.88,207.36,8.64;9,81.78,543.84,207.36,8.64;9,81.78,554.79,207.36,8.64;9,81.78,565.58,177.81,8.81" xml:id="b0">
	<monogr>
		<title level="m" type="main" coords="9,152.96,543.84,136.18,8.64;9,81.78,554.79,207.36,8.64;9,81.78,565.75,35.60,8.64">Keyformer: Kv cache reduction through key tokens selection for efficient generative inference</title>
		<author>
			<persName coords=""><forename type="first">Muhammad</forename><surname>Adnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akhil</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gaurav</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prashant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Purushotham</forename><surname>Soloveychik</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kamath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.09054</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,70.87,585.13,219.51,8.64;9,81.78,596.09,209.01,8.64;9,81.78,606.88,207.35,8.81;9,81.17,617.84,209.21,8.81;9,81.78,628.97,72.50,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coords="9,134.87,596.09,155.91,8.64;9,81.78,607.05,115.27,8.64">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName coords=""><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,215.57,606.88,73.56,8.58;9,81.17,617.84,159.85,8.58">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.87,648.34,219.52,8.64;9,81.47,659.30,207.67,8.64;9,81.47,670.26,207.67,8.64;9,81.78,681.05,207.36,8.81;9,81.78,692.01,209.01,8.58;9,81.78,702.97,209.02,8.58;9,81.42,713.93,207.71,8.58;9,81.53,724.89,209.26,8.81;9,81.78,736.01,202.57,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coords="9,191.25,670.26,97.88,8.64;9,81.78,681.22,140.99,8.64">Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1300</idno>
	</analytic>
	<monogr>
		<title level="m" coords="9,240.78,681.05,48.36,8.58;9,81.78,692.01,209.01,8.58;9,81.78,702.97,209.02,8.58;9,81.42,713.93,121.96,8.58">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s" coords="9,253.30,713.93,35.83,8.58;9,81.53,724.89,48.89,8.58">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.87,755.39,218.27,8.64;9,81.78,766.35,207.67,8.64;9,317.05,75.17,209.01,8.81;9,317.05,86.13,209.02,8.58;9,316.44,97.09,209.21,8.58;9,317.05,108.22,209.01,8.64;9,317.05,119.18,89.12,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coords="9,218.31,766.35,71.13,8.64;9,317.05,75.34,169.31,8.64">What does BERT look at? an analysis of BERT&apos;s attention</title>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4828</idno>
	</analytic>
	<monogr>
		<title level="m" coords="9,507.89,75.17,18.18,8.58;9,317.05,86.13,209.02,8.58;9,316.44,97.09,204.12,8.58">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,306.14,139.94,219.51,8.64;9,316.69,150.90,207.72,8.64;9,316.74,161.86,209.32,8.64;9,317.05,172.65,207.36,8.81;9,317.05,183.61,110.05,8.58" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m" coords="9,379.52,161.86,146.54,8.64;9,317.05,172.82,177.46,8.64">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,204.54,219.92,8.64;9,317.05,215.50,209.10,8.64;9,316.74,226.46,207.67,8.64;9,317.05,237.25,207.36,8.81;9,316.44,248.21,207.97,8.58;9,316.77,259.17,209.29,8.81;9,317.05,270.30,152.21,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coords="9,316.74,226.46,207.67,8.64;9,317.05,237.42,88.00,8.64">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1285</idno>
	</analytic>
	<monogr>
		<title level="m" coords="9,426.29,237.25,98.12,8.58;9,316.44,248.21,207.97,8.58;9,316.77,259.17,42.37,8.58">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,306.14,291.06,218.27,8.64;9,317.05,301.85,207.36,8.81;9,317.05,312.81,110.05,8.58" xml:id="b6">
	<monogr>
		<title level="m" type="main" coords="9,369.14,291.06,155.27,8.64;9,317.05,302.02,169.57,8.64">Flashattention-2: Faster attention with better parallelism and work partitioning</title>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08691</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,333.75,219.52,8.64;9,317.05,344.71,207.35,8.64;9,317.05,355.67,209.10,8.64;9,317.05,366.46,209.01,8.81;9,317.05,377.42,20.75,8.58" xml:id="b7">
	<analytic>
		<title level="a" type="main" coords="9,425.74,344.71,98.67,8.64;9,317.05,355.67,204.72,8.64">FlashAttention: Fast and memory-efficient exact attention with IO-awareness</title>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,328.09,366.46,197.97,8.58;9,317.05,377.42,16.60,8.58">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,306.14,398.35,219.51,8.64;9,317.05,409.31,207.35,8.64;9,317.05,420.27,208.60,8.64;9,317.05,431.23,208.60,8.64;9,316.86,442.19,208.80,8.64;9,316.69,453.15,209.37,8.64;9,317.05,464.10,209.10,8.64;9,317.05,475.06,207.36,8.64;9,317.05,486.02,43.50,8.64" xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stella</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sid</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurence</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alain</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haonan</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niklas</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laria</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hailey</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aviya</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lintang</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.10256836</idno>
	</analytic>
	<monogr>
		<title level="j" coords="9,510.27,453.15,15.80,8.64;9,317.05,464.10,34.85,8.64">Anish Thite</title>
		<imprint/>
	</monogr>
	<note>and Andy Zou. 2023. A framework for few-shot language model evaluation</note>
</biblStruct>

<biblStruct coords="9,306.14,506.79,219.52,8.64;9,316.86,517.75,207.55,8.64;9,316.69,528.71,207.89,8.64;9,317.05,539.50,207.36,8.81;9,316.49,550.46,207.91,8.58;9,316.74,561.41,194.71,8.58" xml:id="b9">
	<analytic>
		<title level="a" type="main" coords="9,463.39,517.75,61.02,8.64;9,316.69,528.71,207.89,8.64;9,317.05,539.67,22.18,8.64">Model tells you what to discard: Adaptive KV cache compression for LLMs</title>
		<author>
			<persName coords=""><forename type="first">Suyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,359.25,539.50,165.16,8.58;9,316.49,550.46,207.91,8.58;9,316.74,561.41,91.23,8.58">Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization</title>
		<imprint>
			<publisher>WANT@NeurIPS</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,306.14,582.35,218.26,8.64;9,316.86,593.31,207.55,8.64;9,317.05,604.27,207.36,8.64;9,317.05,615.06,169.53,8.81" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.16137</idno>
		<title level="m" coords="9,441.00,593.31,83.41,8.64;9,317.05,604.27,207.36,8.64;9,317.05,615.23,26.81,8.64">Lm-infinite: Simple on-the-fly length generalization for large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,306.14,635.99,218.27,8.64;9,316.47,646.95,209.59,8.64;9,317.05,657.91,209.01,8.64;9,317.05,668.70,209.01,8.81;9,317.05,679.66,209.01,8.58;9,317.05,690.62,207.35,8.81;9,317.05,701.75,122.60,8.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coords="9,396.27,646.95,129.80,8.64;9,317.05,657.91,209.01,8.64;9,317.05,668.87,54.47,8.64">2023a. LLMLingua: Compressing prompts for accelerated inference of large language models</title>
		<author>
			<persName coords=""><forename type="first">Huiqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qianhui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lili</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.825</idno>
	</analytic>
	<monogr>
		<title level="m" coords="9,392.19,668.70,133.87,8.58;9,317.05,679.66,209.01,8.58;9,317.05,690.62,27.29,8.58">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="13358" to="13376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,306.14,722.51,218.27,8.64;9,317.05,733.47,209.10,8.64;9,317.05,744.43,207.36,8.64;9,317.05,755.39,209.10,8.64;9,317.05,766.18,134.67,8.58" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Huiqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qianhui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xufang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lili</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06839</idno>
		<title level="m" coords="9,317.05,744.43,207.36,8.64;9,317.05,755.39,204.61,8.64">Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.87,75.34,218.44,8.64;10,81.78,86.13,207.36,8.81;10,81.78,97.09,75.27,8.58" xml:id="b13">
	<monogr>
		<title level="m" type="main" coords="10,172.78,75.34,116.53,8.64;10,81.78,86.30,128.44,8.64">The minipile challenge for data-efficient language models</title>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Kaddour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08442</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.87,117.04,218.27,8.64;10,81.78,128.00,209.01,8.64;10,81.78,138.96,207.36,8.64;10,81.78,149.92,209.01,8.64;10,81.78,160.71,207.36,8.81;10,81.53,171.67,207.61,8.81;10,81.78,182.80,37.36,8.64" xml:id="b14">
	<analytic>
		<title level="a" type="main" coords="10,254.39,138.96,34.74,8.64;10,81.78,149.92,209.01,8.64;10,81.78,160.88,93.39,8.64">Efficient memory management for large language model serving with pagedattention</title>
		<author>
			<persName coords=""><forename type="first">Woosuk</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,193.77,160.71,95.36,8.58;10,81.53,171.67,178.30,8.58">Proceedings of the 29th Symposium on Operating Systems Principles</title>
		<meeting>the 29th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="611" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,202.58,220.01,8.64;10,81.78,213.54,209.01,8.64;10,81.78,224.33,207.36,8.81;10,81.78,235.29,209.01,8.58;10,81.78,246.25,209.02,8.81;10,81.78,257.38,194.82,8.64" xml:id="b15">
	<analytic>
		<title level="a" type="main" coords="10,107.83,213.54,182.96,8.64;10,81.78,224.50,130.09,8.64">Compressing context to enhance inference efficiency of large language models</title>
		<author>
			<persName coords=""><forename type="first">Yucheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Guerin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.391</idno>
	</analytic>
	<monogr>
		<title level="m" coords="10,230.51,224.33,58.62,8.58;10,81.78,235.29,209.01,8.58;10,81.78,246.25,99.73,8.58">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Singapore. Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6342" to="6353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,277.16,218.27,8.64;10,81.31,288.12,209.48,8.64;10,81.78,299.08,209.01,8.64;10,81.78,310.04,207.36,8.64;10,81.78,321.00,207.36,8.64;10,81.78,331.79,209.01,8.81;10,81.78,342.75,110.51,8.58" xml:id="b16">
	<analytic>
		<title level="a" type="main" coords="10,219.43,288.12,71.36,8.64;10,81.78,299.08,142.16,8.64;10,270.46,299.08,20.32,8.64;10,81.78,310.04,207.36,8.64;10,81.78,321.00,207.36,8.64;10,81.78,331.96,16.24,8.64">Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time</title>
		<author>
			<persName coords=""><forename type="first">Zichang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fangshuo</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhaozhuo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,116.50,331.79,174.29,8.58;10,81.78,342.75,106.26,8.58">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Anastasios Kyrillidis, and Anshumali Shrivastava</note>
</biblStruct>

<biblStruct coords="10,70.87,362.70,218.27,8.64;10,81.42,373.49,209.37,8.81;10,81.78,384.45,138.54,8.58" xml:id="b17">
	<analytic>
		<title level="a" type="main" coords="10,245.11,362.70,44.02,8.64;10,81.42,373.66,107.14,8.64">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,206.53,373.49,84.26,8.58;10,81.78,384.45,134.39,8.58">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,404.41,218.27,8.64;10,81.78,415.36,209.02,8.64;10,81.78,426.15,209.01,8.81;10,81.78,437.11,42.90,8.58" xml:id="b18">
	<analytic>
		<title level="a" type="main" coords="10,172.88,415.36,117.91,8.64;10,81.78,426.32,10.16,8.64">Pointer sentinel mixture models</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,109.68,426.15,181.11,8.58;10,81.78,437.11,39.00,8.58">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,457.07,218.27,8.64;10,81.78,468.03,209.01,8.64;10,81.78,478.99,209.01,8.64;10,81.78,489.78,207.36,8.81;10,81.47,500.74,208.91,8.58;10,81.78,511.86,207.36,8.64;10,81.78,522.82,122.60,8.64" xml:id="b19">
	<analytic>
		<title level="a" type="main" coords="10,155.91,468.03,134.88,8.64;10,81.78,478.99,209.01,8.64;10,81.78,489.95,30.05,8.64">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName coords=""><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1260</idno>
	</analytic>
	<monogr>
		<title level="m" coords="10,130.60,489.78,158.53,8.58;10,81.47,500.74,204.75,8.58">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2381" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,542.61,220.01,8.64;10,81.78,553.57,207.36,8.64;10,81.22,564.36,207.91,8.58;10,81.47,575.32,80.35,8.58" xml:id="b20">
	<analytic>
		<title level="a" type="main" coords="10,81.78,553.57,190.35,8.64">Learning to compress prompts with gist tokens</title>
		<author>
			<persName coords=""><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,81.22,564.36,207.91,8.58;10,81.47,575.32,76.09,8.58">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,595.27,219.52,8.64;10,81.59,606.23,207.55,8.64;10,81.42,617.19,209.37,8.64;10,81.78,627.98,207.36,8.81;10,81.78,638.94,147.89,8.81" xml:id="b21">
	<analytic>
		<title level="a" type="main" coords="10,272.42,617.19,18.37,8.64;10,81.78,628.15,148.48,8.64">Efficiently scaling transformer inference</title>
		<author>
			<persName coords=""><forename type="first">Reiner</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sholto</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kefan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shivani</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,238.78,627.98,50.35,8.58;10,81.78,638.94,133.68,8.58">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,658.89,219.92,8.64;10,81.78,669.85,209.10,8.64;10,81.78,680.81,207.36,8.64;10,81.78,691.60,207.36,8.81;10,81.47,702.56,66.32,8.58" xml:id="b22">
	<analytic>
		<title level="a" type="main" coords="10,81.78,680.81,207.36,8.64;10,81.78,691.77,38.22,8.64">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName coords=""><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,138.52,691.60,150.61,8.58;10,81.47,702.56,62.18,8.58">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.87,722.51,219.92,8.64;10,81.78,733.47,207.36,8.64;10,81.78,744.43,209.10,8.64;10,81.78,755.22,207.36,8.81;10,81.50,766.18,207.64,8.81;10,317.05,75.34,209.01,8.64;10,317.05,86.30,79.15,8.64" xml:id="b23">
	<analytic>
		<title level="a" type="main" coords="10,132.08,733.47,157.06,8.64;10,81.78,744.43,204.95,8.64">Subformer: Exploring weight sharing for parameter efficiency in generative transformers</title>
		<author>
			<persName coords=""><forename type="first">Machel</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edison</forename><surname>Marrese-Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yutaka</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.344</idno>
	</analytic>
	<monogr>
		<title level="m" coords="10,94.01,755.22,195.13,8.58;10,81.50,766.18,103.11,8.58">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">Matsuo. 2021</date>
			<biblScope unit="page" from="4081" to="4090" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="10,306.14,105.85,219.92,8.64;10,317.05,116.81,209.01,8.64;10,317.05,127.60,207.36,8.81;10,317.05,138.56,209.02,8.58;10,317.05,149.51,207.53,8.81;10,317.05,160.64,108.49,8.64" xml:id="b24">
	<analytic>
		<title level="a" type="main" coords="10,470.97,105.85,55.10,8.64;10,317.05,116.81,209.01,8.64;10,317.05,127.77,44.47,8.64">Context compression for auto-regressive transformers with sentinel tokens</title>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Siyu Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenny</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.794</idno>
	</analytic>
	<monogr>
		<title level="m" coords="10,379.94,127.60,144.47,8.58;10,317.05,138.56,209.02,8.58;10,317.05,149.51,11.33,8.58">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="12860" to="12867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,180.19,219.92,8.64;10,317.05,191.15,209.01,8.64;10,317.05,201.94,209.01,8.81;10,317.05,212.90,146.37,8.81" xml:id="b25">
	<monogr>
		<title level="m" type="main" coords="10,428.99,191.15,97.07,8.64;10,317.05,201.94,209.01,8.81;10,317.05,212.90,61.44,8.58">Winogrande: An adversarial winograd schema challenge at scale. Communications of the</title>
		<author>
			<persName coords=""><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,232.62,218.26,8.64;10,317.05,243.58,209.01,8.64;10,317.05,254.53,208.75,8.64;10,317.05,265.49,209.01,8.64;10,317.05,276.28,207.36,8.81;10,317.05,287.24,209.01,8.58;10,317.05,298.20,207.36,8.81;10,316.74,309.16,156.10,8.81" xml:id="b26">
	<analytic>
		<title level="a" type="main" coords="10,493.93,243.58,32.13,8.64;10,317.05,254.53,137.60,8.64;10,488.26,254.53,37.53,8.64;10,317.05,265.49,209.01,8.64;10,317.05,276.45,127.77,8.64">FlexGen: High-throughput generative inference of large language models with a single GPU</title>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Binhang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Ryabinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m" coords="10,464.81,276.28,59.60,8.58;10,317.05,287.24,209.01,8.58;10,317.05,298.20,11.19,8.58">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="31094" to="31116" />
		</imprint>
	</monogr>
	<note>Christopher Re, Ion Stoica, and Ce Zhang</note>
</biblStruct>

<biblStruct coords="10,306.14,328.88,219.92,8.64;10,317.05,339.84,209.10,8.64;10,317.05,350.80,207.35,8.64;10,316.80,361.75,90.18,8.64" xml:id="b27">
	<monogr>
		<title level="m" type="main" coords="10,317.05,350.80,207.35,8.64;10,316.80,361.75,85.41,8.64">SlimPajama: A 627B token cleaned and deduplicated version of RedPajama</title>
		<author>
			<persName coords=""><forename type="first">Daria</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faisal</forename><surname>Al-Khateeb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><forename type="middle">R</forename><surname>Steeves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nolan</forename><surname>Dey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,381.30,220.01,8.64;10,317.05,392.26,209.01,8.64;10,317.05,403.05,209.02,8.81;10,317.05,414.01,207.36,8.58;10,316.22,424.97,208.19,8.58;10,316.77,435.93,101.08,8.81" xml:id="b28">
	<analytic>
		<title level="a" type="main" coords="10,346.63,392.26,179.43,8.64;10,317.05,403.22,171.98,8.64">Triton: an intermediate language and compiler for tiled neural network computations</title>
		<author>
			<persName coords=""><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hsiang-Tsung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,507.94,403.05,18.13,8.58;10,317.05,414.01,207.36,8.58;10,316.22,424.97,208.19,8.58;10,316.77,435.93,41.42,8.58">Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
		<meeting>the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,455.65,218.44,8.64;10,317.05,466.61,208.60,8.64;10,317.05,477.56,208.60,8.64;10,317.05,488.52,209.01,8.64;10,317.05,499.31,207.36,8.81;10,317.05,510.27,75.27,8.58" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m" coords="10,317.05,488.52,209.01,8.64;10,317.05,499.48,137.75,8.64">Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,306.14,529.99,218.27,8.64;10,317.05,540.95,209.02,8.64;10,317.05,551.91,209.01,8.64;10,317.05,562.87,208.60,8.64;10,317.05,573.83,208.60,8.64;10,316.74,584.79,208.91,8.64;10,317.05,595.74,209.01,8.64;10,317.05,606.70,209.10,8.64;10,317.05,617.49,207.36,8.81;10,316.63,628.45,207.78,8.58;10,316.69,639.41,207.72,8.81;10,317.05,650.54,122.60,8.64" xml:id="b30">
	<analytic>
		<title level="a" type="main" coords="10,499.89,595.74,26.17,8.64;10,317.05,606.70,205.08,8.64">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m" coords="10,327.71,617.49,196.70,8.58;10,316.63,628.45,207.78,8.58;10,316.69,639.41,62.83,8.58">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,670.09,219.92,8.64;10,317.05,681.05,209.01,8.64;10,317.05,691.84,209.01,8.81;10,317.05,702.80,208.60,8.58;10,317.05,713.92,207.53,8.64;10,317.05,724.88,108.49,8.64" xml:id="b31">
	<analytic>
		<title level="a" type="main" coords="10,447.86,670.09,78.21,8.64;10,317.05,681.05,209.01,8.64;10,317.05,692.01,110.25,8.64">Probabilistic transformer: A probabilistic dependency model for contextual word representation</title>
		<author>
			<persName coords=""><forename type="first">Haoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-acl.482</idno>
	</analytic>
	<monogr>
		<title level="m" coords="10,447.71,691.84,78.35,8.58;10,317.05,702.80,204.12,8.58">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7613" to="7636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,744.43,218.27,8.64;10,317.05,755.39,209.01,8.64;10,317.05,766.18,207.35,8.81" xml:id="b32">
	<monogr>
		<title level="m" type="main" coords="10,433.05,755.39,93.01,8.64;10,317.05,766.35,138.78,8.64">Efficient streaming language models with attention sinks</title>
		<author>
			<persName coords=""><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>In The Twelfth</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
